[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Irhum Shafkat",
    "section": "",
    "text": "Hi! I’m Irhum, a senior pursuing an undergraduate degree in CS at Minerva University. My focus is on applying machine learning to real world problems (particularly with connecting abstract ideas back to the real world), and that’s typically what I also write about here on my blog.\nI’m a strong proponent of interdisciplinary thinking. Computer science is wonderful, and while it is necessary, it alone is not sufficient to solve wicked problems such as climate change that society faces. I’m excited to keep up with work in related fields such as systems thinking, computational biology, physics simulations, and so forth, which combine computation with domain expertise in other fields. They are opening up paths to innovative solutions that cross the traditional boundaries of academia, and are essential to solving some of the largest problems in the world, both today and tomorrow."
  },
  {
    "objectID": "blog/lorawd/index.html",
    "href": "blog/lorawd/index.html",
    "title": "LoRA and Weight Decay",
    "section": "",
    "text": "LoRA (Hu et al., 2021) is a now popular alternative to the full finetuning of a Large Language Models (LLMs): instead of tuning the billions of weights of the full model, we add small “adapter” weight matrices that modify the original weight matrices, and tune those instead.\nThis blogpost dives deeper into a curious behavior: although LoRA is commonly seen an drop-in for full finetuning, its interaction with weight decay means it solves a different optimization problem than full finetuning. Namely, one where the solution weights are regularized towards the frozen base model \\((W \\rightarrow W_{\\text{init}})\\), instead of \\(W \\rightarrow 0\\) as in full finetuning.\nThis means, given increasingly more resources (even equalling that of full finetuning), LoRA does not increasingly better approximate full finetuning, because its objective function is implicitly different to that of full finetuning. This, depending on use case can either be seen as a bug or a feature, but is something practitioners should explicitly account for."
  },
  {
    "objectID": "blog/lorawd/index.html#recap-finetuning",
    "href": "blog/lorawd/index.html#recap-finetuning",
    "title": "LoRA and Weight Decay",
    "section": "Recap: Finetuning",
    "text": "Recap: Finetuning\nWith LLMs, we typically finetune an initial model (that is “good” on a wide range of text-to-text tasks) to boost performance on a specific task of interest (e.g. generating database queries from natural language). We do this in a two-step process:\n\nFirst, creating a finetuning training dataset \\({(x_i, y_i)_n}\\), which contain pairs of inputs \\(x\\) and targets \\(y\\).1\nOptimize the weights of the initial model such that our finetuning training dataset \\({(x_i, y_i)_n}\\) becomes more “probable”. The idea here is that a model that is more likely to generate the correct answers \\(y\\) on \\(x\\)’s from our training set, will generalize and also be more likely to generate \\(y\\)’s on new \\(x\\)’s.\n\n\nFull Finetuning\nFull finetuning means we tune all the weights of the model. For a model such as GPT-3 175B (Brown et al., 2020), this means giving our optimization algorithm 175 billion numbers it can “dial” up and down as needed to make our finetuning training data more “probable”. Let’s dig a bit deeper, and more concretely define what we mean by weights here.\nEach layer in a Transformer is primarily made of two components: a multihead attention network, followed by a feedforward network. This means the bulk of the “weights” that make up each layer are stored in six matrices2, as shown. \\(\\theta\\) then, is used as shorthand refer to all the weights, stored in all the matrices across all the layers of the model.\n\n\n\n\n\nIn full finetuning, every single weight in \\(\\theta\\) is opened up for updating. Our aim is to produce updated weights that minimize the negative log likelihood (NLL) as shown on the left3. There’s no closed form way to get the “optimal” weights, so we solve the optimization problem by repeatedly applying many steps of gradient descent, as shown on the right.\n\n\n\n\n\nNow, directly doing gradient descent this way would quickly lead to overfitting4, so we usually regularize the problem. With LLMs, the regularization tool of choice is usually weight decay. Specifically, when using vanilla SGD5, weight decay is equivalent to having a term in the loss equal to the squared sum of the weights:\n\\[R(\\theta)=\\sum_i \\sum_j[W_{{\\color{RoyalBlue}q}}^{\\color{PineGreen}{1}}]_{ij}^2+\\cdots\\]\nHence, the overall objective now is as follows (where \\(\\lambda\\) is a hyperparameter controlling the strength of the weight decay):\n\\[\\min_{\\color{YellowOrange}{\\theta}} \\biggl[\\underbrace{-\\log P_{\\color{YellowOrange}{\\theta}}({\\color{PineGreen}{y}} \\mid {\\color{RoyalBlue}{x}})}_{\\color{BrickRed}{L}} + \\frac{\\lambda}{2} R({\\color{YellowOrange}{\\theta}})\\biggr]\\]\nDifferentiating this to objective to get the gradient, we notice the gradient update has two distinct terms6: the first corresponding to the minimizing the negative log likelihood as before, and a new second term \\(-\\alpha\\lambda w\\) that pushes the weight towards the origin \\(0\\).\n\\[\n% https://tex.stackexchange.com/a/9477\n\\def\\mathunderline#1#2{\\color{#1}\\underline{{\\color{black}#2}}\\color{black}}\n\\begin{align*}\n&{\\color{YellowOrange}{w}} \\leftarrow {\\color{YellowOrange}{w}} - \\alpha \\left(\\mathunderline{BrickRed}{\\frac{\\partial \\color{BrickRed}{L}}{\\partial \\color{YellowOrange}{w}}} + \\mathunderline{LimeGreen}{\\frac{\\lambda}{2} \\frac{\\partial R}{\\partial \\color{YellowOrange}{w}}} \\right)\\\\\n    \\Rightarrow &{\\color{YellowOrange}{w}} \\leftarrow {\\color{YellowOrange}{w}} - \\alpha \\left(\\mathunderline{BrickRed}{\\frac{\\partial \\color{BrickRed}{L}}{\\partial \\color{YellowOrange}{w}}} + \\mathunderline{LimeGreen}{\\lambda {\\color{YellowOrange}{w}}} \\right)\\\\\n    \\Rightarrow &{\\color{YellowOrange}{w}} \\leftarrow {\\color{YellowOrange}{w}} - \\alpha \\mathunderline{BrickRed}{\\frac{\\partial \\color{BrickRed}{L}}{\\partial \\color{YellowOrange}{w}}} - \\alpha \\mathunderline{LimeGreen}{\\lambda {\\color{YellowOrange}{w}}}\n\\end{align*}\\]\nWhich means the regularized problem now looks like:\n\n\n\n\n\nIn summary, adding a squared sum of weights loss is equivalent to subtracting a scaled version of each weight at each gradient descent step. This shifts the minima towards where the weights are closer to \\(0\\)7; i.e. no one weight can have extremely large effects on the predictions of the model.\nFull finetuning is highly flexible, but also extremely memory intensive: you generally need at least 3x the memory8 required for the model itself, to account for its gradients and optimizer state. This was not an issue when models were \\(O(100M)\\) params, but is certainly so today where they’re regularly \\(O(10B)\\) to \\(O(100B)\\) params. Moreover, if you have 10 sub-tasks in your application (where you’re tuning the model for each task), full finetuning requires you to host 10 versions of the model (as if hosting 1 isn’t expensive as is!).\n\n\nLoRA finetuning\nLoRA (Low Rank Adapter) finetuning takes a different approach: instead of tuning the massive weight matrices of an LLM directly, we use a pair of small adapter matrices for each weight matrix we want to tune, of the following form:\n\n\n\n\n\nThat is, for each initial, frozen weight \\(W_{\\text{init}}\\), we have adapter matrices \\(A\\) and \\(B\\). These two matrices are multiplied together to form \\(\\Delta W\\), which is a low rank “adjustment” matrix for \\(W_{\\text{init}}\\), forming the adapted, tuned matrix \\(W\\). This cuts the number of free parameters significantly: assume the original matrix \\(W_{\\text{init}}\\) is \\(4,096 \\times 16,384\\). In the original, we’d have 67 million parameters to tune just for this one weight matrix, as follows:\n\\[4,096 \\times 16,384 = 67,108,864 \\approx 67 \\text{ million}\\]\nWith LoRA with rank \\(r=4\\), we only have:\n\\[4,096 \\times 4 + 4 \\times 16,384 = 81,920\\]\nThis is less than 0.1% of the original number of parameters; the added overhead of storing 3 variants of these values (weights, gradients and optimizer states) is negligible compared to the memory used by the model itself.\nMoreover, since the initial weights are “shared” across all the finetuning runs, at inference time we only need to load one copy of the initial model to be shared across many finetuned versions, with inference for each task using their own per-task adapter matrices. This makes having a “per-task” tuned LLM in an application not only viable, but easy."
  },
  {
    "objectID": "blog/lorawd/index.html#the-interaction",
    "href": "blog/lorawd/index.html#the-interaction",
    "title": "LoRA and Weight Decay",
    "section": "The Interaction",
    "text": "The Interaction\nNow that we’ve covered what LoRA is, we can begin to discuss how it interacts with weight decay to produce a feature/bug. Since \\(A\\) and \\(B\\) are the “actual” matrices we’re performing gradient descent on, the weight decay term in the objective looks like this, in that we’re moving the minima towards where the adapter matrices are closer to 0:\n\\[R(\\theta)=\\sum_i \\sum_j[A_{{\\color{RoyalBlue}q}}^{\\color{PineGreen}{1}}]_{ij}^2+ \\sum_i \\sum_j[B_{{\\color{RoyalBlue}q}}^{\\color{PineGreen}{1}}]_{ij}^2+ \\cdots\\]\nLet’s contrast this with the formulation in full finetuning:\n\n\n\n\n\n\nIn full finetuning, we have \\(W \\rightarrow 0\\), in that the weight decays to 0 directly.\nHowever, in LoRA, because \\(A\\) and \\(B\\) decay to 0, in effect we have \\(W \\rightarrow W_{\\text{init}}\\) instead.\n\nThis means LoRA solutions are biased towards the original frozen weight matrices, unlike in full finetuning, where they’re biased towards zero. And this behavior does not go away with increasing the LoRA rank \\(r\\) - one could increase it all the way to infinity(!), and the optimization process would still be biased towards the original frozen weights instead of zero. That is, even in the limit, LoRA does not approximate full finetuning, but a different objective.\n\nA fix\nIf we wanted the full adapted matrix to go towards zero (as would happen in full finetuning), we’d need a regularization term where the entire adapted weight matrix goes to zero, as follows:\n\\[\\begin{align*}\n    R(\\theta)&=\\sum_i \\sum_j[W_{{\\color{RoyalBlue}q}}^{\\color{PineGreen}{1}}]_{ij}^2+\\cdots\\\\\n    &=\\sum_i \\sum_j[W_{{\\color{RoyalBlue}q\\color{Black}\\text{,init}}}^{\\color{PineGreen}{1}} + A_{{\\color{RoyalBlue}q}}^{\\color{PineGreen}{1}}B_{{\\color{RoyalBlue}q}}^{\\color{PineGreen}{1}}]_{ij}^2+\\cdots\n\\end{align*}\\]\nThis is actually straightforward to derive, and yields a pair of update equations that can be implemented much like standard weight decay. First, start at the core definition of weight decay, which involves calculating the gradient of the weight w.r.t. the regularization term:\n\\[{\\color{YellowOrange}{w}} \\leftarrow {\\color{YellowOrange}{w}} - \\alpha \\left(\\frac{\\partial \\color{BrickRed}{L}}{\\partial \\color{YellowOrange}{w}} + \\frac{\\lambda}{2} \\frac{\\partial R}{\\partial \\color{YellowOrange}{w}} \\right)\\]\nSecond, compute the gradient of \\(A\\) and \\(B\\)9 w.r.t. the “corrected” \\(R(\\theta)\\) above. This yields:\n\\[\\begin{align*}\n    \\frac{\\partial R}{\\partial \\color{YellowOrange}{A}}&=2 (W_{\\text{init}} + {\\color{YellowOrange}{A}}{\\color{PineGreen}{B}}) {\\color{PineGreen}{B^T}}\\\\\n    \\frac{\\partial R}{\\partial \\color{YellowOrange}{B}}&=2 {\\color{PineGreen}{A^T}}(W_{\\text{init}} + {\\color{PineGreen}{A}}{\\color{YellowOrange}{B}})\n\\end{align*}\\]\nInserting back into the definition of weight decay, we get the following concrete update equations for \\(A\\) and \\(B\\):\n\\[\\begin{align*}\n    {\\color{YellowOrange}{A}} &\\leftarrow {\\color{YellowOrange}{A}} - \\alpha \\frac{\\partial \\color{BrickRed}{L}}{\\partial \\color{YellowOrange}{A}} - \\alpha \\lambda (W_{\\text{init}} + {\\color{YellowOrange}{A}}{\\color{PineGreen}{B}}) {\\color{PineGreen}{B^T}}\\\\\n    {\\color{YellowOrange}{B}} &\\leftarrow {\\color{YellowOrange}{A}} - \\alpha \\frac{\\partial \\color{BrickRed}{L}}{\\partial \\color{YellowOrange}{A}} - \\alpha \\lambda {\\color{PineGreen}{A^T}}(W_{\\text{init}} + {\\color{PineGreen}{A}}{\\color{YellowOrange}{B}})\n\\end{align*}\\]\n\nIn code\nThis is what the standard formulation of weight decay in the Optax (Babuschkin et al., 2020) library looks like. It’s quite clean: add a weight_decay (\\(\\lambda\\)) scaled version of the parameter p to its current update g10.\n\n# from https://github.com/google-deepmind/optax/blob/master/optax/_src/transform.py#L766\ndef update_fn(updates, state, params):\n    if params is None:\n        raise ValueError(base.NO_PARAMS_MSG)\n    updates = jax.tree_util.tree_map(\n        lambda g, p: g + weight_decay * p, updates, params)\nreturn updates, state\n\nTo modify this to implement the math we just described above takes some of extra code, mostly in extracting the W_init, A and B matrices11. The core logic is just the two lines 18 and 20.\n\ndef update_fn(updates, state, params):\n    def per_param_update_fn(path, update, param):\n        # Get the params dict for the layer as a whole.\n        param_name = path[-1].key\n        # If current parameter is an adapter matrix.\n        if param_name in ['kernelA', 'kernelB']:\n            layer_params = params\n            for dict_key in path[:-1]:\n                layer_params = layer_params[dict_key.key]\n\n            # Extract the initial weight matrix and adapter matrices.\n            W_init = layer_params['kernel']\n            A = layer_params['kernelA']\n            B = layer_params['kernelB']\n\n            # Compute the corrected decay term.\n            if param_name == 'kernelA':\n                decay_term = (W_init + A@B)@B.T\n            else:\n                decay_term = A.T@(W_init + A@B)\n        \n        # If current parameter is *not* an adapter matrix, use\n        # default version of weight decay.\n        else:\n            decay_term = param\n\n        return update + weight_decay * decay_term\n\n    if params is None:\n      raise ValueError(base.NO_PARAMS_MSG)\n    updates = jax.tree_util.tree_map_with_path(\n        per_param_update_fn, updates, params)\n    return updates, state"
  },
  {
    "objectID": "blog/lorawd/index.html#conclusion",
    "href": "blog/lorawd/index.html#conclusion",
    "title": "LoRA and Weight Decay",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, LoRA has a different implicit objective than full finetuning, but it’s also easy to correct if desired. That’s it, really!\nTo my knowledge, there isn’t literature documenting this interaction of LoRA with weight decay in depth. Conjecturing purely from first principles12, I’d argue the default behavior is both a feature and a bug, depending on the amount of data - when there’s a very few number of training points, it is a feature because it regularizes the updated model to stay close to the initial, “generally-capable” one. However, it’s a bug when given large amounts of data, as the optimization process is less capable of straying too far from the base weights, even if it would aid end-task performance.\nThat said, as neat as the math is, empirical results are the only truth here. With so many free parameters, it may well turn out to be in practice there are solutions just as good (when regularized to be close to \\(W_{\\text{init}}\\)) as full finetuning (regularized close to \\(0\\)) given enough capacity."
  },
  {
    "objectID": "blog/lorawd/index.html#appendix-a-momentum-and-weight-decay",
    "href": "blog/lorawd/index.html#appendix-a-momentum-and-weight-decay",
    "title": "LoRA and Weight Decay",
    "section": "Appendix A: Momentum and Weight Decay",
    "text": "Appendix A: Momentum and Weight Decay\nOne odd thing you’ve likely noticed is that I spent a substantial amount of time explicitly working out the gradient for the regularizer term \\(R(\\theta)\\), instead of just directly absorbing it into \\(L\\) and letting autodiff take care of all this for me. That’s because the equivalency (weight decay of gradients = adding an \\(L_2\\) regularization term to the loss) is only true for non-momentum based optimizers like vanilla SGD, not momentum based optimizers such as Adam(Kingma & Ba, 2015) or AdamW (Loshchilov & Hutter, 2019).\nThe AdamW paper13 is a solid, in-depth read to understand why this is the case, but in brief: to do weight decay we want to subtract away a scaled version of the parameter’s value at the current timestep. However, adding an \\(L_2\\) regularization term to the loss directly means the regularization gradient is added to the momentum state of the optimizer: the past value of the parameter now influence its weight decay, not just the current value. The overall effect here is parameters which had “large” values early in training are regularized less, defeating the point of weight decay!\nThe way modern optimization libraries such as Optax implement AdamW is by first implementing Adam’s transformation of the gradient as a seperate subroutine \\(\\text{adam}\\), that:\nNote that this is not a simplification, the Optax library has an actual function called scale_by_adam here that does exactly this.\n\ntakes in the NLL loss gradient \\(\\frac{\\partial L}{\\partial w} = g\\)\nas well as past optimizer states \\((m, v)\\)\nreturn a “transformed” gradient, an update \\(u_t\\), that is, \\(\\text{adam}(g, m, v) \\rightarrow u\\).\n\nFrom there on out, the weight decay looks just like it did before, but swapping in \\(u\\).\n\\[{\\color{YellowOrange}{w}} \\leftarrow {\\color{YellowOrange}{w}} - \\alpha \\left(u + \\frac{\\lambda}{2} \\frac{\\partial R}{\\partial \\color{YellowOrange}{w}} \\right)\\]\nWhich means, a version of our corrected (decays to 0) LoRA update that is compatible with AdamW looks like:\n\\[\\begin{align*}\n    {\\color{YellowOrange}{A}} &\\leftarrow {\\color{YellowOrange}{A}} - \\alpha u  - \\alpha \\lambda (W_{\\text{init}} + {\\color{YellowOrange}{A}}{\\color{PineGreen}{B}}) {\\color{PineGreen}{B^T}}\\\\\n    {\\color{YellowOrange}{B}} &\\leftarrow {\\color{YellowOrange}{A}} - \\alpha u - \\alpha \\lambda {\\color{PineGreen}{A^T}}(W_{\\text{init}} + {\\color{PineGreen}{A}}{\\color{YellowOrange}{B}})\n\\end{align*}\\]An alternate, equivalent way to look at this is that we use Adam on the NLL loss, and pure SGD on the \\(R(\\theta)\\) loss\nThe code snippet above (implementing the decay to 0 LoRA) is actually already compatible with AdamW in Optax. This very nice behavior comes mostly from free because of the fact AdamW in Optax is already a decomposed chain of three operators (\\(\\text{adam}\\), weight decay, and then scaling by the learning rate); Optax’s actual implementation is as follows:\n\n# from https://github.com/google-deepmind/optax/blob/master/optax/_src/alias.py#L298\nreturn combine.chain(\n      transform.scale_by_adam(\n          b1=b1, b2=b2, eps=eps, eps_root=eps_root, mu_dtype=mu_dtype),\n      transform.add_decayed_weights(weight_decay, mask),\n      _scale_by_learning_rate(learning_rate),\n  )\n\nAll we’d need to do is create a new optimizer, where we swap in the transform.add_decayed_weights with our custom version, and we’d be set."
  },
  {
    "objectID": "blog/lorawd/index.html#footnotes",
    "href": "blog/lorawd/index.html#footnotes",
    "title": "LoRA and Weight Decay",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the database query example, the \\(x\\)’s can be strings in English, and the \\(y\\)’s are then strings corresponding to the query translated from English into the query schema.↩︎\nNote that if you use a GLU-variant activation (Shazeer, 2020), then you add in a 7th “gating” weight matrix.↩︎\nThis is the precise mathematical definition of what we just described: a function whose minimization makes our finetuning training data \\({(x_i, y_i)_n}\\) “more likely” to be generated.↩︎\nIn that the weights would all be optimized to perfectly repeat \\(y_i\\) for any \\(x_i\\) in the finetuning training set, at the expense of performing much worse on any \\(x_i\\) not in the training set.↩︎\nStochastic Gradient Descent, a.k.a. the core workhorse of deep learning. In practice we use more sophisticated momentum-based methods, whose impact is described in Appendix A.↩︎\nThis directly stems from the fact that the gradient of a sum (here, the two terms are NLL and regularization) equals the sum of the gradients (of each term).↩︎\nTo reason why this is true, notice that the larger the weight, the “more” of it is subtracted away from itself.↩︎\nAssuming you’re using an optimizer with some form of momentum (vanilla SGD doesn’t need an optimizer state). It goes up to 4x for Adam, as it has two states: an exponential moving average of both the gradient means, and the gradient-squared means.↩︎\nWe’re dropping the \\(q, k,...\\) subscripts as the derivation is identical for all the weight matrices.↩︎\nWe add terms to the update, as the subtraction of the update happens at the very end.↩︎\nThis exact formulation assumes the adapters are defined inside the same layer as the original matrix; that is the params dict looks like {'params': {'kernel': ..., 'kernelA': ..., 'kernelB': ...}}. The actual implementation will depend on how the LoRA adapters have been defined (even though the underlying math will remain the same).↩︎\nWhich, in classic deep learning fashion could turn out to be wholly incorrect.↩︎\nWhich pointed out this non-equivalency, and produced a version of Adam that “decoupled” weight decay.↩︎"
  },
  {
    "objectID": "blog/resilientsystems/index.html",
    "href": "blog/resilientsystems/index.html",
    "title": "Resilience in Complex Systems",
    "section": "",
    "text": "Our everyday lives revolve around well-abstracted systems that have clear cause and effect mechanisms. I flip a switch, the light turns on; the specifics of the electricity grid are irrelevant. I keep money at the bank, the bank gives me interest. I pay for supplies at the store, now I can take them home.\nBut when it comes to changing systems, our everyday cause and effect intuition fails us. Increasing dollars spent per child at schools doesn’t seem to improve educational outcomes. (Where does the money go?). Countries that impose travel restrictions a few days before their neighbors have exponentially lower fatalities from COVID-19. Teams whose members gel well outperform teams with more accomplished individuals, but lower team cohesion.\nLinear changes produce nonlinear results. The whole is not its sum. Our intuition flies out the window.\nThis essay briefly explores these phenomena through the paradigm of systems thinking. Of particular interest are a class of systems called Complex Systems, which seem to evade reductionist, clean explanations. They’re complex, not because they’re complicated; it’s because even simple yet nonlinear interactions between their components give rise to rich behavior that cannot be understood in terms of any one individual factor (Meadows, 1982). Our goal here is to take a “whole systems” view of why many of these complex systems exhibit resilience to change, by examining their underlying dynamics; more specifically, their attractors.\nWe start by exploring attractors with the Lorenz system, then introduce causal loop diagrams as duals to differential equations, concluding with the impacts of attractors on climate change policies."
  },
  {
    "objectID": "blog/resilientsystems/index.html#the-lorenz-system",
    "href": "blog/resilientsystems/index.html#the-lorenz-system",
    "title": "Resilience in Complex Systems",
    "section": "The Lorenz System",
    "text": "The Lorenz System\nThe Lorenz System is a classic set of three differential equations. It’s a useful model, as while the dynamics of the three components can be succinctly summarized in the three equations, they give rise to highly dynamic, chaotic behavior1 that is a characteristic of many real world systems. Specifically, the equations describe the evolution over time of three variables \\(x\\), \\(y\\) and \\(z\\) (Lorenz, 1963).\n\\[\\begin{aligned}\n&\\frac{\\mathrm{d} x}{\\mathrm{d} t}=\\sigma(y-x) \\\\\n&\\frac{\\mathrm{d} y}{\\mathrm{d} t}=x(\\rho-z)-y \\\\\n&\\frac{\\mathrm{d} z}{\\mathrm{d} t}=x y-\\beta z\n\\end{aligned}\\]\nYou can think of them as representing any physical quantity of your choice (fish populations, air temperature, etc.) What’s key here is to examine how these variables interact with each other. When the system is integrated over time, we get three time series each for \\((x(t), t)\\), \\((y(t), t)\\) and \\((z(t), t)\\):\n\n\n\nAt best it just looks like three rather peculiar time series.\n\n\nHowever, this isn’t very insightful. Alternatively, we can think of the values of all three variables for a given time \\(t\\) to represent a single point \\((x(t), y(t), z(t))\\). We can then plot these points in 3D space over different values of t, from start to end:\n\nIn doing so, we can visualize the overall dynamics of the whole system over time. It’s important enough that it has its own name: the phase space, which represents all possible states of the system (in this case, all possible values of \\(x\\), \\(y\\) and \\(z\\)). The visualization here reveals a pattern: The trajectory through time for the state of the system \\((x, y, z)\\) isn’t erratic or covering all of 3D space, but a rather limited, butterfly shaped subset.\nThis could very well be an anomaly; perhaps our starting condition was special. So let’s start at 10 different points, and produce 10 different trajectories:\n\n\n\nRegardless of where the trajectory begins, it converges to moving on the butterfly attractor. Points are start points, corresponding colored lines are trajectories.\n\n\nAs we see, regardless of the starting conditions \\((x(t_0), y(t_0), z(t_0))\\), in the Lorenz system, the trajectories converge to moving on the butterfly-shaped subset of 3D space.2 This subspace has a name: the attractor manifold.3. This convergence is true of any system with an attractor: given it runs for long enough, regardless of the initial starting point, it will become limited to moving only on the attractor manifold.\nAttractors are emergent properties of a system: none of the three components in this system individually are responsible for this butterfly shaped subspace. Rather, it’s the interactions between them that give rise to the attractors. The system, as a whole, produces its own dynamics and attractors."
  },
  {
    "objectID": "blog/resilientsystems/index.html#attractors-and-feedback-loops",
    "href": "blog/resilientsystems/index.html#attractors-and-feedback-loops",
    "title": "Resilience in Complex Systems",
    "section": "Attractors and Feedback Loops",
    "text": "Attractors and Feedback Loops\nReal systems have feedback loops (Meadows, 1982) which, over time tend to balance out. This leads to the system to only move around in states that are a small subset of all possible states, that is, an attractor manifold.\nTo illustrate this, let’s use a more grounded example: suppose there is an island of rabbits and foxes, and we’d like to model their populations. We make the following simplifying assumptions:4\n\nThe amount of rabbits the island can support is limited at \\(r_{\\text{max}}\\) due to limited vegetation available as food.\nRabbits can only die from being consumed by foxes (not from natural causes).\nFoxes only eat rabbits, and nothing else.\nThis is an isolated ecosystem, with the only interaction being between rabbits and foxes.\nThe population of rabbits and foxes are continuous.\nThe feedback is instant, that is, the population of foxes begins to adjust instantly to any change in the rabbit population, and vice versa.5\n\n\nFeedback Loops\nThe dynamics of populations of foxes and rabbits can then be described as following.6 Importantly, both of these views describe the same underlying reality, just through different lenses.\n\n\n\nOn the causal loop diagram on the right, a solid arrow represents a positive relation (if A → B, if A goes up, B goes up), whereas a dashed arrow represents a negative relation (if A goes up, B goes down)\n\n\nThe differential equations perspective is useful for understanding the behavior of individual components of the system (individual populations of foxes and rabbits). However, the dynamics of the whole system are not as obvious (what’s the effect of the \\(r_{\\text{max}}\\) on the fox population?). On the other hand, what we have on the right, a causal loop diagram is clearer. We can immediately start to infer information about the system:\n\nMore rabbits results in more foxes (positive relation). However, more foxes result in fewer rabbits (negative relation). These two relations combine to form a balancing feedback loop,7 such that the populations of both stay relatively constant. 8\n\n\nIf there are more rabbits than the maximum number of rabbits that can survive on the island (\\(r &gt; r_{\\text{max}}\\)), then the difference becomes negative. Since the difference has a positive relation with the rabbit population (but the value itself is negative!), the rabbit population declines. This decline then increases the difference back to a positive value (such that \\(r &lt; r_{\\text{max}}\\)).\nWe can even glean second-order and higher order effects from the causal loop diagram!9 For instance, more foxes has a positive relation with the difference \\(r_{\\text{max}} - r\\). This is because more foxes result in fewer rabbits, and fewer rabbits pushes the difference toward a more positive value. (In general, two negative relations together form a positive relation, just like in ordinary arithmetic)\n\nYou can indeed verify that the predictions made by the causal loop diagram are consistent with what you’d obtain by analyzing the differential equations. For instance, one prediction (the dashed self loop from foxes to itself) is more foxes leads to less foxes (as they die off in larger numbers). This directly corresponds to the \\(-\\gamma f\\) term in the second equation. Each of the arrows on the right correspond to terms 10 in the differential equations on the left.\nThese loops increase and decrease in strength, until they balance each other.11 Here, this results in a steady population of foxes and rabbits. This steady state of the system of course, is our attractor, which in this case is a single point.\nIt is important to note that the feedback loops never stop! Even if the overall population remains constant, the rabbits and foxes continue to undergo birth and death. The individual components of the system remain dynamic even if the overall system stays at the attractor point.\n\n\nResilience\nIf a system is disrupted, feedback loops will work to push the system back towards the attractor. To illustrate this, suppose on day 10 a ship full of rabbits crashes onto the island. This change is represented by the blue arrow on both the time series and phase space plots:\n\nEven though the system is changed significantly, it ends up returning to its attractor! There’s some great dynamics going on here, so let’s put both our Differential Equations and Complex Systems lenses on and analyze this:\n\nThe Differential Equations lens\nThe derivatives of both rabbit and fox populations change to values with much larger magnitudes. We can see this directly on the phase plot, where the system has been pushed into a region of phase space with stronger derivatives (represented by the arrows). The population of foxes and rabbits continues to evolve according to the derivatives, until it converges back to the attractor point.\n\n\nThe Complex Systems lens\nDue to the disruption, the feedback loops become abnormally strong, with considerably more rabbits being eaten, and foxes being born that usual. However, this change isn’t permanent, as the system is returned to its attractor point by the feedback loops balancing themselves out again. Once at the attractor, the feedback loops have returned to their “usual” values, representing the typical births and deaths in the system.\n\n\n\nSummary\nAs we just saw, both differential equations and causal loop diagrams are complementary approaches. Neither is necessarily superior to, or always better than the other. Rather, both provide information about the system at different levels (differential equations at the level of individual components, causal loop diagrams at the level of the whole system) that allow us to effectively analyze the system at multiple scales at once."
  },
  {
    "objectID": "blog/resilientsystems/index.html#attractors-in-the-real-world",
    "href": "blog/resilientsystems/index.html#attractors-in-the-real-world",
    "title": "Resilience in Complex Systems",
    "section": "Attractors in the real world",
    "text": "Attractors in the real world\nIn the context of systems, both real and simulated, attractors are stability, and feedback loops are what drive them toward stability. You can push them beyond their usual states, but (within reasonable bounds) their feedback loops will drive them back to their attractors. Real systems do have some differences compared to the simplified predator-prey model above:\n\nNon-point attractors\nMany real systems do have attractors that are singular points. For instance, a pendulum has an attractor at the point of zero velocity and zero degrees from the vertical. This is because of air drag, as no matter where in the phase space of angle and velocity it starts at, it will eventually lose all velocity and end up back at zero. These attractors are easier to find, as they’re simply points where the gradient is \\(0\\).\nHowever, many complex systems have attractors that are not points.12 Indeed, for many real complex systems like the human, a point attractor is effectively death. Their attractor manifolds take on the shapes of lines, curves, loops, or as is quite common with chaotic systems, even fractals (they’re called strange attractors. The attractor of the Lorenz system is one such example). This variety means that systems continue to move on the attractor manifold, with their states changing, never reaching equilibrium.\n\nExample: Breathing\nA straightforward example is that of a human breathing: our lungs don’t converge to a “equilibrium” (that’d be terrible news!), but rather cycle through the typical states (inhale in, exhale out) to produce the breathing pattern. This continues until you temporarily disrupt the pattern with an external input (say, a friend challenges you to hold your breath, so you consciously stop).\n\n\n\nChaotic Behavior\nReal systems tend to exhibit chaotic behavior. Crucially, this is not the same as random. Chaotic means you could predict the state of the system at any future time \\(t\\) if you could measure the initial state to infinite decimal places (Mitchell, 2009). Unfortunately, we do not have infinite precision. And in chaotic systems, even points that start out extremely close (say, due to a small measurement error) diverge quickly13, resulting in highly erroneous predictions the further out we predict. We can still say the trajectory will remain on the attractor manifold over the long term, we just don’t know exactly where.\n\nExample: Weather Forecasting\nChaos is why it’s challenging to predict the weather more than a few days in advance: the weather likely follows long term, deterministic dynamics, but our measurements have finite precision. Even small errors add up exponentially quickly the further out the prediction.14 Nonetheless, we can still say the weather will remain around its typical state (say, around 20°C), instead of somehow becoming 200°C.\n\n\n\nMultiple Attractors\nReal systems tend to have multiple attractors (Ott, 2006). Points in the phase space that will converge to a specific attractor are part of the attractor’s basin of attraction. You can think of the basins as partitions of the phase space. Normally, systems remain on their current attractor manifold, unless an external force pushes them into the basin of attraction of a different attractor, at which point their dynamics will take place on the new attractor manifold.\n\n\n\nThe colored regions represent each of the three basins of attraction. The colored circles represent the attractor points themselves.\n\n\nFor example, even the simplified predator-prey system above actually has three attractors, with three corresponding basins of attraction.\n\nThe first basin of attraction was the one we saw above, with the corresponding basin being all points in phase space where the population of rabbits is greater than 0. In general, this converges to \\((\\frac{\\gamma}{\\delta}, \\frac{\\alpha(\\delta - \\gamma k)}{\\beta \\delta})\\). For the parameters we used,15 this is \\((0.33, 0.38)\\)\nThe second basin is all points where the population of rabbits is 0, in which case the number of foxes will also become zero (because there’s no food!). This always converges to \\((0, 0)\\), regardless of the starting number of foxes.\nThe third basin is all points where the population of foxes is 0. With no foxes, the rabbit population grows until it reaches the maximum the island can support, which is \\(\\frac{1}{k}\\). Our attractor is \\((\\frac{1}{k}, 0)\\), and for the parameters used this is \\((1.34, 0)\\).\n\nWhich attractor the system converges to depends on where it starts out at. But just as importantly, external forces can push the system from the basin of attraction of one attractor to a different one. If there’s no foxes, and we introduce one, we push the system from basin 3 to basin 1. Likewise, if there’s both foxes and rabbits, and we take every last rabbit off the island, we push the system from basin 1 to basin 2.\nSystems are resilient on their own, but external forces can push them too far, at which point their dynamics change, because the basin of attraction the system is in has changed!\n\nMultiple attractors and disruption\nFrom the perspective of complex systems, a disruptive technology and an ecological collapse are the same thing: the system being pushed towards a new attractor. If an external force pushes a system too far, it’ll keep balancing itself until it is pushed over a critical point, which divides two basins of attraction. Once it is pushed into the new basin of attraction, different feedback loops “take over”, causing it to converge to the other attractor.\n\nExample: climate change\nWith human-induced climate change, the earth still appears relatively stable, as even with increased CO2 emissions, the other components in the system (e.g. ocean acidity levels) act as counterbalances to keep the temperature stable. However, once the system is pushed too far, over the critical point, the system will fail (i.e. move to the new attractor) quickly without warning.\nIndeed, one hypothesis is a 4°C rise could result in clouds thinning out (Schneider et al., 2019) (Quanta Magazine has an article here). At this point, Earth would be pushed into the basin of attraction of a new attractor, where cloud loss would result in more heat being trapped, leading to further cloud loss, and so on, forming a reinforcing feedback loop, which could add an additional 8°C for a total of 12°C increase. Humans would only need to cause the first 4°C, and Earth’s new feedback loops would cause the rest.\n\n\nExample: disruptive technology\nA disruptive technology pushes the present state of the market towards the basin of attraction of a different attractor. This is also why they’re so risky: until you cross the critical point dividing the new basin and the present one, the market’s dynamics work against you, pushing the market’s state back towards the status quo attractor. (Think of every new social media app, and how quickly they’re abandoned as people realize no one else is using them).\nTruly disruptive technologies are able to cross the critical point, and at that point, the market’s dynamics become favorable. You effectively “cross over the mountains into a new valley”, so to speak. Smartphones have only been around for a little over a decade, and yet modern life is practically unimaginable without them: the technology has transitioned from “disruptive” to the “practically obvious”.\n\n\n\n\nHumans in Complex Systems\nBecause real systems have multiple attractors with highly non-linear dynamics, they can act counterintuitively to what humans expect. For instance, many expect a gradual, linear increase in average global temperatures due to global warming. However, our understanding of complex systems tells us that is the opposite of what will happen, in that many components of the system will remain as they are today before everything changes, without much warning.\n\n\n\nBecause systems fail all at once, instead of gradually, it can be challenging to reverse momentum for policies that are taking us towards dangerous critical points, because “everything seems fine”.\n\n\nThe resilience of systems is what makes earth habitable, by keeping long term conditions stable.16 But it simultaneously blinds us by not allowing us to see the total consequences of our actions until we cross over the critical point (because change isn’t linear), at which point we’ll be fighting against Earth’s dynamics instead of working with them.\n\nExample: Geoengineering\nAs the climate crisis worsens, there has been substantial discussion on whether methods such as carbon capture and dimming the skies could be viable solutions. I think it’s important to continue to research new ways to ultimately have as many options as possible. At the same time, it’s important to note that when you mess with systems, they tend to mess back.\n\n\n\nNew technologies push the critical point further to the right. However, without changes to the system’s underlying dynamics, humans adjust their behavior to compensate, eliminating the benefit\n\n\nFrom a systems lens, the mere availability of such technologies will shift the critical point of the above system further right. This is because more options will increase the level of CO2 concentration we can ultimately reverse.\nBut CO2 concentration isn’t the correct problem! It’s the underlying dynamics of unprecedented fossil fuels consumption that is. CO2 concentrations just happen to be the visible symptom. If you’re only solving for one part of a system, instead of a broader whole, you may be very surprised by the outcome!\nReturning back to the fox-rabbit island, if you increase the maximum amount of rabbits the island can support (say, permanently increase the food available) without changing the feedback loops of the system itself, the difference \\(r_{\\text{max}} - r\\) would increase, and rabbit populations would simply grow to the new maximum the island can support.\nIn the case of geoengineering, the first order effect is that maximum CO2 concentration that can be reversed will increase, which is great. However, it then gets accompanied by the second order effect of humans adjusting their behavior accordingly. (Think of every time a highway is expanded, only for the number of cars to increase as well). Indeed, a possibly dangerous outcome here is a system archetype known as “shifting the burden to the intervenor” (Meadows, 2008), where we become dependent on frequent use of geoengineering to avoid ecological collapse.\nGeoengineering can turn out to be a practical tool, but looking at the whole system, we must make sure its deployment is accompanied by changes to the underlying feedback loops17, such as a negative relation exerted on fossil fuel use by the plummeting costs of renewable energy sources.\nWe mean our technologies to do well, but technologies at the end of the day are just tools. What we really want to do, is solve problems, and we cannot do that effectively in complex systems if we fail to pay attention to second and higher order effects. Actions not only have consequences, but chains of consequences, and all those effects matter. As Hardin (1963) put it:\n\nAs concerns the basic mechanism, side-effects no more deserve the adjective “side” than does the “principal” effect. It is hard to think in terms of systems, and we eagerly warp our language to protect ourselves from the necessity of doing so. (p. 78)\n\n\n\n\nConclusion: why systems thinking?\nDoes systems thinking allow us to predict the future? Is it a theory of everything? No, and no:\n\nNo matter how well you model a system, our rationality is bounded, our measurements have errors, and our computers have finite power. There will always be some 10th order effect you’ll fail to take into account, and will completely mess up your predictions.18\nSystems thinking is one lens to view reality through. The lenses of math, physics, chemistry, neuroscience, biology, anthropology, art and culture all have value to offer, with their own strengths and weaknesses. Systems thinking is no different: it is a paradigm, not the paradigm.\n\nSo why bother with this? It’s because factoring in even second and third order effects allows us a richer model of reality than simple first order ones. Planet-level problems become tractable, as we realize inexplicable and contradictory behavior arises from fairly simple effects working together. Systems thinking allows us access to these concepts through a new lens, a shared conceptual language of feedback loops, attractors, chaos, emergence, archetypes and so forth; simply having words for them makes them easier to reason about (Meadows, 2008).\nAnd it’s because these effects are so common across a diverse range of systems, that we can rapidly transfer solutions from one domain to another, once we recognize them to have shared underlying dynamics. Nuclear arms races and price wars turn out to be two instantiations of the same dynamics. So does an opioid addiction and our reliance on fossil fuels.\nAppreciating the full complexity of the problem then, if anything, allows us to better abstract it. System archetypes become similar to design patterns in programming: as we identify new, emerging problems as having similar dynamics to previous ones, we can quickly pull from a reference library of solutions to solve them. Systems thinking, in brief, allows us to build a better tomorrow, and I hope this essay has served as a helpful starting point."
  },
  {
    "objectID": "blog/resilientsystems/index.html#further-resources",
    "href": "blog/resilientsystems/index.html#further-resources",
    "title": "Resilience in Complex Systems",
    "section": "Further Resources",
    "text": "Further Resources\n\nReadings\n\nThinking in Systems: A Primer - If you want to gain a deeper understanding of systems, this book is delightful; it was the primary inspiration for this essay. Meadows is concise (it’s 185 pages!).19 Particularly illuminatory was Chapter 5: “System Traps…and Opportunities”, where Meadows discusses eight common system archetypes, and how to resolve them.\nComplexity: A Guided Tour - This book is a deeper dive into complexity itself, the history of its study, and how it emerges in the natural world. There’s a more extensive focus here on computational aspects, such as cellular automata models and large-scale networks in systems.\nWhole Earth Models and Systems - This is an accessible essay by Meadows that serves as a fast-paced introduction to systems thinking, particularly the archetypes mentioned above.\nSeeing Whole Systems - This highly visual talk by Nicky Case contains insight on why complex systems can behave so counterintuitively.\n\n\n\nModeling and Mathematics\n\nIntroduction to Agent Based Modeling - This is the Santa Fe Institute’s (SFI) course on Agent Based Modeling, an approach commonly used to model complex systems from a bottom-up approach. ABMs have seen usage across a broad range of fields in recent years, from traffic control to economics.\nABM Libraries - In addition to NetLogo, which is used by SFI’s course, two other libraries are Mesa in Python, and Agents.jl in Julia. I’ll remark that the Julia library is considerably more integrated with the rest of that ecosystem, such as allowing you to use DifferentialEquations.jl for agents.\nComplex Systems - More technically minded readers may enjoy this journal devoted to cellular automata and discrete dynamical systems. The lens used here is primarily that of formal mathematics.\n\n\n\nAcknowledgements\nI’m grateful to Gordon Brander, Jiaying Xu and Michael Garfield for reading early drafts, providing feedback and identifying resources to deepen my own understanding. Any resource on complex systems is, almost by definition, incomplete; nonetheless, this essay is more complete than it would be otherwise thanks to their assistance.\nTo help me continue to improve the clarity of my explanations, I’d appreciate if you’d consider leaving feedback here."
  },
  {
    "objectID": "blog/resilientsystems/index.html#references",
    "href": "blog/resilientsystems/index.html#references",
    "title": "Resilience in Complex Systems",
    "section": "References",
    "text": "References\n\n\nHardin, G. (1963). The cybernetics of competition: A biologist’s view of society. Perspectives in Biology and Medicine, 7(1), 58–84.\n\n\nLorenz, E. N. (1963). Deterministic nonperiodic flow. Journal of the Atmospheric Sciences, 20, 130–141.\n\n\nMeadows, D. H. (1982). Whole earth models and systems. The Coevolution Quarterly, 98–108.\n\n\nMeadows, D. H. (2008). Thinking in systems: A primer (D. Wright, Ed.). Chelsea Green Publishing.\n\n\nMitchell, M. (2009). Complexity: A guided tour. Oxford University Press, Inc.\n\n\nOtt, E. (2006). Basin of attraction. Scholarpedia, 1(8), 1701. https://doi.org/10.4249/scholarpedia.1701\n\n\nSchneider, T., Kaul, C. M., & Pressel, K. G. (2019). Possible climate transitions from breakup of stratocumulus decks under greenhouse warming. Nature Geoscience, 12(3), 163–167. https://doi.org/10.1038/s41561-019-0310-1"
  },
  {
    "objectID": "blog/resilientsystems/index.html#footnotes",
    "href": "blog/resilientsystems/index.html#footnotes",
    "title": "Resilience in Complex Systems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnder specific parameters for the equations, that is. The most commonly used ones, and the ones we use here are σ=10, β=8/3 and ρ=28.↩︎\nFor a visual representation, do see this, using the “large cube” option. The “hide curves” option is particularly illuminatory.\nNotice how for many different random starting locations, the points (represented by butterflies) ultimately converge to moving on the attractor manifold, instead of all of 3D space.↩︎\nIt’s helpful to think of manifolds as “curvy objects” in a higher dimensional space. For example, a balloon is 3D object, even though it really has a 2D surface that exists in 3D space.↩︎\nExplicitly listing assumptions in your models and methods is a practice I recommend. It’s easier to “debug” bad assumptions when you know what they are, as obvious as that sounds.↩︎\nI’d argue this is an even more unrealistic assumption than assuming the values are continuous. Every real system has delayed feedback. Stock markets are volatile because they’re acting on past information, even from a few seconds ago. Climate policies proceed at a glacial pace since the feedback from the environment takes years.\nDelay is at the core of real systems, and I’d like to cover it more extensively in the future. But for curious readers: if you’re using a top-down model like differential equations, you can use Delay Differential Equations. If you’re using a bottom-up model, Agent Based Modeling techniques are flexible enough that you can integrate delays into the ruleset of agents, such as updating each agent at random time intervals.↩︎\nSome readers may recognize these differential equations as modified versions of the Lotka-Volterra equations. They indeed are!↩︎\nAlso called a negative feedback loop, as the two relations work in opposite directions.↩︎\nThe other type of feedback loop is a reinforcing feedback loop (also called a positive feedback loop), which is when two relations strengthen each other.\nThis can happen with either two positive relations or two negative relations. In both cases, it’s called a positive feedback loop as the two relations continually strengthen the magnitude each other’s feedback loops i.e. move in the same direction.↩︎\nAnd this is important, as if you only look at first order effects, some dynamics can seem downright bizarre.\nFor example, can harsher penalties for drug possession lead to more deaths? This seems counterintuitive. However, you notice the intermediate link is drugs becoming more concentrated, increasing the risk of an overdose as well. At that point, the causal chain seems plausible if not obvious.↩︎\nThis can be direct, as in the case of the death of foxes being −γf. This can also be indirect depending on how we’ve written the equations (for example, the rmax − r corresponds to (1-kr) in the first equation).↩︎\nTo understand why this is true, remember that when the rabbit population increases, so does the foxes, driving the rabbit population back down.\nEven though many systems have components with reinforcing feedback loops, all real systems are ultimately bounded overall by balancing feedback loops, because the world is finite.\n(e.g. even though pandemics spread exponentially at first, they slow down as there’s only a finite number of people left to infect).↩︎\nAnd as we’ll see below, since systems can have multiple attractors, they can have a mix of both!\nFor say, humans, the non-point attractor would correspond to the everyday states of being alive, whereas the point attractor would be the equilibrium state of rigor mortis.↩︎\nSee this for a nice illustration of the concept using the Lorenz.↩︎\nThis is also why I entirely ignore 10 day weather forecasts. With current technologies, we can’t even predict a hurricane more than 5 days out!↩︎\nα=1, β=2, δ=1.5, γ=0.5, k=0.75. There’s nothing particularly special about these parameters, just that I wanted to produce a concrete, useful visualization instead of just neat looking algebra.↩︎\nOne balancing loop is the oceans absorbing CO2 and becoming more acidic as a counterbalance to more CO2 in the air. This can only go for so long before we have both the atmosphere and the ocean in jeopardy.↩︎\nFrom the lens of causal loop diagrams, changing the loops means either:\n\nthe creation or removal of new arrows on the diagram.\nchanging the baseline strength of existing arrows on the diagram (before the quantities are factored in). For example, that would entail changing δ in the equations above, which governs how much the fox population increases for a small increase in the rabbit population.\n\n↩︎\nI certainly couldn’t have anticipated in summer 2019 that a virus that’d be detected that December would completely upend my life three months after that. No one did!↩︎\nI truly cannot exaggerate how delightful this book is. I’m rather averse to most non-fiction books (they really could just be short essays), and yet this book had me engaged start to finish.↩︎"
  },
  {
    "objectID": "blog/pjit/index.html",
    "href": "blog/pjit/index.html",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "",
    "text": "As of this writing in late-2022, large language models (LLMs) can now easily exceed 10B+ parameters (and the largest at 100B+ parameters). Pure data parallel strategies are no longer viable as the model itself no longer fits on single devices. Fortunately, research and engineering in scaling them have not slowed down; in the JAX (Bradbury et al., 2018) ecosystem in particular we now have pjit, enabling an orthogonal way to parallelize models called tensor parallelism. In this post, we’ll explore the mathematical underpinnings of tensor parallelism, and learn how to implement it for a 15B param language model using pjit."
  },
  {
    "objectID": "blog/pjit/index.html#intro-parallelism",
    "href": "blog/pjit/index.html#intro-parallelism",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Intro: Parallelism",
    "text": "Intro: Parallelism\n\nData Parallelism\nUntil recently, large scale training of deep learning models have primarily used data parallelism:\n\nEach device stores a full copy of the model, and receives a “shard” of the batch (if the full batch is 8 training examples, split along 2 devices each device receives 4 examples).\nEach device independently computes the loss, and its gradient (w.r.t. the parameters) using its data shard.\nOnly once during each step, they synchronize their gradients and update their own copy of the model.\n\nAs long as a full copy of the model1 fits on device, this general strategy can scale gracefully to the typical maximum of 8 GPUs on a single host, and was the likely strategy used to train the “big” (213 million params) Transformer with in the original Attention is All You Need (Vaswani et al., 2017, p. 7) paper 2. Properly optimized, data parallelism scales to even hundreds of GPUs on multiple hosts.\nHowever, data parallelism isn’t enough when the model itself can no longer fit on a single device. This is where model parallelism comes in.\n\n\nTensor Parallelism\nModel parallelism is when we split the model itself across multiple devices. Tensor Parallelism (“sharding”) is one of two ways to do this; the other is Pipeline Parallelism (“pipelining”). The latter is briefly discussed at the end, but the focus here really is on the former.\nTensor parallelism is the answer to this question: what if we could compute the activations of every layer of our model, distributed across all our devices?\nSuppose we have 4 devices: with standard data parallelism we make each device compute all the embedding dimensions for 1/4th of the batch:\n\n\n\n\n\nBut perhaps we could make each device compute 1/4th the embedding dimensions for the entire batch, like this:\n\n\n\n\n\nEven more: instead of sharding on one axis, we could shard both axes. What if we arranged these 4 devices in a \\(2\\times2\\) mesh, such that the first (top left) device computed 1/2 the embedding dimensions for 1/2 the batch?\n\n\n\n\n\nThis is the big idea behind tensor parallelism: arranging our devices into a 2D mesh, and then sharding both our weights and activations on both axes, for all the layers. That is, each device holds a single “shard” of every layer in the model. When done properly, it is possible to run calculations with only one full copy of the model distributed across all the devices.\nWe’ll start ground up: at the level of the dot products themselves, and see how sharding allows us to do very large matrix multiplies, by trading off increased communication for reduced memory use. Then, we’ll scale it up to a full model in JAX, implementing sharding with pjit on a 15B language model for inference, with focus on the exact code changes, keeping them minimal."
  },
  {
    "objectID": "blog/pjit/index.html#intro-dot-products",
    "href": "blog/pjit/index.html#intro-dot-products",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Intro: Dot products",
    "text": "Intro: Dot products\nLet’s start with an observation: any dot product between two vectors can be broken down into the sum of multiple smaller dot products. Suppose:\n\\[\na = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n2 \\\\\n-1\n\\end{bmatrix}, b = \\begin{bmatrix}\n-1 \\\\\n2 \\\\\n0 \\\\\n2\n\\end{bmatrix}\n\\]\nThen, the dot product of these two vectors of length 4 is \\[a \\cdot b = (1 \\times -1) + (0 \\times 2) + (2 \\times 0) + (-1 \\times 2) = -3\\]\nBut we could easily re-write that expanded calculation as \\[\\textcolor{BurntOrange}{\\underbrace{[(1 \\times -1) + (0 \\times 2)]}_\\text{-1}} + \\textcolor{Plum}{\\underbrace{[(2 \\times 0) + (-1 \\times 2)]}_\\text{-2}}\\]\nEach of these two terms individually is also a dot product of two vectors of length 2. Recoloring the original vectors, we can imagine them as composed of two “partitioned”-vectors:\n\\[\na = \\begin{bmatrix}\n\\textcolor{BurntOrange}{1} \\\\\n\\textcolor{BurntOrange}{0} \\\\\n\\textcolor{Plum}{2} \\\\\n\\textcolor{Plum}{-1}\n\\end{bmatrix} \\;\\; b = \\begin{bmatrix}\n\\textcolor{BurntOrange}{-1} \\\\\n\\textcolor{BurntOrange}{2} \\\\\n\\textcolor{Plum}{0} \\\\\n\\textcolor{Plum}{2}\n\\end{bmatrix}\n\\]\nNow, say I wanted my friend to help out with this tedious calculation. If I calculated the dot product with the first partition of each vector (getting back \\(\\textcolor{BurntOrange}{-1}\\)), they’d only need to return the result (\\(\\textcolor{Plum}{-2}\\)) of their partition (and not their entire sub-vectors) for me to calculate the full dot product, \\((-1)+(-2)=-3\\)."
  },
  {
    "objectID": "blog/pjit/index.html#intro-matrix-multiplies",
    "href": "blog/pjit/index.html#intro-matrix-multiplies",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Intro: Matrix multiplies",
    "text": "Intro: Matrix multiplies\nLet’s build on this with another observation: In a matrix multiply \\(AB=C\\), \\(C\\) is simply a storage mechanism for the pairwise dot-products of all the (row) vectors of \\(A\\) and (column) vectors of \\(B\\)3. Specifically, let:\n\\[A = \\begin{bmatrix}\n\\textcolor{LimeGreen}{1} & \\textcolor{LimeGreen}{0} & \\textcolor{LimeGreen}{2} & \\textcolor{LimeGreen}{-1} \\\\\n2 & 1 & 0 & -2\n\\end{bmatrix} \\;\\; B = \\begin{bmatrix}\n0 & \\textcolor{LimeGreen}{-1} \\\\\n1 & \\textcolor{LimeGreen}{2} \\\\\n2 & \\textcolor{LimeGreen}{0} \\\\\n0 & \\textcolor{LimeGreen}{2}\n\\end{bmatrix}\\;\\; AB = C = \\begin{bmatrix}\n4 & \\textcolor{LimeGreen}{-3} \\\\\n1 & -4\n\\end{bmatrix}\n\\]\n\\(A\\)’s first row vector and \\(B\\)’s second column vector should seem familiar: we just took their dot products. And as expected, the element of the first row, second column of \\(C\\) is that dot product \\(-3\\). This perspective also neatly explains two facts about matrix multiplication:\n\nWhy \\(C\\) is a \\(2 \\times 2\\) matrix: \\(A\\) has two row vectors, and \\(B\\) has two column vectors, resulting in a \\(2 \\times 2\\) matrix to capture all the pairwise dot products. (Likewise, if \\(A\\) had \\(3\\) row vectors, \\(C\\) would be of shape \\(3 \\times 2\\)).\nWhy the “inner axes” (\\(A\\) being \\(2 \\times \\textcolor{LimeGreen}{4}\\), \\(B\\) being \\(\\textcolor{LimeGreen}{4} \\times 2\\)) have to match: we can’t take dot products of vectors of different lengths. Take note of this “inner axes” terminology, we’re about to build on this right now!\n\nBoth combined, we have the general rule for the shapes: \\(\\underbrace{A}_{n\\times d} \\underbrace{B}_{d\\times m} = \\underbrace{C}_{n\\times m}\\)"
  },
  {
    "objectID": "blog/pjit/index.html#sharding-a-matrix-multiply",
    "href": "blog/pjit/index.html#sharding-a-matrix-multiply",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Sharding: A Matrix Multiply",
    "text": "Sharding: A Matrix Multiply\nThe vast majority of compute time in deep neural networks is spent on the matrix multiplies (matmuls) between data (activation) matrices and weight matrices. We focus on a single matmul in this section, building up two cases (1A, 1B and 2), that together can explain every possible sharding pattern.\nLet’s start:\n\nSuppose we now have a new \\(A\\) matrix of shape \\(4 \\times 16\\), and a \\(B\\) matrix of shape \\(16 \\times 4\\). That is, each has 4 vectors of length 16 each. We want to compute \\(C=AB\\), where \\(C\\) will be a \\(4\\times4\\) matrix.\n\n\n\n\n\n\n\nAlso suppose we have 8 GPUs/TPUs, which are much slower at multiplying matrices than adding matrices4. We arrange these 8 devices in a \\(2\\times4\\) mesh.\n\nOur goal here is to do as little compute as possible here: in the final version, we should split up this large matrix multiply such that we only compute one full copy of \\(C=AB\\) split across all 8 devices.\n\nCase 1: Inner Axes\nFor now, let’s try a version of the problem that may be easier to reason about: split up \\(AB\\) such that each device has one full copy of \\(C=AB\\).\nTo get started, we can partition both matrices, just as we previously partitioned vectors. Note that unlike vectors, with matrices we have multiple axes we can “slice” on. For now, let’s use this pattern, where each “sub-matrix” is a \\(4\\times 4\\) matrix:\n\n\n\n\n\nSince \\(A\\) is the first matrix (row vectors) and \\(B\\) is the second matrix (column vectors) in the matmul \\(AB\\), we’re effectively partitioning along the “inner axes”; cutting each vector into multiple sub-vectors, just as in the dot product example. To make this clearer, we might write down this “sharding configuration” in pseudocode as:\n\n\\(A\\): (full, sharded)\n\\(B\\): (sharded, full)\n\nContinuing this reasoning, just as we decomposed the large dot product into the sum of multiple smaller dot products, we can decompose this larger matrix multiply into the sum of smaller matrix multiplies. Specifically, \\(AB = A_1B_1+A_2B_2+A_3B_3 +A_4B_4= C_1 + C_2 + C_3+C_4\\).\nEach of these terms is the pairwise dot product of a subset of \\(A\\) and \\(B\\)’s vectors’ feature dimensions; \\(C_1 = A_1B_1\\) is the matrix holding the pairwise dot products of the vectors’ first four dimensions, \\(C_2 = A_2B_2\\) is the second four, and so on. Summing the four \\(C_i\\) matrices, we have the pairwise dot products computed along all the vector dimensions.\nThis result implies something curious: to multiply a \\(4 \\times 16\\) and \\(16 \\times 4\\) matrix, we don’t need to do it in one go; we can just do \\(4 \\times 4\\) multiplies and add the results. If we did these multiplies on different devices, that’s a neat way of speeding up the multiplication, and that’s the idea at the heart of tensor parallelism. There’s two possible cases here: when the sharding axes “match”, and when they “mismatch”, both of which will appear in the full neural network we’ll examine.\n\nCase 1A: Mesh-axes match\nSince we have a \\(2\\times4\\) device mesh, one way to partition \\(A\\) and \\(B\\) onto this mesh is the following way:\n\n\n\n\n\n\n\nIn technical terms, \\(A\\)’s column dimensions and \\(B\\)’s row dimensions are sharded along the Y axis. We could write our sharding config now as:\n\n\\(A\\): (full, sharded_Y)\n\\(B\\): (sharded_Y, full)\n\nThis config concisely captures how the matrices are divided across this mesh. For instance, on the four devices where X=0 (the top row), we have all of \\(A\\)’s rows (first axis), and the second shard of \\(A\\)’s columns (the second axis; here, the second 4 dimensions). A full copy of \\(A\\) then is distributed across Y=0,1,2,3.\nHowever, there’s duplication: since we’re not sharding on X, it means there are two full groupings with Y=0,1,2,3, each with a full copy of \\(A\\). The same reasoning applies to \\(B\\) (as we see in the diagram), but with rows and columns reversed.\n\nMultiply\nNow, each device multiplies the shard of \\(A\\) and \\(B\\), to produce part of the multiplied value \\(C\\). For example, the device at X=0, Y=1 on the mesh produces \\(C_2 = A_2B_2\\).\n\n\n\n\n\nNote that the duplication over the X axis results in both duplicate memory use and computation: for instance, both X=0, Y=0 and X=1, Y=0 are performing the same calculation.\n\n\nAllReduce\nAll these matmuls are only fragments of \\(C\\): they have the same shape as \\(C\\), but each only contains pairwise dot products of parts of \\(A\\) and \\(B\\)’s vectors. To calculate the full \\(C\\), the devices can “pull” the other necessary values from its neighbors and then add them to its own calculation. These collective ops are well known, and this specific one is called AllReduce5. Let’s visualize this from X=0, Y=1’s perspective (but remember, for now and the rest of this post, in actuality collective ops are performed by all devices at the same time):\n\n\n\n\n\nTo recap, our starting scenario was one where each device had to perform this \\((4\\times 16) \\times (16 \\times 4)\\) matrix multiply to get a copy of the final result \\(C\\). With sharding, each device only had to do one \\((4\\times 4) \\times (4\\times 4)\\) matrix multiply, and sum together the partial results from its neighbors.\nAnd here’s the real lesson: this “shard \\(\\rightarrow\\) multiply \\(\\rightarrow\\) allreduce” strategy scales to far larger matrices! If in the self-attention block of a Transformer we had \\(Q\\) and \\(K\\) matrices of shape \\(512 \\times 4096\\) each (\\(512\\) vectors, each of length \\(4096\\)), then getting the “match” between every pair of queries and keys \\(QK^T\\) is a \\((512 \\times 4096) \\times\\) \\((4096 \\times 512)\\) matrix multiply.\nWith this strategy, by splitting the inner axes along the mesh Y axis’s 4 devices, each device in the group of 4 only has to do a \\((512 \\times 1024) \\times\\) \\((1024 \\times 512)\\) multiply (that is, pairwise dot products with just 1024 of 4096 dimensions each), which is about 4x faster6.\nBut we’re still duplicating work along the X axis; we’ll see shortly how with Case 2, this can be this 8x faster, leveraging all 8 devices.\n\n\n\nCase 1B: Mesh-axes mismatch\nLet’s try the above problem (each device obtains a full copy of \\(C\\)), with a modified sharding config:\n\n\\(A\\): (full, sharded_Y)\n\\(B\\): (sharded_X, full)\n\nThat is, we now shard \\(B\\)’s first axis on X, not Y. Visualized, the mesh now looks like this:\n\n\n\n\n\nThis doesn’t appear to be a very useful mesh: we now have 4 copies of \\(B\\) over the mesh, compared to the previous two. Worse, because the sharding axes mismatch, we can’t actually multiply: \\(A_1\\) is of shape \\(4\\times4\\) (since 16 divided on 4 devices), and \\(B_1\\) is of shape \\(8 \\times 4\\) (16 divided on 2 devices). That is, their number of inner dimensions don’t match: each device has twice as many dimensions of \\(B\\)’s (column) vectors than they have of \\(A\\)’s (row) vectors.\nThis inability to multiply will be true in general: assume we start off with a valid matmul (inner axes match in number of dimensions), we’re dividing this inner axis length by the number of devices on the corresponding mesh axis, and that may not be the same. Here, \\(A\\) does \\(\\frac{16}{4}=4\\) due to the four devices on Y, while \\(B\\) does \\(\\frac{16}{2}=8\\) due to the two devices on X.\nBut this is fine: we can use another collective op called AllGather, to restore the full matrices on each device. Visualizing this on X=0, Y=1 (and remembering that in reality all devices are doing this at the same time):\n\n\n\n\n\nOnce each device restores a full copy of \\(A\\) and \\(B\\), they multiply it as normal to directly get \\(C\\). Now, this may seem odd: why’d we shard the matrix, only to gather it (then multiply) in full on every single device? (that is, no compute benefit, since every device does the same calculation).\nBecause it turns out when we combine Case 1B with Case 2, we can quite cleanly eliminate this duplicate computation. Let’s see how.\n\n\n\nCase 2: Outer Axes\nWe’ve focused on sharding on the “inner axes” of a matrix multiply so far, but there’s nothing stopping us from sharding the outer axes; if anything, it’s easier! Building on prior examples, let’s try this sharding pattern:\n\n\\(A\\): (sharded_X, sharded_Y)\n\\(B\\): (sharded_X, full)\n\nVisualizing, we have the following:\n\n\n\n\nSince there’s a lot of partitions this point on, it’s easier to keep track of them by keeping track of their “colored in” areas than naming them individually.\n\n\n\nThis is still the same as case 1B, with one change: A’s outer axis is now sharded on X. This means that devices with X=0 contain shards of the first 2 (of 4) row vectors, whereas devices where X=1 contain shards of the second 2 (of 4) vectors. The feature dimensions of these vectors remain sharded over Y.7 We now only have one full copy of \\(A\\), fully distributed over the entire \\(2\\times4\\) mesh.\nWe now proceed as previously with case 1B, allgathering \\(A\\) and \\(B\\) on their inner axes. Visualizing from the perspective of X=0,Y=1:\n\n\n\n\n\nNow we multiply the first 2 (of 4) row vectors of \\(A\\) with the 4 column vectors of \\(B\\). To store these pairwise dot products, we end up creating a \\(2\\times4\\) matrix, that is, the top half of the (final) values of \\(C\\):\n\n\n\n\n\nOnce all 8 devices do their respective allgathers and multiplies, we have:\n\n\n\n\n\nThis should feel familiar! If \\(A\\) is a batch of data (what we’d typically call \\(X\\)), with individual training examples as row vectors, then this is just batch parallelism: the two “submeshes” (corresponding to X=0 and X=1) are each processing half of this batch, and then store the half of \\(C\\) that corresponds to the shard of \\(A\\) it processed.\nBut there’s still duplicate computation going on: all four devices in the submesh (X=0 or X=1) are doing the same work!\n\n\nFull Sharding\nWhat if we also sharded \\(B\\) along its outer axis? That is, we shard both \\(X\\) and \\(Y\\) over X and Y, with the following sharding pattern:\n\n\\(A\\): (sharded_X, sharded_Y)\n\\(B\\): (sharded_X, sharded_Y)\n\nVisualized, we see that we only have one copy of \\(A\\) and \\(B\\) distributed over the whole mesh:\n\n\n\n\n\n\n\nSince this sharding spec produces a mismatch along the inner axes, using case 1B, we do an all gather. However, this time as we’ve also sharded both the outer axes, X=0,Y=1 will only be processing the full pairwise dot products of the first shard (first 2 row vectors) of \\(A\\), with the second shard (the 2nd column vector) of \\(B\\).\n\n\n\n\n\nAnd then, a multiply, resulting in a \\(2 \\times 1\\) shard of the final output:\n\n\n\n\n\nZooming out, we see all 8 devices have computed a unique part of the output. Even more wonderfully, \\(C\\) has the same sharding pattern as \\(A\\) and \\(B\\), (sharded_X, sharded_Y):"
  },
  {
    "objectID": "blog/pjit/index.html#sharding-gspmd-style",
    "href": "blog/pjit/index.html#sharding-gspmd-style",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Sharding: GSPMD-style",
    "text": "Sharding: GSPMD-style\n The GSPMD paper (Xu et al., 2021) introduced an optimal sharding pattern for Transformers, subsequently used to train PaLM (Chowdhery et al., 2022). To reinforce our understanding of the two cases above, we’ll now look at the sharding spec for the feedforward network in the Transformer layer. But before that, one final detour:GSPMD is the parallelization system that enables pjit, implemented as an extension to XLA (JAX’s compiler).\n\nWhat is \\(XW\\)?\nDeep learning models at their core are straightforward: a number of layers stacked on top of each other. The core building block is the fully connected layer, often written as \\(\\sigma(XW + b)\\). It’s really three operations:\n\n\\(XW\\)8 is a matrix multiply, where \\(X\\) is a matrix of datapoints as row vectors, and \\(W\\) is a weight matrix.\n\\(b\\) is a vector of biases we add to \\(XW\\).\n\\(\\sigma\\) is a non-linearity of our choice, often ReLU but more recently enhanced variants such as GeLU in Transformer models.\n\nWe know what it means to shard the data \\(X\\) along its outer axis over X: it’s just splitting the batch into yet smaller batches, standard data parallelism. But what does it mean to shard \\(W\\) along its outer axis, over Y? To understand that, let’s dive deeper: what is \\(XW\\)?\nWell, it’s just a storage mechanism for the dot products of \\(X\\)’s row vectors, and \\(W\\)’s column vectors. We can directly reuse the math at the start of this post, replacing \\(A\\) and \\(B\\) with \\(X\\) and \\(W\\):\n\\[X = \\begin{bmatrix}\n\\textcolor{LimeGreen}{1} & \\textcolor{LimeGreen}{0} & \\textcolor{LimeGreen}{2} & \\textcolor{LimeGreen}{-1} \\\\\n2 & 1 & 0 & -2\n\\end{bmatrix} \\;\\; W = \\begin{bmatrix}\n0 & \\textcolor{LimeGreen}{-1} \\\\\n1 & \\textcolor{LimeGreen}{2} \\\\\n2 & \\textcolor{LimeGreen}{0} \\\\\n0 & \\textcolor{LimeGreen}{2}\n\\end{bmatrix}\\;\\; XW = \\begin{bmatrix}\n4 & \\textcolor{LimeGreen}{-3} \\\\\n1 & -4\n\\end{bmatrix}\n\\]\nThe first row of the output matrix, stores the dot products of the first row vector of \\(X\\) with all of the column vectors of \\(W\\). Since the dot product calculates how much two vectors “match” 9, the new output row vector (of the \\(XW\\)) stores how much the original row vector (of \\(X\\)) matches with each of the \\(W\\) weight vectors. Since we have two weight vectors, the output row vectors have two feature dimensions; if we have 20,000 weight vectors, they’d have 20,000 feature dimensions!\nThis output, once passed through a nonlinearity are the new data vectors (the “activations”), which will then be multiplied with the next layer’s weight matrix to produce yet newer activations. This really is what we mean by stacking layers on top of each other: using the feature matching scores of one layer as the data to the next layer.\nAnswering the question, to shard \\(W\\)’s outer axis along Y is to ask each device with a given Y value to compute the dot products with a subset of \\(W\\)’s column vectors. We can do this since the dot product of a data vector with a weight vector doesn’t depend on any other weight vectors. This means on a \\(2\\times4\\) mesh with Y=0,1,2,3, devices with Y=0 can calculate dot products with the first 1/4th of the weight vectors, devices with Y=1 the second 1/4th, and so on. This is exactly what we see in the calculation of \\(C\\) in the previous section (again, replace \\(A\\) and \\(B\\) with \\(X\\) and \\(W\\))\n\n\nGSPMD’s sharding spec\nTo recap, in the original Transformer (and most variants since), “Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.” (A. Huang et al., 2022) Let’s look at the GSPMD paper’s proposed sharding spec for the second sub-layer, the feedforward network (FFN):10\n\n\n\nFig. 7 from the GSPMD paper, modified with data \\(X\\) on the left (weight \\(W_{\\text{in}}\\) on the right) to emphasize the \\(XW_{\\text{in}}\\) matmul\n\n\nThe FFN is made of two fully-connected layers (the \\(\\sigma(XW + b)\\) we just discussed). Breaking this down:\n\nThe first multiplies a weight (\\(W_{\\text{in}}\\), sharded as \\(\\textcolor{Maroon}{m_x}\\textcolor{Blue}{h_y}\\)) to transform the input features (the original embeddings, sharded as \\(\\textcolor{Maroon}{b_x}S\\textcolor{Blue}{m_y}\\)) into the features of the “hidden layer” (sharded as \\(\\textcolor{Maroon}{b_x}S\\textcolor{Blue}{h_y}\\)). In most implementations, the hidden layer has 4x the number of features of the input/outputs of the FFN.\nThe second multiplies a weight (\\(W_{\\text{out}}\\), sharded as \\(\\textcolor{Blue}{h_y}\\textcolor{Maroon}{m_x}\\)) to transform the features of the “hidden layer” (sharded as \\(\\textcolor{Maroon}{b_x}S\\textcolor{Blue}{h_y}\\)) into the output features (the new embeddings, sharded as \\(\\textcolor{Maroon}{b_x}S\\textcolor{Blue}{m_y}\\)). The number of output features are the same as the input features, but the features themselves are different!\n\nLet’s zoom in on this, looking at each of these multiplies (the einsums in the diagram) separately:\n\nEmbed \\(\\rightarrow\\) Hidden\nTo make things concrete, let’s say we have \\(8\\) sequences, each of length \\(512\\), and an embedding dimension of \\(5120\\). That is, the shape of \\(X\\) is \\((B \\times S \\times M)\\)\\(=(8 \\times 512 \\times 5120)\\). Since we want 4x the number of features for the hidden layer, we’ll need \\(20480\\) weight vectors of length equalling the embedding dimension \\(5120\\); hence \\(W_{\\text{in}}\\) is of shape \\((M \\times H)\\)\\(= (5120 \\times 20480)\\).\nThe sharding spec for the data \\(X\\) and weights \\(W\\), in the concise form in the diagram are \\(\\textcolor{Maroon}{b_x}S\\textcolor{Blue}{m_y}\\), and \\(\\textcolor{Maroon}{m_x}\\textcolor{Blue}{h_y}\\). We can rewrite this more verbosely as:\n\n\\(X\\): (shard_X, full, shard_Y)  # (batch, seq_len, embed)\n\\(W\\): (shard_X, shard_Y)  # (embed, hidden)\n\nNote that we don’t shard along the sequence length axis. This means, for instance, device X=0, Y=1 has:\n\n\n\n\n\n\nthe second of four slices (the second \\(1280\\) of \\(5120\\)) along the embedding dimension\nof all the timesteps\nof the first of two slices (the first \\(4\\) of \\(8\\)) along the batch dimension.\n\nThis sharding pattern should look familiar: this is Case 2 (since the outer axes are sharded) combined with Case 1B (since the inner axes are sharded on mismatched mesh axes), again! Visualizing again from the perspective of X=0, Y=1:\n\n\n\n\n\nAs previously in Case 1B, we allgather the first matrix (here, \\(X\\)) along Y, and the second matrix (here, \\(W_{\\text{in}}\\)) along X. Once allgathered, both matrices have their inner axes = \\(M = 5120\\) dimensions.\n\n\n\n\n\nWe leave the outer axes sharded: this means we’re only multiplying the embeddings of the first 4 (of 8) sequences in the batch. We’re multiplying all the input dimensions, but only computing the second (of four slices) of the hidden dimensions, that is, dimensions (5120, 10239) of \\(20480\\) dimensions.\n\n\n\n\n\nAll 8 devices then compute a unique shard of the output (corresponding to a different batch slice along X, and hidden dimension slice along Y), successfully avoiding any duplications.\n\n\nHidden \\(\\rightarrow\\) Embed\nSo far, it seems like we’ve only been using Case 1B and Case 2. But here, the sharding spec for the data and weight, in the concise form are \\(\\textcolor{Maroon}{b_x}S\\textcolor{Blue}{h_y}\\) and \\(\\textcolor{Blue}{h_y}\\textcolor{Maroon}{m_x}\\). In verbose form:\n\n\\(X\\): (shard_X, full, shard_Y)  # (batch, seq_len, hidden)\n\\(W\\): (shard_Y, shard_X)  # (hidden, embed)\n\n\n\n\n\n\nThat is, the inner axes are sharded on the same mesh axis, which is case 1A. This means we can directly multiply on-device (without having to allgather over the inner axes), and then run allreduce to get the final output. There is one problem though: when both matrices are fully sharded over a 2D mesh, and the mesh axes of their inner axes match, the mesh axes of their outer axes also match.\nThis means if we multiply the shards directly, we’ll have the opposite problem of duplicate work: incomplete work. Specifically, since both the outer axis of \\(X\\) (batch) and \\(W_{\\text{out}}\\) (embed) are sharded on X, for the first batch slice on the X=0 submesh we’ll only compute the dot product with the first half of the weight vectors (on X=1, the dot products of the second batch slice with the second half of the weight vectors).\nWe don’t have any devices computing the dot products of the first batch slice with the second half of weight vectors (and vice versa)!\nTo fix this, we need to allgather either \\(X\\) or \\(W\\) across their outer axis. Generally, we prefer to keep the batch axis sharded, so we allgather along \\(W\\)’s outer axis. This means each device now computes:\n\nthe dot product over 1/4th of the hidden dimensions\nfor 1/2 of the batch\nfor all of the weight vectors.\n\n\n\n\n\n\nThen multiplying, each device along X=0 produces part of the dot products for all the embedding dimensions (case 1A), for the first batch shard (case 2):\n\n\n\n\n\nAt this point, we have two options: we can either proceed with an allreduce, as previously covered in case 1A. Or, we can notice what we want is to produce an output tensor with the same sharding pattern as the input to the entire FFN subnetwork. In this case, we can use a different collective op called reduce-scatter. Unlike allreduce, which sums up values across all devices for the entire shard, this sums up values only for the smaller part of the shard we want:\n\n\n\n\n\n\n\nWrapping it up\nWe did it! As we see, Case 2, combined with Case 1A or 1B as appropriate allowed us to stack two fully sharded, fully connected layers on top of each other, and produce an output with the exact same sharding as the input (no resharding needed!).\nWe won’t cover the sharding of the self-attention sublayer here, but you should be able to combine the two cases here, with Table 1 in the GSPMD paper to work out how the attention heads are sharded over the Y axis11."
  },
  {
    "objectID": "blog/pjit/index.html#the-pjit-programming-model",
    "href": "blog/pjit/index.html#the-pjit-programming-model",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "The pjit programming model",
    "text": "The pjit programming model\nA good way to think of pjit is a supercharged jax.pmap. If you recall, pmap runs the same program on multiple devices, each with a different shard of input data over the batch axis. pjit is more flexible: it allows us to shard both the data and weights (and when training, even the optimizer states) in whatever configuration we please over a mesh. To do so, pjit requires three things from us:\n\nA mesh specification, mapping the “logical” devices on the 2D (or higher-D) mesh to the physical devices available.\nThe sharding spec of all tensors being passed as input to, and returned as output to from the function.\nSharding constraints for select intermediate tensors inside the function. This isn’t strictly necessary (XLA GSPMD will try to find a viable layout), but can lead to improved memory usage.\n\nNote what isn’t here: JAX doesn’t need us to insert any of the collective ops we discussed. It uses a constraint based model, where we specify sharding constraints for the “big”, memory intensive tensors, and it automatically determines the sharding pattern for all other intermediate tensors in the function, as well as any collective ops that need to be inserted to meet these constraints.\n\nSharding constraints\nSo how do we specify a sharding constraint? Well, we write our function as we normally would, and then call with_sharding_constraint on the tensor we’d like to place the constraint on: Note this is flax’s version of the method, which comes with nice add-ons such as checking if we’re inside a pjit and rule translation. jax has a more direct version here.\n\nfrom flax.linen import partitioning as nn_partitioning\n\ndef forward(x):\n    # ...run some code here.\n    # note x here has shape [batch x seq_len x embed]\n    x = nn_partitioning.with_sharding_constraint(x, (\"batch\", None, \"embed\"))\n    # ...continue more code here.\n    return x\n\nWhen not inside a pjit, this is a no-op, returning the original input unchanged. Inside a pjit, XLA will insert any collective ops needed to meet the constraints we specified in the second argument.\nThe second argument here specifies the mesh axis to shard each of the axes of this rank-3 tensor over. The None in the middle means “do not shard over the sequence length axis”. But note that the other two don’t have X or Y, but “names” (batch and embed). This is because at runtime, we’ll use a set of rules mapping these from names to the mesh axes. For instance, here’s the one I use for the ESM2 models on a \\(2 \\times 4\\) mesh:\n\n# Create 2D TPU mesh\nDEFAULT_TPU_RULES = [\n    (\"batch\", \"X\"),\n    (\"hidden\", \"Y\"),\n    (\"heads\", \"Y\"),\n    (\"embed_kernel\", \"X\"),\n    (\"embed\", \"Y\"),\n]\n\nUnder these rules, (\"batch\", None, \"embed\") will become (\"X\", None, \"Y\"). Writing our code this way keeps it flexible as we potentially try out different sharding configurations.\n\nA first run\nThen, at runtime there’s three things we need to do:\n\nCreate the mesh: the mesh is the object that translates our abstract, 2D mesh to the actual physical hardware that will be running the computation. This is where we specify, say, which of the 8 physical devices becomes the mesh device X=0,Y=1.\n\n\nfrom jax.experimental import maps\n\nmesh_shape = (2, 4)\n# We reshape the devices into a 2D mesh, and name the mesh axes.\ndevices = np.asarray(jax.devices()).reshape(*mesh_shape)\nmesh = maps.Mesh(devices, (\"X\", \"Y\"))\n\n\npjit the function: This is similar to jax.jit, except we now also need to specify the sharding spec for the input and output.\n\n\nThe input to this transformer is, say, a \\(8 \\times 512\\) matrix of integers, and we shard the first (batch) axis along the mesh X axis.\nThe output is \\(5120\\) dimensional vectors, so the output is a matrix of shape \\(8 \\times 512 \\times 5120\\). We keep the batch axis sharded over X, and the embedding axis sharded over Y, which we define as P(\"X\", None, \"Y\"). Note that pjit is lower level than the flax methods, and needs the mesh axes directly inside a PartitionSpec; the translation rules we define above are only used for the constraints inside the nn.Module. You can read more about pjit at the JAX level here.\n\n\nfrom jax.experimental import pjit, PartitionSpec as P\n\n# Create fn for inference.\npjit_forward = pjit.pjit(\n    forward,\n    in_axis_resources=P(\"X\", None),\n    out_axis_resources=P(\"X\", None, \"Y\"),\n)\n\n\nCall the function: we use two context managers, one to activate the mesh, and the other specifying the translation rules for all the sharding constraints inside the function.\n\n\nwith maps.Mesh(mesh.devices, mesh.axis_names), nn_partitioning.axis_rules(\n    DEFAULT_TPU_RULES\n):\n    x = pjit_forward(x)\n\nLet’s now apply these constraints to the weights and activations of the feedforward network."
  },
  {
    "objectID": "blog/pjit/index.html#applying-constraints-to-a-ffn",
    "href": "blog/pjit/index.html#applying-constraints-to-a-ffn",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Applying constraints to a FFN",
    "text": "Applying constraints to a FFN\nIn a previous section, we looked at the sharding spec the GSPMD paper proposed for the FFN in a transformer layer. To summarize in a table: The full sharding spec for both the FFN and self-attention can be found in Table 1 in the GSPMD paper. PaLM uses this same spec, but over a much larger \\(256\\times12\\) mesh per pod. (Sec. 4)\n\n\n\n\n\n\n\nTensor\nSharding Spec [shape]\n\n\n\n\n\\(\\text{activation: embedding}\\)\n\\(X, \\_, Y\\) [batch, seq_len, embed]\n\n\n\\(W_{\\text{in}}\\)\n\\(X, Y\\) [embed, hidden]\n\n\n\\(\\text{activation: hidden}\\)\n\\(X, \\_, Y\\)[batch, seq_len, hidden]\n\n\n\\(W_{\\text{out}}\\)\n\\(Y, X\\) [hidden, embed]\n\n\n\nThis sharding spec is for any generic, dense Transformer. The code below is the second sublayer (the FFN network) of an encoder layer in the ESM2 model. We apply this sharding spec to the weights on lines 11 and 21, and to the activations on lines 13 and 23:\n\n\n# ... we apply a layer norm and multi-head attention before this.\n\n# Create second residual block (LayerNorm + MLP)\nresidual = x\nx = nn.LayerNorm(name=\"final_layer_norm\", epsilon=1e-5)(x)\n\n# Create + apply first MLP layer with weight + activation sharding constraints.\nx = partitioning.Dense(\n    self.ffn_embed_dim,\n    name=\"fc1\",\n    shard_axes={\"kernel\": (\"embed_kernel\", \"hidden\")},\n)(x)\nx = nn_partitioning.with_sharding_constraint(x, (\"batch\", None, \"hidden\"))\n# Don't approximate gelu to avoid divergence with original PyTorch.\nx = nn.gelu(x, approximate=False)\n\n# Create + apply second MLP layer with weight + activation sharding constraints.\nx = partitioning.Dense(\n    self.embed_dim,\n    name=\"fc2\",\n    shard_axes={\"kernel\": (\"hidden\", \"embed_kernel\")},\n)(x)\nx = nn_partitioning.with_sharding_constraint(x, (\"batch\", None, \"embed\"))\nx = residual + x\n\nreturn x\n\n\nThe activation sharding specs are applied as in the initial example: we just with_sharding_constraint. But there’s two new things:\n\nThere’s a new shard_axes argument being passed into the layer definition on lines 11 and 21.\nWe’re using the partitioning.Dense layer instead of the standard nn.Dense.\n\nLet me elaborate on what’s going on here.\n\nSharding constraints: Weights\nHere’s the goal: we want to apply the same with_sharding_constraint function we used on the activation tensors, to the weight tensors in these Dense layers. Problem is, it’s defined as a local variable inside the __call__ method of nn.Dense; and is not an attribute I can access from the outside. There’s two options here:\n\nFirst is the t5x (Roberts et al., 2022) route of creating a new version of Dense directly with the constraint applied inside the __call__ method. This works in a robust, well-tested library! However, we’d need to make a modified copy of every layer where we want to apply a constraint over a weight.\nThe second approach is noticing that all flax.linen layers use the nn.Module’s (their parent class) .param method to create their params. Then, we can write a simple mix-in class that overrides the default param method to apply the sharding right as it is created:\n\n\n\n@dataclasses.dataclass\nclass ShardMixIn:\n    \"\"\"Adds parameter sharding constraints for any flax.linen Module.\n    This is a mix-in class that overrides the `param` method of the\n    original Module, to selectively add sharding constraints as specified\n    in `shard_axes`\"\"\"\n\n    shard_axes: Optional[Mapping[str, Tuple[str, ...]]] = None\n\n    # Modifies off \n    # https://github.com/google/flax/blob/main/flax/linen/partitioning.py#L304\n    def param(self, name: str, *init_args):\n        # Initialize using the original Module's `param` method\n        param = super().param(name, *init_args)\n\n        # If `shard_axes` specified and param name in the dict, apply constraint\n        if self.shard_axes and (name in self.shard_axes.keys()):\n            axes = self.shard_axes[name]\n\n            # Apply the sharding constraint (e.g. axes=('embedding', 'hidden'))\n            param = nn_partitioning.with_sharding_constraint(param, axes)\n\n            # Sow this, to have the AxisMetadata available at initialization.\n            self.sow(\n                \"params_axes\",\n                f\"{name}_axes\",\n                nn_partitioning.AxisMetadata(axes),\n                reduce_fn=nn_partitioning._param_with_axes_sow_reduce_fn,\n            )\n\n        return param\n\n\nThere’s only 10 lines of code here, which add the shard_axes argument to any Flax nn.Module. As we can see, with_sharding_constraint is applied on line 21, only when a given param has a constraint specified. No need to rewrite the original layer definition, we simply create a new version that inherits the original, and our mix-in:\n\nclass Dense(ShardMixIn, nn.Dense):\n    pass\n\nThis is just one solution to be able to apply with_sharding_constraint to the weights, and I’m definitely quite open to feedback on whether this is a sensible strategy!\n\n\nPutting it all together\nWe omitted a key detail in the opening example: in the real forward pass (the .apply method) we need to pass in both esm_sharded_params, and the data batch. Since the params are an input argument, they will also need a sharding spec. The params in Flax are a PyTree (specifically, a nested dict) and so the sharding spec is a nested dict with the same structure. There’s some plumbing here, so let’s go through it step by step:\nBecause the ShardMixIn .sow’s the sharding metadata into the module, this metadata is available at model initialization with the .init method. Let’s initialize the 15B model, and inspect the shapes of the parameters of layer 42:\n\n\nCode\nimport functools\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\n\nfrom esmjax.modules import modules\nfrom esmjax.modules import partitioning\n\n\nembed_dim = 5120\nnum_heads = 40\nnum_layers = 48\n\nembedding = nn.Embed(33, embed_dim)\nblock_fn = functools.partial(modules.EncoderLayer, num_heads, embed_dim, embed_dim * 4)\nesm2 = modules.ESM2(embedding, block_fn, num_layers)\n\nkey = jax.random.PRNGKey(0)\narr = jnp.array([[0, 1, 2]])\n\n\nWe can see that the 5120-dimensional embeddings are projected to produce embeddings for 40 heads, with 128 dims each.\n\n# jax.eval_shape replaces all actual arrays with ShapeDtypeStruct\n# This avoids memory use, *and* allows us to inspect the param shapes.\nparams = jax.eval_shape(esm2.init, key, arr)\nparams['params']['42']['self_attn']\n\nFrozenDict({\n    k_proj: {\n        bias: ShapeDtypeStruct(shape=(40, 128), dtype=float32),\n        kernel: ShapeDtypeStruct(shape=(5120, 40, 128), dtype=float32),\n    },\n    out_proj: {\n        bias: ShapeDtypeStruct(shape=(5120,), dtype=float32),\n        kernel: ShapeDtypeStruct(shape=(40, 128, 5120), dtype=float32),\n    },\n    q_proj: {\n        bias: ShapeDtypeStruct(shape=(40, 128), dtype=float32),\n        kernel: ShapeDtypeStruct(shape=(5120, 40, 128), dtype=float32),\n    },\n    v_proj: {\n        bias: ShapeDtypeStruct(shape=(40, 128), dtype=float32),\n        kernel: ShapeDtypeStruct(shape=(5120, 40, 128), dtype=float32),\n    },\n})\n\n\nWe can also see the axis metadata generated when calling the .init method:\n\nparams['params_axes']['42']['self_attn']\n\nFrozenDict({\n    k_proj: {\n        kernel_axes: AxisMetadata(names=('embed_kernel', 'heads', None)),\n    },\n    out_proj: {\n        kernel_axes: AxisMetadata(names=('heads', None, 'embed_kernel')),\n    },\n    q_proj: {\n        kernel_axes: AxisMetadata(names=('embed_kernel', 'heads', None)),\n    },\n    v_proj: {\n        kernel_axes: AxisMetadata(names=('embed_kernel', 'heads', None)),\n    },\n})\n\n\nOnly the params that we’ve specified a sharding constraint over exist in this PyTree. To pass into pjit, we use a utility function to convert the names into mesh axes, and replicate the structure of the full params. The AxisMetadata are replaced with proper PartitionSpecs, and all other params have their sharding pattern set to None, meaning full replication.12\n\nparams, params_axes = params.pop(\"params_axes\")\nesm_axes = partitioning.get_params_axes(params, \n        params_axes, \n        rules=partitioning.DEFAULT_TPU_RULES)\nesm_axes['params']['42']['self_attn']\n\nFrozenDict({\n    k_proj: {\n        bias: None,\n        kernel: PartitionSpec('X', 'Y', None),\n    },\n    out_proj: {\n        bias: None,\n        kernel: PartitionSpec('Y', None, 'X'),\n    },\n    q_proj: {\n        bias: None,\n        kernel: PartitionSpec('X', 'Y', None),\n    },\n    v_proj: {\n        bias: None,\n        kernel: PartitionSpec('X', 'Y', None),\n    },\n})\n\n\nWe now pass this sharding spec (esm_axes) into the pjit definition. Then, we have a fully sharded inference method, distributing the computation work of this 15B model across all 8 cores of a TPU. You can find a fully runnable notebook here.\n\napply_fn = pjit.pjit(\n    esm.apply,\n    in_axis_resources=(esm_axes, P(\"X\", None)),\n    out_axis_resources=P(\"X\", None, \"Y\"),\n)\n\nwith maps.Mesh(mesh.devices, mesh.axis_names), nn_partitioning.axis_rules(\n    partitioning.DEFAULT_TPU_RULES\n):\n    embeds = apply_fn(esm_sharded_params, batch)"
  },
  {
    "objectID": "blog/pjit/index.html#footnotes",
    "href": "blog/pjit/index.html#footnotes",
    "title": "Tensor Parallelism with jax.pjit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nand a copy of the optimizer state and gradients when we’re training, so potential total memory use of upto 3x the size of the model itself.↩︎\nSpecifically, in section 5.2 the authors note “We trained our models on one machine with 8 NVIDIA P100 GPUs.”↩︎\nWhy the row vectors of \\(A\\) and the column vectors of \\(B\\)? Why not just the column vectors of both? This is mostly due to convention, as confusing as it can be for newcomers. I like the Mnemonic RAC-B (“rack b”), rows of A, columns of B.↩︎\nFor an \\(N \\times N\\) matrix, matrix multiplication has time complexity \\(O(N^3)\\) while matrix addition is \\(O(N^2)\\). For a large enough matrix, the speedup from the parallel matrix multiplies can outweigh the cost of communicating then adding afterwards.↩︎\nYou may also recognize this as just jax.lax.psum, which it is!↩︎\nOf course, not accounting for the communication overhead of the AllReduce. For systems that are compute-bound, splitting the actual multiplication across multiple devices and syncing them together (due to the \\(O(N^3)\\) compute costs of a matrix multiply vs the \\(O(N^2)\\) memory transfer) is still often worth it.↩︎\nAgain, the outer axes in a matrix multiply are individual vectors; the inner axes are the feature dimensions of those vectors, as we saw in the intro. Feel free to scroll back if you need to recap!↩︎\nNote that in math contexts this is often written as \\(Wx\\), where \\(x\\) is a column vector. In Python, data points are stored as row vectors, which means \\(X\\) has to be on the left hand side (because the feature dimension needs to be the inner dimension, the one we take dot products over!).↩︎\nRoughly speaking. The dot product is sensitive to the magnitude of the vectors too, not just their direction. If a particular weight vector is very large, it will have large dot products even if the data vector isn’t that similar. However, with proper regularization, most weight vectors should be within a “reasonable” range, enabling comparison.↩︎\nThis is specifically the “2D finalized” sharding pattern presented in Table 1.↩︎\nOr in the math textbook way, “this is left as an exercise to the reader”. But really, I think working it out would be a neat exercise!↩︎\nOn a larger model, we’d shard even the biases and layer norms, but on this scale it’s fine not to. They’re a lot smaller than the weights.↩︎\nA quick estimate counting both the communication ops (e.g. allgather, fused reduce-scatters) as well as data formatting ops.↩︎\nIt’s worse in the self-attention sublayer (29%), which also takes less time overall, resulting in an average of 20%. Would be a better layer to focus more for improvement!↩︎\nAlthough, newer offerings such as the p4d.24xlarge or BM.GPU4.8 have considerably better inter-node bandwidth. At the same time, the A100 GPUs themselves are much faster, which means the inter-node bandwidth must keep up just to avoid becoming a bottleneck.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Irhum’s Notes",
    "section": "",
    "text": "LoRA and Weight Decay\n\n\n\n\n\n\n\nLLM\n\n\nLoRA\n\n\n\n\nLoRA doesn’t approximate a solution to full-finetuning; it solves a different (albeit similar) optimization problem\n\n\n\n\n\n\nSep 27, 2023\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nNeural Language Models\n\n\n\n\n\n\n\nLLM\n\n\nJAX\n\n\n\n\nWe look at language models parametrized by neural networks, and how they’re capable of near transfer, generalizing to sequences similar to (but not the exact same) as those in their training sets.\n\n\n\n\n\n\nNov 14, 2022\n\n\n42 min\n\n\n\n\n\n\n  \n\n\n\n\nTensor Parallelism with jax.pjit\n\n\n\n\n\n\n\nJAX\n\n\nFlax\n\n\nLLM\n\n\nParallelism\n\n\n\n\nWith proper sharding, LLMs can scale far beyond the memory capacity of a single GPU/TPU. We explore the math underpinning this from the ground up, and then implement a fully working implementation with JAX/Flax.\n\n\n\n\n\n\nOct 10, 2022\n\n\n41 min\n\n\n\n\n\n\n  \n\n\n\n\nVisual Notes on Spherical Harmonics\n\n\n\n\n\n\n\ngeometry\n\n\nequivalence\n\n\n\n\nSpherical Harmonics are a core building block of Equivariant Neural Networks. This post breaks them down by analyzing them as 3D extensions of the Fourier Series.\n\n\n\n\n\n\nOct 10, 2021\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nResilience in Complex Systems\n\n\n\n\n\n\n\nsystems\n\n\n\n\nEveryday human reasoning breaks down as the scale of time and space at play increases. Complex systems thinking gives us a new set of tools to better understand the chains of consequences involved, and make better decisions.\n\n\n\n\n\n\nJul 21, 2021\n\n\n25 min\n\n\n\n\n\n\n  \n\n\n\n\nCUDA programming with Julia\n\n\n\n\n\n\n\nJulia\n\n\nCUDA\n\n\n\n\nCUDA has a hierarchical programming model, requiring thought at the level of Grids, Blocks and Threads. We explore this directly, using our understanding to write a simple GPU accelerated addition kernel from scratch.\n\n\n\n\n\n\nMay 7, 2021\n\n\n23 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/lm/index.html",
    "href": "blog/lm/index.html",
    "title": "Neural Language Models",
    "section": "",
    "text": "At their core, language models (LMs) are simply functions that assign a probability to a snippet of text. A common variant are autoregressive LMs: functions which produce a probability for the “next token” conditioned on some already written text, that is \\(p(\\text{token} \\mid \\text{context})\\). Such a models can be immensely powerful, as many tasks can be re-written to fit into a “next token prediction” problem, for instance:\nIf we had an infinite amount of text data, observing every possible sentence proportional to their “true probability”, estimating these probabilities is straightforward.\nUnfortunately, we do not have an infinite corpus. Even with a massive text corpus, we likely won’t observe most valid sequences of say, 100 words, even once since there’s an exponentially large number of sequences.\nThis post focuses on how neural networks allow us to build language models that can produce “good” next token probabilities for sequences that are “similar” (but not the exact same) as sequences in the training set, as first introduced in Bengio et al. (2003). Reusing language from human learning, this is akin to near transfer (Schunk, 2011, p. 320), where there is substantial overlap between the “training” and “test” contexts."
  },
  {
    "objectID": "blog/lm/index.html#maximum-likelihood-estimation",
    "href": "blog/lm/index.html#maximum-likelihood-estimation",
    "title": "Neural Language Models",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nSuppose we receive a coin, and we wish to find out if it’s biased or not. We run 100 tosses, and 62 of them come up heads, so we figure it’s biased at 62% heads, 38% tails.\nWhat just happened here? To rewind, the scenario looks like this:\n\nSupport: We have a random variable \\(X\\), with exactly two possible1 outcomes: heads or tails.\nParametrization: \\(X\\) is a discrete random variable, and each of its outcomes has a true probability value, \\(p(0)\\) and \\(p(1)\\). We don’t know what these values are, so we parametrize a two-valued discrete probability distribution, with \\(p_\\theta(1) = \\theta\\) and \\(p_\\theta(0) = 1 - \\theta\\).\nSampling: Although we don’t know the distribution, we can generate samples from it by tossing the coin repeatedly. We gather a “dataset” of 62 heads and 38 tails.\nEstimation: We can then use these samples to estimate the value of the parameter \\(\\theta\\). Specifically, we can find the value of \\(\\theta\\) that maximizes the likelihood of the outcome we observed.\n\nGiven a value of \\(\\theta\\), we can compute the probability we’d see a given outcome:\n\nThe probability of observing one heads is \\(\\theta\\).\nSince each observation is independent, the probability of observing one heads and two tails is \\(\\theta (1 - \\theta)(1-\\theta) = \\theta(1-\\theta)^2\\)\nFor the overall observation here, of 62 heads and 38 tails, we have \\(\\theta\\theta...(1-\\theta)(1-\\theta)=\\) \\(\\theta^{62}(1-\\theta)^{38}\\)\n\nIndependence between individual observations is a key assumption; if they’re not independent, \\(p(\\text{obs}) = p(\\text{obs}_1)p(\\text{obs}_2)...\\) doesn’t hold.However, \\(\\theta\\) is a variable for which we need to estimate a value. One process to do so, is to see the above term as a function of theta \\(L(\\theta) = \\theta^{62} (1 - \\theta)^{38}\\), known as the likelihood function. We can then find the value of \\(\\theta\\) that maximizes the likelihood function is a process, aptly called maximum likelihood estimation, or MLE. This is the value of \\(\\theta\\) that, when used in \\(p_\\theta(\\cdot)\\) will result in the highest2 probability being assigned to our observation.\nNote that the likelihood function as is will produce extremely tiny values; for numerical stability, we usually minimize the negative log-likelihood, which here is:\n\\[\\begin{align}\n-\\ln L(\\theta) &= -\\ln(\\theta^{62} (1 - \\theta)^{38})\\\\\n&= -62 \\ln (\\theta) - 38 \\ln(1 - \\theta)\\\\\n\\end{align}\\]\nVisualizing both, we see they are maximized and minimized (respectively) at the same value, \\(\\theta=0.62\\), the same as our earlier “back of the hand” estimate:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef prob(p):\n    return p**62 * (1-p)**38\n\ndef log_prob(p):\n    return -62*np.log(p) - 38*np.log(1-p)\n\nfig, ax = plt.subplots(figsize=(9, 4), ncols=2)\nxs = np.linspace(0.3, 0.9, 300)\nax[0].plot(xs, prob(xs))\nax[0].set_xlabel('Value of theta')\nax[0].set_xlim(0.3, 0.9)\nax[0].set_ylabel('Likelihood')\nax[0].plot([0.62], [prob(0.62)], marker='x', markersize=10, color=\"royalblue\")\n\n\nax[1].plot(xs, log_prob(xs))\nax[1].set_xlabel('Value of theta')\nax[1].set_ylabel('Negative Log Likelihood')\nax[1].set_xlim(0.3, 0.9)\nax[1].plot([0.62], [log_prob(0.62)], marker='x', markersize=10, color=\"royalblue\")\n\n\n\n\n\nNow suppose we learn the true value of \\(p(1) = 0.6\\). Note that \\(0.6\\) is in fact, less likely than our estimate of \\(p_\\theta(1) = \\theta = 0.62\\):\n\n\nCode\ndef log_prob(p):\n    return -62*np.log(p) - 38*np.log(1-p)\n\nfig, ax = plt.subplots(figsize=(9, 4), ncols=1)\nxs = np.linspace(0.55, 0.65, 300)\n\nax.plot(xs, log_prob(xs))\nax.set_xlabel('Value of theta')\nax.set_ylabel('Negative Log Likelihood')\nax.set_xlim(0.55, 0.65)\nax.plot([0.62], [log_prob(0.62)], marker='x', markersize=10, color=\"royalblue\")\nax.plot([0.6], [log_prob(0.6)], marker='x', markersize=10, color=\"red\")\n\n\n\n\n\nThis is because we estimated \\(\\theta\\) with the value that maximizes the probability of the observed samples. In the limit of an infinite number of samples, the likelihood function will be maximized at a value of \\(\\theta\\) that produces the true probabilities. But in practice, there will be variance in the number of heads in 100 samples (we may have 61 heads, or 58, or…); and in turn, variance in our estimates."
  },
  {
    "objectID": "blog/lm/index.html#discrete-distributions-as-vectors",
    "href": "blog/lm/index.html#discrete-distributions-as-vectors",
    "title": "Neural Language Models",
    "section": "Discrete distributions as vectors",
    "text": "Discrete distributions as vectors\nWe can write the probability mass function of the “coin toss” random variable in the following vector. Each entry simply stores the probability for one of the possible outcomes:\n\n\n\n\n\nIn general, for a discrete random variable with \\(V\\) possible outcomes, there are \\(V-1\\) free parameters. This is because all the probabilities must sum to \\(1\\):\n\nFor \\(V=2\\), as in the case above, we have the probability for one outcome be \\(\\theta\\). The other must then be \\(1 - \\theta\\) for both to sum to \\(1\\).\nFor \\(V=3\\), we can have probabilities for two of the outcomes specified by \\(\\theta_1\\) and \\(\\theta_2\\). The third outcome then must have the probability \\(1 - \\theta_1 - \\theta_2\\).\nFor \\(V=n\\), we have the probabilities for the first \\(n-1\\) outcomes specified by \\(\\theta_i\\), and the final one by \\(1 - \\sum_{i=1}^n \\theta_i\\).\n\nThere are potentially detrimental consequences when we “tie” the estimates of the probabilities of different outcomes, as we see next.\n\nConstrained estimates\nSuppose we have a discrete random variable with three possible outcomes (\\(V=3\\)). The vector encodes these three outputs for the true probability mass function \\(p(\\cdot)\\)3:\n\n\n\n\n\nAs before, we don’t know \\(p(\\cdot)\\), so we approximate it with \\(p_\\theta(\\cdot)\\). To find the best value of the parameters \\(\\theta\\), we use maximum likelihood estimation. We define two parametrizations \\(p_\\theta(\\cdot)\\):\n\none that is free with \\(V-1 = 2\\) parameters (\\(\\theta_1\\) and \\(\\theta_2\\)). That is, we can independently update \\(p_\\theta(a)\\) without affecting \\(p_\\theta(b)\\).4\nanother that is constrained to only have one parameter (\\(\\theta_c\\)). Note that \\(p_\\theta(a)\\) and \\(p_\\theta(b)\\) are now tied; updating one will also change the other.\n\nAs like the true \\(p(\\cdot)\\), we can also write the outcomes of these approximations \\(p_\\theta(\\cdot)\\) in a vector:\n\n\n\n\n\nSuppose we draw 1000 samples, receiving 38 a’s, 7 b’s and 955 c’s. For each parametrization, we find the values of the parameters that maximize the likelihood of receiving this outcome:\n\n\nCode\ndef log_prob(theta0, theta1):\n    return -(38*np.log(theta0) + 7*np.log(theta1) + 955*np.log(1-theta0-theta1))\n\ndef log_prob_constrained(theta):\n    return -(38*np.log(theta) + 7*np.log(theta**2) + 955*np.log(1-theta-theta**2))\n\ntheta0_s = np.linspace(0.02, 0.06, 100)\ntheta1_s = np.linspace(0.001, 0.02, 100)\nmesh_b0, mesh_b1 = np.meshgrid(theta0_s, theta1_s)\nmesh_y = log_prob(mesh_b0.reshape(-1), mesh_b1.reshape(-1)).reshape(100, 100)\n\nfig, ax = plt.subplots(figsize=(9, 4), ncols=2)\ncontour = ax[0].contourf(mesh_b0, mesh_b1, mesh_y, levels=30, cmap=\"Blues\")\ncontour_line = ax[0].contour(mesh_b0, mesh_b1, mesh_y, levels=[203.3, 207.19], cmap=\"Blues_r\", vmin=202, vmax=220)\nax[0].clabel(contour_line, inline=True, fontsize=10)\nax[0].plot([0.038], [0.007], marker='x', markersize=10, color=\"royalblue\")\nax[0].set_xlabel(\"theta0\")\nax[0].set_ylabel(\"theta1\")\nax[0].set_yticks(np.arange(0.0, 0.02, 0.003))\nax[0].set_ylim(0.001, 0.02)\nax[0].set_title(\"(Free) Lowest NLL = {:.2f}\".format(log_prob(0.038, 0.007)))\n\nxs = np.linspace(0.03, 0.06, 300)\nax[1].plot(xs, log_prob_constrained(xs))\nax[1].plot([0.047], [log_prob_constrained(0.047)], marker='x', markersize=10, color=\"royalblue\")\nax[1].set_xlabel('theta')\nax[1].set_ylabel(\"Negative Log Likelihood\")\nax[1].set_title(\"(Constrained) Lowest NLL = {:.2f}\".format(log_prob_constrained(0.047)))\nax[1].set_xlim(0.03, 0.06)\nax[1].set_yticks(np.arange(204, 215, 2))\nax[1].set_ylim(204, 214.3)\nplt.show()\n\n\n\n\n\nOn the left, in the “free” parametrization, the minimum is reached at \\(\\theta_1=0.038, \\theta_2=0.007\\), just as would be expected by estimating from the proportions directly (\\(\\theta_1=\\frac{38}{1000}, \\theta_2=\\frac{7}{1000}\\)). In general, this is true for any number of outcomes \\(V\\); the maximum likelihood estimate for each probability \\(\\theta_i\\) is the proportion of times said outcome appears in the entire dataset.\nOn the right however, with only one free parameter under the above parametrization, the minimum is reached at \\(\\theta_c=0.047\\). This corresponds to \\(p_\\theta(a)=0.047, p_\\theta(b)=0.047^2=0.002\\). Not only is this a bad estimate of the true probabilities (where \\(p(a)=0.03\\)), it’s far even from the estimate that was reached by the free model (where \\(p_\\theta(a)=0.038\\)).\nThe lowest NLL reached here is \\(207.19\\). By plotting a contour back on the free parametrization (left), we can find parameter pairs (\\(\\theta_1, \\theta_2\\)) with a lower NLL. With the free parametrization, \\(\\theta_1\\) could be anywhere between 0.025 (2.5%) and 0.055 (5.5%) and still have a higher likelihood for the observation than the best value of \\(\\theta_c\\).\n\nA better constraint\nHowever, suppose we knew more about the problem: this is actually the annual failure rate of a widget, with a, b and c meaning “big gear failure”, “small gear failure” and “no failure”. Moreover, prior physical knowledge of the system informs us that the big gear should fail at 3x the rate of the smaller one, that is: \\(p(a)=3p(b)\\). This inspires the following parametrization, where this constraint holds for all values of \\(\\theta_c\\):\n\n\n\n\n\nBy having \\(p_\\theta(a)=\\theta_c\\) and \\(p_\\theta(a)=\\frac{\\theta_c}{3}\\), we guarantee the constraint \\(p_\\theta(a)=3p_\\theta(b)\\) is met for all values of \\(\\theta_c\\). As before, we find the value of \\(\\theta_c\\) with the lowest NLL:\n\n\nCode\ndef log_prob_constrained(theta):\n    return -(38*np.log(theta) + 7*np.log(theta**2) + 955*np.log(1-theta-theta**2))\n\ndef log_prob_constrained2(theta):\n    return -(38*np.log(theta) + 7*np.log(1/3*theta) + 955*np.log(1-4/3*theta))\n\nfig, ax = plt.subplots(figsize=(9, 4), ncols=1)\nxs = np.linspace(0.03, 0.06, 300)\n\nax.plot(xs, log_prob_constrained(xs))\nax.plot(xs, log_prob_constrained2(xs))\nax.set_xlabel('Value of theta_c')\nax.set_ylabel('Negative Log Likelihood')\nax.set_xlim(0.03, 0.06)\nax.plot([0.047], [log_prob_constrained(0.047)], marker='x', markersize=10, color=\"royalblue\", label=\"old min NLL={:.2f}\".format(log_prob_constrained(0.047)))\nax.plot([0.0338], [log_prob_constrained2(0.0338)], marker='x', markersize=10, color=\"orange\", label=\"new min NLL={:.2f}\".format(log_prob_constrained2(0.0338)))\nax.legend()\nplt.show()\n\n\n\n\n\nThe optimal of \\(\\theta_c\\) then is \\(\\theta_c=0.034\\), corresponding to \\(p_\\theta(a)=0.034\\), \\(p_\\theta(b)=\\frac{0.034}{3}=0.011\\).\nNot only is this new constrained estimate \\((0.034)\\) better than the previous one, the probabilities are closer to the true ones \\((0.03)\\) than even with the most flexible model \\((0.038)\\). This is because we’ve correctly incorporated our prior knowledge that \\(p_\\theta(a)=3p_\\theta(b)\\). To summarize:\n\nA parametrization for a discrete distribution with \\(V\\) outcomes, that is guaranteed to be capable of converging to the true distribution (given an infinite amount of data) has \\(V-1\\) parameters; one parameter per outcome.\nBad constraints will permanently bias the model; even with an infinite amount of data its ability to represent the correct probabilities will be constrained (in the first constrained example, we have \\(p_\\theta(b) = (p_\\theta(a))^2\\), even though it is false)\nHowever, a truthful constraint (here, \\(p_\\theta(a)=3p_\\theta(b)\\)) will converge to the true parameters with an infinite amount of data. Moreover, it will converge faster, since the constraints will reduce the impact of variance in the samples."
  },
  {
    "objectID": "blog/lm/index.html#joint-distributions-as-tensors",
    "href": "blog/lm/index.html#joint-distributions-as-tensors",
    "title": "Neural Language Models",
    "section": "Joint Distributions as tensors",
    "text": "Joint Distributions as tensors\nLet’s now look at a situation with two random variables. Suppose we now have the following setup:\n\nI have two time slots each day, each of which I can fill up with either reading or hiking.\nThe activity done in the first and second time slots are \\(X_1\\) and \\(X_2\\) respectively.\n\nThen, let the joint distribution5 of \\(X_1\\) and \\(X_2\\) be the following:\n\n\n\n\n\nNote that these variables are not independent: for instance, if we learn hiking is the second activity of the day, we can deduce hiking could not have been the first activity that day, as \\(p(\\text{hiking}_1, \\text{hiking}_2) = 0\\). Estimation works the same as previously: if 21 of 200 observed days are “read first, then hike”, then \\(p_\\theta(\\text{read}_1, \\text{hiking}_2) = \\frac{21}{200}\\).\nMoreover, we note that each of the \\(X\\)’s has the same \\(V=2\\) outcomes: \\(H\\) or \\(T\\). We could then convert the “vector” above into the following matrix:\n\n\n\n\n\nIn general, with \\(m\\) random variables, each with \\(V\\) outcomes individually, there are \\(V^m\\) total possible joint outcomes. Here, this is \\(2^2 = 4\\) outcomes. One can imagine storing these joint outcomes in a tensor with \\(m\\) axes, each with \\(V\\) dimensions. Here we store these 4 outcomes in a \\(2\\times 2\\) matrix; for 4 variables with 3 outcomes each we could use a \\(3\\times3\\times3\\times3\\) tensor (with 81 total “outcomes”).\nNote that this grows exponentially in the number of outcomes. With \\(V=10\\) and \\(m=10\\), we have \\(10^{10}\\) (ten billion) possible outcomes. Each of these is an outcome we need to observe (multiple times) to estimate the probabilities for. Compared to the previous \\(V - 1\\) parameters for the distribution of a single random variable, we now have \\(V^m - 1\\) parameters for the joint distribution of \\(m\\) random variables.\nThis exponential growth (and in turn, the number of samples required) makes directly estimating probabilities for anything but the simplest of joint distributions intractable. Maybe conditional probabilities can help?"
  },
  {
    "objectID": "blog/lm/index.html#conditional-distributions-as-vectors",
    "href": "blog/lm/index.html#conditional-distributions-as-vectors",
    "title": "Neural Language Models",
    "section": "Conditional Distributions as vectors",
    "text": "Conditional Distributions as vectors\nConditional distributions also fit neatly into this tensor form. The chain rule in probability tells us that any joint probability can be decomposed into the product of conditional probabilities. Extending the previous example:\n\\[p(\\text{hiking}_1, \\text{reading}_2) = \\textcolor{NavyBlue}{p(\\text{reading}_2 \\mid \\text{hiking}_1)}\\textcolor{Orange}{p(\\text{hiking}_1)}\\]\nHere, the joint probability of “hiking then reading” is the product of the marginal probability of hiking first and the conditional probability of reading after hiking. We can represent this factorization in vector form as follows:\n\n\n\n\n\nTo analyze this factorization:\n\nWe’ve initially stored the joint probabilities \\(p_\\theta(\\cdot, \\cdot)\\) in a matrix form. This has \\(V^m = 2^2 = 4\\) outcomes, and so \\(V^m - 1 = 3\\) parameters.\nWe can factorize that matrix into a vector \\(p_\\theta(\\cdot)\\) representing the marginal probability, and another vector \\(p_\\theta(\\cdot \\mid \\text{hiking}_1)\\) representing the conditional probability.\nEach of these two vectors has \\(V=2\\) entries, and so \\(V - 1 = 1\\) parameters. With 2 vectors, we have 2 parameters total.\n\nIt appears the factorized version has fewer parameters (2 vs 3) than the original. There’s just one problem: it’s incomplete! We’re only looking at the conditional probability table for where \\(X_1 = \\text{hiking}_1\\); there’s another one for \\(X_1 = \\text{read}_1\\):\n\n\n\n\n\nWith three vectors (one marginal, two conditional), we have 3 parameters, the same as the original joint probability matrix. This unfortunately is true in general as well; for \\(m\\) discrete random variables (each with \\(V\\) outcomes):\n\nthe conditional probability vector of \\(p(\\cdot \\mid x_{1:k-1})\\) has \\(V\\) entries (\\(V-1\\) parameters), as \\(X_k\\) only has \\(V\\) outcomes.\nHowever, there are \\(V^{k-1}\\) such tables, one for every possible combination of prior values \\(x_{1:k-1} = x_1, ..., x_{k-1}\\).\nThe total number of parameters for a variable conditioned on \\(k-1\\) “already observed” outcomes then is \\(V^{k-1}(V-1)\\).\n\n\nGeneral Case\nThe general chain rule for a joint distribution with \\(m\\) variables is:\n\\[p(x_1, x_2,..., x_m) = p(x_m \\mid x_{1:m-1})p(x_{m-1} \\mid x_{1:m-2})...p(x_2\\mid x_1)p(x_1)\\]\nUsing the result in the previous section, let’s compute the number of free parameters in the distributions in the product above:\n\nFor the marginal \\(p(\\cdot)\\), there are \\(V-1\\) parameters.\nFor \\(p(\\cdot \\mid x_1)\\), conditioned on one outcome there are \\(V(V-1)\\) parameters.\nFor \\(p(\\cdot \\mid x_{1:m-1})\\), conditioned on \\(m-1\\) outcomes there are \\(V^{m-1}(V-1)\\) parameters.\n\nSumming up the number of parameters for all \\(m\\) distributions in the product, we have:\n\\[\\begin{align}\nN &= (V - 1) + V(V-1) + V^2(V-1)+...+V^{m-1}(V-1)\\\\\n&= \\sum_{k=0}^{m-1} V^k(V-1)\\\\\n&= V^m - 1\n\\end{align}\\]\nWe’re back at \\(V^m - 1\\) parameters, same as in the joint distribution case. Breaking the joint probability matrix into conditional probability vectors changes the parametrization, but doesn’t change the number of parameters.\n\n\nIndependence as constraints\nEarlier with the widgets example, we used context to reduce the number of parameters needed. Now suppose here, we knew that each of the \\(X_i\\) was a coin toss (with 60% probability of heads as before), and we had a sequence of \\(m=30\\) coin tosses. There’s \\(2^{30}\\) (\\(\\approx\\) a billion) possible sequences of coin tosses here. And you’d need to observe each of these billion sequences multiple times to estimate the proportion for each sequence compared to the total.\nBut it’s silly to store a separate probability for each unique sequence (e.g. \\(\\text{HTH...T}\\)): we know the coin tosses are independent from each other, and identically distributed. We could simplify the chain rule from before as follows:\n \\[\\begin{align}\np_{X_1, X_2,...,X_m}(x_1, x_2,..., x_m) &= p_{X_m}(x_m \\mid x_{1:m-1})p_{X_{m-1}}(x_{m-1} \\mid x_{1:m-2})...p_{X_2}(x_2\\mid x_1)p_{X_1}(x_1)\\\\\n&=p_{X_m}(x_m)p_{X_{m-1}}(x_{m-1})...p_{X_2}(x_2)p_{X_1}(x_1)\\\\\n&=p(x_m)p(x_{m-1})...p(x_2)p(x_1)\\\\\n\\end{align}\\]Note that here we explicitly subscript each \\(p\\) with the random variable it is of. This is to clearly demonstrate when we drop them in the third line. We do so as we integrate the knowledge all variables are identically distributed (and hence have the same \\(p\\).)\nThis simplifies things greatly:\n\nPreviously, each \\(p(x_k \\mid x_{1:k-1})\\) required \\(V^{k-1}(V-1)\\) parameters to represent: each conditional probability vector has \\(V-1\\) parameters, and there’s \\(V^{k-1}\\) such vectors as there are \\(V^{k-1}\\) combinations of \\(k-1\\) previous outcomes.\nBut you don’t actually need a separate table for each combination of past outcomes: independence means \\(p(x_k \\mid x_{1:k-1}) = p(x_k)\\). Each of the \\(V^{k-1}\\) vectors are the same vector with 2 outcomes, which means there’s only \\(V-1=1\\) parameter here.\nMoreover, each of the \\(X_i\\)’s are identically distributed (since it’s the same coin), that is, they all share that \\(1\\) parameter.\n\nOverall, we see that as the coin tosses are independent, one parameter is sufficient to express the entire joint probability for any of the \\(2^{30}\\) possible sequences. In general, knowing (conditional) independence between variables helps greatly: if (some) of the previous variables don’t change the probability, you don’t need a separate conditional probability vector for each of those previous combinations of outcomes. Here, we cut \\(V^m-1\\) free parameters to \\(V-1=2-1=1\\) parameter."
  },
  {
    "objectID": "blog/lm/index.html#dataset",
    "href": "blog/lm/index.html#dataset",
    "title": "Neural Language Models",
    "section": "Dataset",
    "text": "Dataset\nWe use the dataset from Karpathy (2015), which is a 4.6MB file containing all the works of Shakespeare. Here’s the first 75 characters in the dataset:\n\n\nCode\nimport requests\n\nurl = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\ntext = requests.get(url).text\ntext[:75]\n\n\n'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, s'\n\n\n\nTokenization\nBefore building a language model, we must choose how to represent our raw text as individual “tokens” in a “sequence”. This requires concretely defining what the vocabulary for each token is:\n\nWord-level: If each entire English word is a token, then the size of the vocabulary \\(V\\) grows quickly. This dataset alone has over 60,000 unique words.\nCharacter-level: If each individual character is a token, \\(V\\) is small (here, there’s only 67 unique characters, including whitespaces like \\n.). However, for a given amount of information, \\(m\\) needs to be much larger since there’s more tokens. For instance, “the weather in Berlin” is 4 words, but 21 characters.\nSubword-level: The most common way to build language models today is to use subword tokenization, such as using the SentencePiece tokenizer (Kudo & Richardson, 2018). These keep small words intact, but break up longer words into smaller “subwords”.\n\nFor this article we’ll be tokenizing at the character level: it’s straightforward, and will allow us to directly visualize the probability matrices. Breaking up the first 75 characters into individual tokens, we have:\n\n\nCode\nprint([c for c in text[:75]])\n\n\n['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'A', 'l', 'l', ':', '\\n', 'S', 'p', 'e', 'a', 'k', ',', ' ', 's']\n\n\nThen, converting each unique token into a unique integer, we have:\n\n\nCode\nvocab = sorted(set(text))\nchar2idx = {c: i for (i, c) in enumerate(vocab)}\ntokens = [char2idx[c] for c in text]\n\nprint(tokens[:75])\n\n\n[18, 49, 58, 59, 60, 1, 15, 49, 60, 49, 66, 45, 54, 10, 0, 14, 45, 46, 55, 58, 45, 1, 63, 45, 1, 56, 58, 55, 43, 45, 45, 44, 1, 41, 54, 65, 1, 46, 61, 58, 60, 48, 45, 58, 6, 1, 48, 45, 41, 58, 1, 53, 45, 1, 59, 56, 45, 41, 51, 8, 0, 0, 13, 52, 52, 10, 0, 31, 56, 45, 41, 51, 6, 1, 59]\n\n\nNote that our input pipeline looks like this:\n\\[\\text{raw text} \\rightarrow \\text{tokens} \\rightarrow \\text{integers}\\]\nNow suppose we’ve set out to build a language model with context length \\(m=1024\\); that is, a model can assign a probability to all character sequences of length 1024. Even here, we run into combinatorial explosion: with \\(V=67\\) characters and context length \\(m=1024\\), there’s \\(67^{1024}\\) possible sequences.\nMost of those sequences would have zero probability (on a smaller scale, “big city”6 is a valid sequence of length 8, while “xcsazmad” is not), but even among the valid sequences, you’d need a truly staggering number of samples to even observe every valid sequence. Since this is infeasible, let’s try simplifying."
  },
  {
    "objectID": "blog/lm/index.html#chain-rule",
    "href": "blog/lm/index.html#chain-rule",
    "title": "Neural Language Models",
    "section": "Chain Rule",
    "text": "Chain Rule\nUsing the chain rule as discussed earlier, we can represent any joint probability as a product of conditional probabilities. With \\(m=1024\\), we have:\n\\[\\begin{align}\np(w_1, w_2,..., w_{1024}) &= p(w_{1024} \\mid w_{1:1023})p(w_{1023} \\mid w_{1:1022})...p(w_2\\mid w_1)p(w_1)\\\\\n&= \\prod_{i=1}^{1024} p(w_i \\mid w_{1:i-1})\\\\\n\\end{align}\\]\nIn this case, it means iteratively computing the probability of each token conditioned on all the previous tokens. On its own, this is of little help. As before, although we’re “splitting up” the joint probability tensor, there’s still \\(V^{1024} - 1\\) possible variables in total, since each “next token prediction” would need a unique conditional probability vector for each combination of previous words \\(w_{1:i-1}\\)."
  },
  {
    "objectID": "blog/lm/index.html#unigram",
    "href": "blog/lm/index.html#unigram",
    "title": "Neural Language Models",
    "section": "Unigram",
    "text": "Unigram\nWhat if we pretended, just like coin tosses, each token was independent of its previous tokens? This is not true: We likely have \\(p(\\text{'h'}\\mid \\text{'t'})\\) greater than \\(p(\\text{'h'})\\), as “th” appears in “the”, one of the most common words in the English language. Knowing prior tokens definitely changes the probability of the next token.\nBut continuing forward with this naive assumption, we write:\n\\[\\begin{align}\np(w_1, w_2,..., w_{1024}) &= p(w_{1024} \\mid w_{1:1023})p(w_{1023} \\mid w_{1:1022})...p(w_2\\mid w_1)p(w_1)\\\\\n&\\approx p(w_{1024})p(w_{1023})...p(w_2)p(w_1)\n\\end{align}\\]\nThat is, the joint distribution is the product of the marginal distributions. Since each marginal \\(p(\\cdot)\\) has \\(V=67\\) outcomes, they have \\(66\\) parameters. We make another naive assumption: each of the \\(X_i\\)’s are identically distributed7. This means these 66 parameters are shared across all 1024 terms in the product above.\nNote that with these two assumptions, we’re no longer estimating the probability of length \\(m=1024\\) sequences, but rather \\(m=1\\). We’re then assuming we can approximate the probability of a longer sequence with the products of these individual probabilities.\n\nEstimation\nWith the two naive assumptions above (independence + sharing across timesteps) we have 66 parameters that need to be estimated. We know the maximum likelihood estimate here from the previous section: count the proportion of times an outcome happened, among the total.\nLet’s first split the corpus (of 4,573,338 tokens) into a training set (first 4 million) and a validation set (remaining 573,338).\n\n\nCode\ntrain_tokens, valid_tokens = tokens[:4000000], tokens[4000000:]\n\n\nThen, to estimate the parameters, we count the number of times the token appears and divide by the total8:\n\n# Create \"initial\" counts\ncounts = np.zeros(shape=(len(vocab),)) + 0.1\n\n# Loop over tokens in dataset\nfor token in train_tokens:\n    counts[token] += 1\n\n# Normalize to 1\nparams = counts / counts.sum()\n\nWe can then visualize this one-variable probability distribution, in the following bar graph:\n\n\nCode\nidxs = np.argsort(params)[::-1]\nfig, ax = plt.subplots(figsize=(9, 4))\nax.bar([vocab[idx].replace(\"\\n\",\"\\\\n\") for idx in idxs], params[idxs])\nax.set_xlabel(\"Outcome\")\nax.set_ylabel(\"Probability of outcome\")\nplt.show()\n\n\n\n\n\nAs we see, the most “likely” token is ' ', followed by e, with the lowest probability one being $.\n\n\nEvaluation\nHow good of a model of language is this? Even though very constrained, we can see it’s correctly “learned” a qualitative aspect of English: that e is the most commonly occurring letter. But how do we know these frequencies are reliable, and not due to idiosyncrasies in the first 4 million tokens9?\nOne metric commonly used for language models is perplexity, which works as follows:\n\nFor each token, compute the log probability of that token under the model.\nTake the mean of the log probability across all tokens.\nTake the negative exponential of this mean.\n\nIn code, we have10: Note that perplexity maxes out at \\(V\\) (here, 67) when every token has probability \\(\\frac{1}{V}\\) (that is, uniform) under the model. It has a minimum at 1, when the model assigns a probability of 1 to every token; that is, it perfectly predicts the sequence11.\n\ndef perplexity_unigram(probs_vec, tokens, start=7):\n    log_probs = [np.log(probs_vec[token]) for token in tokens[start:]]\n    return np.exp(-np.mean(log_probs))\n\nComputing the perplexity over the training and test sets, we have train_ppl=27.53, and valid_ppl=27.14. One way to interpret this value is that the model would be “choosing” between 27.14 outcomes (out of 67) at every step if asked to reproduce the validation sequence (lower is better).\n\n\nGeneration\nNow that we’ve estimated the parameters, we can generate a new sequence. Let’s generate a new sequence of length 30:\n\nsequence = ''\n\nfor _ in range(30):\n    sequence += np.random.choice(vocab, p=params)\n\nsequence\n\n'dweIdi ta rru mSd if  \\nt aeoe '\n\n\nAs we can see, this isn’t very English like: after all, it’s treating every token as a 67-way coin toss, with no regard for the previous tokens. Let’s make this more realistic."
  },
  {
    "objectID": "blog/lm/index.html#bigrams",
    "href": "blog/lm/index.html#bigrams",
    "title": "Neural Language Models",
    "section": "Bigrams",
    "text": "Bigrams\nInstead of assuming each token is independent, let’s assume that a token \\(w_i\\) and tokens \\(w_{i-2}, w_{i-3}...\\) are conditionally independent, given token \\(w_{i-1}\\). This means if we know the immediately previous token, knowing tokens more previous will not change the probability. We have the approximation:\n\\[\\begin{align}\np(w_1, w_2,..., w_{1024}) &= p(w_{1024} \\mid w_{1:1023})p(w_{1023} \\mid w_{1:1022})...p(w_2\\mid w_1)p(w_1)\\\\\n&\\approx p(w_{1024} \\mid w_{1023})p(w_{1023} \\mid w_{1022})...p(w_2\\mid w_{1})p(w_1)\n\\end{align}\\]\nAgain, this is a faulty approximation: \\(p(\\text{'e'}\\mid \\text{'h'}, \\text{'t'})\\) is greater than \\(p(\\text{'e'}\\mid \\text{'h'})\\), as knowing the first two letters th would give us much stronger confidence in the completion the than just the previous letter h. But it is better than assuming complete independence between tokens.\nFor each conditional probability term, there’s \\(V(V-1)\\) parameters as we need one vector for each possible “prior” token. If we share these parameters across timesteps as before12, then there’s \\(V(V-1)\\) parameters in total to be estimated to be able to compute the conditional probabilities. With \\(V=67\\), this is 4422 free parameters.\n\n\nEstimation\nEstimation with maximum likelihood remains similar: To estimate \\(p_\\theta(b \\mid a) = \\theta_{a,b}\\), we find all the cases where \\(a\\) happens, and then compute the proportion of them that are followed by \\(b\\). In practice, we create a counts matrix, and normalize it such that each vector sums to 1:\nNote that we add a 0.1 count to each “transition” pair. Without it, if a pair \\((a, b)\\) doesn’t appear in the training sequence it would have \\(p_\\theta(b \\mid a) = 0\\). If it subsequently appeared in the validation sequence, it would also have \\(p_\\theta(b \\mid a) = 0 \\Rightarrow -\\ln p_\\theta(b \\mid a) = \\infty\\) (and in turn, a sequence perplexity of \\(\\infty\\)). Adding it ensures a small probability is assigned to every possible transition.\n\n# Create \"initial\" counts\ncounts = np.zeros((len(vocab), len(vocab))) + 0.1\n\n# Compute the counts matrix\nfor i in range(1, len(train_tokens)):\n    prev_token = train_tokens[i-1]\n    current_token = train_tokens[i]\n    counts[prev_token, current_token] += 1\n\n# Normalize to get proportions that sum to 1.\nparams = counts / counts.sum(axis=-1, keepdims=True)\n\nWhen visualized, the matrix of estimated conditional probabilities is as follows. Note that each row sums to 1.\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(1, 1)\nfig.add_trace(\n    go.Heatmap(\n        z=params,\n        x=[v.replace(\"\\n\", \"\\\\n\") for v in vocab],\n        y=[v.replace(\"\\n\", \"\\\\n\") for v in vocab],\n        hovertemplate=\"p(%{x}|%{y})=%{z}&lt;extra&gt;&lt;/extra&gt;\",\n        coloraxis=\"coloraxis\",\n    ),\n    row=1,\n    col=1,\n)\n\nfig.update_xaxes(title_text=\"Next Token\", tickmode=\"linear\", tickangle=0)\nfig.update_yaxes(title_text=\"Current Token\", tickmode=\"linear\")\nfig.update_layout(autosize=False, \n                width=700, \n                height=700,\n                coloraxis_showscale=False, \n                coloraxis={\"colorscale\": \"Greys\"})\nfig.show()\n\n\n\n                                                \n\n\nWe can glean patterns from this matrix: for instance, the estimated probability of a newline following a newline \\(p_\\theta('\\backslash\\text{n}' \\mid '\\backslash\\text{n}')\\) is 0.187.\n\n\nEvaluation\nEvaluating, we have train_ppl=11.87, and valid_ppl=11.95, which more than halves the perplexity values of the unigram model. This makes sense: our estimates for the next token should be better when we account for the previous token.\n\n\nGeneration\nSampling is also straightforward: given a starting token \\(w_{i-1}\\), we sample the next token using the conditional probability vector corresponding to \\(p(\\cdot \\mid W_{i-1} = w_{i-1})\\), as follows:\n\nsequence = 'e'\n\nfor i in range(1, 30):\n    prior_token = char2idx[sequence[i-1]]\n    conditional_prob = params[prior_token, :]\n    sequence += np.random.choice(vocab, p=conditional_prob)\n\nsequence\n\n\"eanghitheyen'somillllomp th d \"\n\n\nThe text now feels a bit more plausible, but not by much: it’s challenging to form full words when you’re constrained to ignore all tokens other than the one just before."
  },
  {
    "objectID": "blog/lm/index.html#bigrams-neural-networks",
    "href": "blog/lm/index.html#bigrams-neural-networks",
    "title": "Neural Language Models",
    "section": "Bigrams: Neural Networks",
    "text": "Bigrams: Neural Networks\nThe computation we derived above was to represent the joint probability \\(p(w_1,...,w_{1024})\\) as the product of conditional probabilities \\(p(w_{i} \\mid w_{i-1})\\). The representation we chose was to have each \\(p(b \\mid a) = \\theta_{a, b}\\)13. This resulted in a \\(67 \\times 67\\) matrix14.\nBut we only need an individual parameter for each conditional probability in \\(p_\\theta\\), if we want to be able to update a probability without altering the others. If we’re okay with a constrained parametrization, we can use just about anything: a Fourier series, a sequence of piecewise linear functions, etc. Since neural networks are universal function approximators15 (Hornik, 1991), we could represent \\(p_\\theta\\) using a neural network; that is \\(p_\\theta(b \\mid a) = f_\\theta(a)[b]\\). In detail:\n\nInput: The neural network takes in the token \\(a\\) as input, and returns a vector of probabilities corresponding to \\(p_\\theta(\\cdot \\mid a)\\).\nOutput: We then index into this vector with \\(b\\), getting the probability \\(p_\\theta(b \\mid a)\\).\n\nThere’s a problem here: while the output (a probability) is continuous, the input \\(a\\) (a token) is a discrete integer. We can’t use backpropagation here, since we can’t differentiate through \\(a\\). Or can we?\n\nEmbeddings\nOne approach is to use embedding vectors: we associate a real valued vector with each token in the vocabulary, and use that as an input to the neural network:\n\n\n\n\n\nThis converts the continuous nature of neural networks from a problem to a feature. Recall that with continuous functions, small changes in the input result in small changes in output.\nIf the next token probabilities (the outputs) for any two tokens \\(p\\) and \\(q\\) are similar (that is, \\(p(\\cdot \\mid p) \\approx p(\\cdot \\mid q)\\), then during training, the embedding vectors for \\(p\\) and \\(q\\) (the inputs) can be optimized to be close to each other (again, similar inputs \\(\\rightarrow\\) similar outputs). Instead of a human defining a hard constraint (such as \\(\\theta_{p, \\cdot} = \\theta_{q, \\cdot}\\) if we were using explicit conditional probability vectors), the network can learn these associations directly from data.\nNote that our input pipeline now looks like this: in doing so, we convert a raw string into a sequence of vectors.\n\\[\\text{raw text} \\rightarrow \\text{tokens} \\rightarrow \\text{integers} \\rightarrow \\text{embedding vectors}\\]\n\n\nTraining\nTraining (or learning) really is just a concise way of saying “solving an optimization problem”. Specifically, the following problem:\n\\[\\min_\\theta [-\\ln L(\\theta)]\\]\nThat is, we wish to:\n\nfind the values of the parameters \\(\\theta\\) of the neural network\nthat minimize the negative log-likelihood of our observed training sequence16\nor equivalently, maximize the probability of our observed training sequence\n\nUnlike previously, where we “knew” the closed-form, optimal solution (calculate the proportion of the total), this is a considerably more complex parametrization with no closed form solution17. After all, instead of having each \\(p_\\theta(b \\mid a)\\) represented by a separate parameter \\(\\theta_{a, b}\\), we now have every parameter being used to compute every conditional probability.\nWe instead use gradient descent to iteratively estimate values of \\(\\theta\\) with lower negative log-likelihood. We set up a simple neural network with 2 hidden layers as follows:\n\nclass LanguageModel(nn.Module):\n    num_embeddings: int = 67\n    features: int = 16\n\n    @nn.compact\n    def __call__(self, x):\n        embed = nn.Embed(self.num_embeddings, self.features)\n        # Get the embedding vectors for each input token\n        x = embed(x)\n        batch_dim, hist_size, features = x.shape\n        x = x.reshape(batch_dim, hist_size * features)\n        # Apply two hidden layers\n        x = nn.Dense(self.features * 4)(x)\n        x = nn.gelu(x)\n        x = nn.Dense(self.features * 4)(x)\n        x = nn.gelu(x)\n        # Get logits for next token prediction\n        x = nn.Dense(self.features)(x)\n        x = embed.attend(x)\n\n        return x\n\nWe optimize for 50,000 steps, and compute the perplexity (over the entire train and validation sequences) every 1000 steps. The training notebook is here:\n\n\nCode\nbigram_nn = np.load('assets/values.npz')\nfig, ax = plt.subplots(figsize=(9,4))\nax.plot(np.arange(1000, 50001, 1000), bigram_nn[\"train_ppls\"], label=\"Train (final PPL={:.2f})\".format(bigram_nn[\"train_ppls\"][-1]))\nax.plot(np.arange(1000, 50001, 1000), bigram_nn[\"val_ppls\"], label=\"Validation (final PPL={:.2f})\".format(bigram_nn[\"val_ppls\"][-1]))\nax.set_xlim(0, 50000)\nax.set_ylim(12.0, 13.2)\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nAnalysis\nFirst, let’s look at the outputs. Recall that the network parametrizes \\(p_\\theta(b \\mid a) = f_\\theta(a)[b]\\). We can compute all the conditional probability vectors \\(p_\\theta(\\cdot \\mid a) = f_\\theta(a)\\) by passing in all 67 unique values of \\(a\\). Doing so, and concatenating these vectors into a matrix, we have a matrix quite similar to that from the previous section:\n\n\nCode\nfig = make_subplots(1, 2, subplot_titles=(\"Free\", \"Constrained (Neural Network)\"))\nfig.add_trace(\n    go.Heatmap(\n        z=params,\n        x=[v.replace(\"\\n\", \"\\\\n\") for v in vocab],\n        y=[v.replace(\"\\n\", \"\\\\n\") for v in vocab],\n        hovertemplate=\"p(%{x}|%{y})=%{z}&lt;extra&gt;&lt;/extra&gt;\",\n        coloraxis=\"coloraxis\",\n    ),\n    row=1,\n    col=1,\n)\nfig.add_trace(\n    go.Heatmap(\n        z=bigram_nn[\"implicit_cond_probs\"],\n        x=[v.replace(\"\\n\", \"\\\\n\") for v in vocab],\n        y=[v.replace(\"\\n\", \"\\\\n\") for v in vocab],\n        hovertemplate=\"p(%{x}|%{y})=%{z}&lt;extra&gt;&lt;/extra&gt;\",\n        coloraxis=\"coloraxis\",\n    ),\n    row=1,\n    col=2,\n)\n\nfig.update_xaxes(title_text=\"Next Token\")\nfig.update_yaxes(title_text=\"Current Token\")\nfig.update_layout(coloraxis_showscale=False, coloraxis={\"colorscale\": \"Greys\"})\n\n\n\n                                                \n\n\nThe similarity arises because, despite the different parametrizations, both of these models have the same objective: produce values of \\(p_\\theta(b \\mid a)\\) for pairs of tokens \\(a, b\\), that minimizes the negative log-likelihood of the training sequence. If for 100 appearances of e, 10 are followed by o, then the maximum likelihood estimate is \\(p_\\theta(\\text{'o'} \\mid \\text{'e'}) = \\frac{10}{100}\\), regardless if its parametrized with individual parameters or a neural network.\nLet’s also look at the embedding vectors associated with each token, after training is completed. Since these are 16-dimensional vectors, we project them down to 2 using PCA:\n\n\nCode\nimport plotly.express as px\nproj_embeds = bigram_nn[\"proj_embeds\"]\nfig = px.scatter(x=proj_embeds[:, 0], y=proj_embeds[:, 1], text=[v.replace(\"\\n\", \"\\\\n\") for v in vocab], width=700, height=700)\nfig.update_traces(textposition='top center')\nfig.show()\n\n\n\n                                                \n\n\nThere’s substantial structure here: the embeddings for the capital letters and lowercase letters form distinct groupings. Conceptually, this makes sense: the next token probabilities (the output) for all capital letter tokens (the input) are likely to have higher probability placed on lowercase letters (than just a lowercase \\(\\rightarrow\\) lowercase transition). Since their outputs share that similarity, their input embeddings should be similar (but not same) too.\n\n\nn-grams\nThe neural network in the previous section had 7,360 free parameters; whereas the matrix of conditional probabilities only had 4,422. The neural net appears to consume more memory and has a higher validation perplexity than the explicit parametrization. But now, instead of conditioning on the previous token, let’s condition on the previous seven:\n\\[\\begin{align}\np(x_1, x_2,..., x_{1024}) &= p(x_{1024} \\mid x_{1023}, ..., x_{1})p(x_{1023} \\mid x_{1022}, ..., x_{1})...\\\\\n&\\approx p(x_{1024} \\mid x_{1023}, ..., x_{1017})p(x_{1023} \\mid x_{1022}, ..., x_{1016})...\\\\\n\\end{align}\\]\nWe continue the “shared across timesteps” assumption; that is, the next token probability only depends on the previous 7, and not what numbered token it is in the sequence. Even then, conditioning on 7 tokens we’d have \\(67^7(67-1) \\approx 10^{14}\\) parameters. Since most combinations of the previous 7 tokens are invalid, we could use a sparse representation, but we’d still need a huge number of samples to accurately estimate next token probabilities.\nBut we could also just parametrize this with a neural network, as \\(p_\\theta(x_i \\mid x_{i-1},...,x_{i-7}) =\\) \\(f_\\theta(x_{i-1},...,x_{i-7})[x_i]\\). And neural networks do not have their number of parameters exponential in \\(m\\). In fact, RNNs and Transformers have a constant number of parameters, independent of \\(m\\)18.\nTraining the network from the previous section (but modified to accept 7 inputs vs 1 input, notebook here), and optimizing, we have:\n\n\nCode\neightgram_nn = np.load('assets/values_8gram.npz')\nfig, ax = plt.subplots(figsize=(9, 4))\nax.plot(np.arange(1000, 50001, 1000), eightgram_nn[\"train_ppls\"], label=\"Train (final PPL={:.2f})\".format(eightgram_nn[\"train_ppls\"][-1]))\nax.plot(np.arange(1000, 50001, 1000), eightgram_nn[\"val_ppls\"], label=\"Validation (final PPL={:.2f})\".format(eightgram_nn[\"val_ppls\"][-1]))\nax.set_xlim(0, 50000)\nax.legend()\nplt.show()\n\n\n\n\n\nThe train perplexity shows a substantial improvement, at 5.80 for this constrained 8-gram model vs 11.87 for the “free” bigram model19. The learned function stores the next-token conditional probability vectors for all \\(67^7\\) possible sequences of prior tokens. Most of this space of \\(67^7\\) sequences are nonsensical (like “gsdaksx”). But of the ones in its training set, it is able to exploit shared structure between similar sequences20 to predict similar conditional probabilities.\n\nConstraints as Generalization\nEarlier in the widgets example, we saw that having \\(p_\\theta(a)=\\theta\\) and \\(p_\\theta(b)=\\theta^2\\) effectively tied their probability estimates together. This has a key consequence: If in a larger sample we saw a lower proportion of outcomes \\(a\\), we couldn’t decrease \\(p_\\theta(a)\\) without also decreasing \\(p_\\theta(b)\\).\nThis behavior carries over when we use a neural network. Consider the following:\n\nWe train a bigram model (parametrized by a neural network), with sequences tokenized at the word-level.\nDuring training, the model learns an embedding vector for \"dog\" that is very close to the one for \"cat\".\nA training minibatch contains the token pair (\"dog\", \"sad\"), and the optimization process attempts to increase \\(p_\\theta(\\text{'sad'} \\mid \\text{'dog'})\\).\n\nThen, \\(p_\\theta(\\text{'sad'} \\mid \\text{'cat'})\\) will also increase as \"dog\" and \"cat\" have quite close embedding vectors. In a way, this is a feature: the network is able to exploit the fact both \"dog\" and \"cat\" are “similar”, and the former being followed by \"sad\" means the latter should also likely be followed by \"sad\"; it is able to generalize this update to another animal.\nThis generalization behavior is pointed out by Bengio et al. (2003). In their example, a well-trained model should assign the same probability to the sentences “The cat is walking in the bedroom” and “A dog was running in a room”. Even if it sees only one during training, at test time the other will have similar embedding vectors, and in turn similar outputs.\nBut this generalization only works holds for a well-trained model, with a large enough corpus such that the embeddings have the correct separation. Suppose that in the “true” corpus, our dogs are sad and our cats are happy. The optimization process will only separate the embeddings sufficiently if we have both pairs (\"dog\", \"sad\") and (\"cat\", \"happy\") in our dataset. Without a requirement to predict different conditional probabilities \\(p_\\theta(\\text{'sad'} \\mid \\cdot)\\) for each, we might have unintended generalization. And this is just one pair of embeddings.\n\nAn Emergent Property\nZooming in between the computational21 and representational22 levels here, we see a macro-level property arises: inputs with similar “meaning” have similar next-token predictions. At no point do we hand-specify the representation of each token; simply tuning the embeddings + weights over a sufficiently large corpus results in the “clustering” of tokens23. Referring back to the bigram model, at the micro scale we do not explicitly optimize the embeddings for uppercase letters for “grouping with other uppercase letters”; it emerges organically.\nAnd this emergent property, of token sequences with similar “meaning” having similar inputs, allows a neural network to effectively compress the conditional probability matrix into its weights, taking up less memory and achieving near transfer."
  },
  {
    "objectID": "blog/lm/index.html#conclusions",
    "href": "blog/lm/index.html#conclusions",
    "title": "Neural Language Models",
    "section": "Conclusions",
    "text": "Conclusions\n\nScaling and Generalization\nAt their core, large language models (LLMs) are not fundamentally different from the models we look at here: they too are functions that produce a conditional probability for the next token, conditioned on tokens already observed. Their functions have considerably more capacity: the largest neural network we look at has 13,504 params and has a context window of exactly 7 tokens. The largest GPT-3 model from Brown et al. (2020) has 175B parameters, and a context window of up to 2048 tokens.\nBut in keeping with the spirit of emergent properties (Anderson, 1972), it’s not merely that these language models are larger; they’re also different. While they obey the “near transfer” properties described here (similar input embeddings \\(\\rightarrow\\) similar outputs), they also exhibit a different kind of generalization called in-context learning (ICL) (Min & Xie, 2022). With ICL, the prompt itself can be composed of a few “examples” (e.g. a few translation input-output pairs), that dramatically improve the model’s next-token prediction capabilities. This is hard to explain with just the “similar embedding vectors” property alone.\nOur current understanding is that this behavior arises from “induction heads” (Olsson et al., 2022) that emerge organically during training in the Transformer networks (Vaswani et al., 2017) that parametrize these language models. One way to view this is a higher-order emergent property24, that exists at the level of substructures in the network parameters (and not just the input embeddings as previously). ICL also appears to be dependent on both the use of Transformers, and the data distribution of natural language (Chan et al., 2022). In summary, our understanding of language models paramterized by neural networks is necessary, but not sufficient to understand LLMs, as there are emergent properties specific to the use of the Transformer architecture.\n\n\nData Distribution\nIn language modeling, we assume our data is i.i.d. drawn from a true distribution \\(p\\). That is, the proportion of a snippet of text in an infinite-sized corpus should be \\(p(\\text{text})\\). But in our corpus, what we have can be better described as samples from \\(p(\\text{text} \\mid \\text{time})\\), with different values of \\(\\text{time}\\).\nThis has consequences: If the context is The top song on the Billboard Hot 100 is, then \\(p_\\theta(\\cdot \\mid \\text{context})\\) will only be able to return conditional probabilities which minimized negative log-likelihood on the training data. The next token to this context snippet, in the real world changes on a weekly basis!\nThis problem is particularly exacerbated with fact-heavy completions. Recall that language modeling doesn’t distinguish between the core structure of language, and one-off facts: they’re all just tokens. The “facts” are stored in the weights of the model, no different than the conditional probabilities for the next token of any input sequence. Two potential ways to improve on this:\n\nEdit model weights: The recent work ROME (Meng, Bau, et al., 2022) introduced a method called “Causal Tracing” to find the subset of model weights that most influence the “next token” conditional probabilities for the “fact tokens” in a sample sentence, and an efficient editing mechanism for those weights. Follow up work then scales this to editing thousands of “stored facts” at once (Meng, Sen Sharma, et al., 2022).\nRetrieval-based LMs: Recent models such as RETRO (Borgeaud et al., 2022) and ATLAS (Izacard et al., 2022) augment a large transformer model with access to a database. Other works such as Lazaridou et al. (2022) directly use tools like search engines to add tokens to their input prompt. The overall effect is a model, which can learn to produce factual evidence by copying from retrieved data than by storing facts in its weights. Then, if the fact source is updated, the model simply copies the new information into its generated output.\n\n\n\nNon-autoregressive models\nThe autoregressive, “generate a single token at a time, left to right” method we explore here is just one way to build a language model. A few recent papers that explore other strategies are25:\n\nThe SUNDAE model introduced in Savinov et al. (2022) trains an autoencoder that iteratively denoises a snippet of text. Of note here is Table 4, where they show results on code-generation experiments, and how SUNDAE can account for context tokens both before and after the token to be generated.\nThe SED model introduced in Strudel et al. (2022) uses diffusion directly on the embedding vectors of tokens. Instead of generating left-to-right, this allows them to iteratively refine an entire snippet of text, at each step.\nFill-in-the-middle models explored in Bavarian et al. (2022) restructure the training data from (prefix, middle, suffix) \\(\\rightarrow\\) (prefix, suffix, middle). This allows a standard left-to-right architecture to be repurposed to “infill” text.\n\nOverall, by leveraging shared structure, language models can store conditional probability tables for an exponential number of possible inputs. Many active lines of research continue to explore how these models work, and how they can be made better. And even today, because so many tasks can be rephrased as token prediction problems, they’re beginning to power a large range of practical products, and their impact will only increase as they improve."
  },
  {
    "objectID": "blog/lm/index.html#footnotes",
    "href": "blog/lm/index.html#footnotes",
    "title": "Neural Language Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPossible meaning “non-zero probability”↩︎\ncompared to other values of \\(\\theta\\)↩︎\nFor example, \\(p(a) = 0.03\\)↩︎\nWe do keep the baseline constraint that the probability of all three outcomes must sum to 1.↩︎\nThat is, the function that stores the probabilities of specific outcomes for both \\(X_1\\) and \\(X_2\\), compared to all other pairs of outcomes.↩︎\n is a character too, so there’s 8 in total.↩︎\nThis carries on the coin logic: the coin doesn’t care if it’s the 1st or the 100th toss, the probability for the outcome heads remains the same.\nLikewise, \\(X_{562}\\) having the same distribution as \\(X_1\\) means the probability for an outcome (e.g. \\(p(\\text{'t'})\\)) is the same at both timesteps.↩︎\nNote an implementation detail: we add a 0.1 as a “starting count” for each of the 67 outcomes. We’ll get to this when we cover bigrams next.↩︎\nFor instance, if ? appears 10x more in the training set compared to the overall corpus, the maximum likelihood estimate \\(p_\\theta(\\text{'?'})\\) will be 10x larger than \\(p(\\text{'?'})\\) .↩︎\nNote that we begin computing perplexity starting at the 8th token, for both the training and validation sequences. Later in the article we’ll build a model estimating probabilities conditioned on the 7 previous tokens; adjusting now means the perplexity comparisons across all models remain comparable.↩︎\n1 is a computational minimum. In practice, language itself has a minimum perplexity larger than 1, and even the best model we’ll ever build won’t go lower.\nAnalogously, even with a perfect estimate of \\(\\theta=0.6\\) for the biased coin earlier, we cannot perfectly predict the sequence of heads and tails in a series of tosses; there is randomness inherent to the process itself.↩︎\nThat is, if \\(x_a = x_b\\), and \\(x_{a-1} = x_{b-1}\\), then \\(p(x_a \\mid x_{a-1}) =\\) \\(p(x_b \\mid x_{b-1})\\).\nThis is a sensible assumption, as \\(p(\\text{'e'}\\mid \\text{'h'})\\) should be the same regardless if e is the 2nd or 285th token: all the information needed to predict it is (assumedly) in the prior token h.↩︎\nSubject to \\(\\sum_b \\theta_{a, b} = 1\\)↩︎\nWe could have technically used a \\(67 \\times 66\\) matrix. For each of the 67 conditional probability vectors, once we know the first 66 probabilities, we can compute the 67th by subtracting the sum from 1. Here it’s just easier to have the extra column.↩︎\nIn the case of infinite width; but even in finite cases, given sufficient width and depth they’re effective.↩︎\nNote that in practice, we’re solving for \\(\\min_\\theta [-\\ln L(\\theta) + R(\\theta)]\\), where \\(R(\\theta)\\) is some regularization term (such as the squared sum of the weights \\(\\lambda \\sum_i \\theta_i^2\\)).↩︎\nAnd many local optima, not just one global optima.↩︎\nThe naive feedforward network we use here has its number of parameters increase linearly with \\(m\\). Here, moving from 1 to 7 inputs, we have an increase from 7,360 to 13,504 params.↩︎\nAnd 11.87 is the lower bound on training perplexity possible with the bigram assumption, since the probability estimates for each conditional probability were at the global minima.↩︎\nJust as the bigram model did by placing all capital letters close to each other in input space.↩︎\nminimize negative log-likelihood↩︎\nthe embedding vectors + network weights↩︎\nAnd this property was what drove key earlier methods such as Word2Vec (Mikolov et al., 2013).↩︎\nExisting in between the computation and representation levels.↩︎\nAnd I note this is hardly exhaustive.↩︎"
  },
  {
    "objectID": "blog/cudajulia/index.html",
    "href": "blog/cudajulia/index.html",
    "title": "CUDA programming with Julia",
    "section": "",
    "text": "As Moore’s Law comes to an end, parallel computing hardware such as GPUs have been fundamental in driving progress in compute-intensive applications across nearly every field, such as bioinformatics, self-driving cars, deep learning, and so on. The core idea is straightforward: instead of having one large task you assign to a very fast CPU, you break the task into many small pieces. Each of these small sub-tasks is then assigned to one of the thousands of cores in a modern GPU, which all complete the sub-tasks in parallel.\nEven though each of the GPU cores individually is weak compared to a CPU core, their sheer number, working on the problem in parallel, can result in speedups of several orders of magnitude.\nIn this blog post, we cover the basics of how to write programs for Nvidia GPUs in the Julia language. We’ll go step-by-step through a function that adds two 1D arrays (vectors) in parallel. By the end of this post, we’ll have added two vectors of a million elements each on the GPU:"
  },
  {
    "objectID": "blog/cudajulia/index.html#but-first",
    "href": "blog/cudajulia/index.html#but-first",
    "title": "CUDA programming with Julia",
    "section": "But first",
    "text": "But first\n\nYou (probably) shouldn’t write GPU code\nBeing able to program GPUs opens up an entire world of parallel programming to you, but it’s also a skill you should only use as necessary. For most common operations, there already exist optimized implementations you should use. For instance, you can add two arrays A and B on the GPU with A .+ B 1 and Julia2 will automatically call optimized code, saving you the need to write a kernel.\nThis ties into the basic programming principle of avoiding the “not invented here” syndrome: in this case, existing libraries already cover the majority of use cases by running GPU operations for you, and you should avoid writing your own GPU code in these cases.\nNonetheless, it’s the edge cases (for which existing GPU implementations do not exist) where innovation happens, such as custom simulations or a new neural network operation. If that’s true and your problem lends itself well to being parallelized, consider the possibility of writing your own GPU code.\n\n\nAnd if you should, consider using Julia\nMost tutorials on CUDA you’ll find online will use C/C++, with good reason: CUDA was originally designed to work with C/C++, which are generally considered high-performance languages. The issue is that they’re also low-level languages, which leads to code that is hard to read and even harder to maintain. For instance, the code to transpose a 2D matrix (without optimizations) looks like this in C++:\n// from https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/transpose/transpose.cu\nconst int TILE_DIM = 32;\nconst int BLOCK_ROWS = 8;\n\n__global__ void transposeNaive(float *odata, const float *idata)\n{\n  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n  int width = gridDim.x * TILE_DIM;\n\n  for (int j = 0; j &lt; TILE_DIM; j+= BLOCK_ROWS)\n    odata[x*width + (y+j)] = idata[(y+j)*width + x];\n}\nIn comparison, the equivalent Julia code compiles to the same GPU instructions as the C++ version while being considerably more readable:\nconst TILE_DIM = 32\nconst BLOCK_ROWS = 8\n\nfunction transpose!(odata, idata)\n    x = (blockIdx().x - 1) * TILE_DIM + threadIdx().x \n    y = (blockIdx().y - 1) * TILE_DIM + threadIdx().y \n\n    for j in 0:BLOCK_ROWS:TILE_DIM-1\n        odata[x, y + j] = idata[y + j, x]\n    end\nend\nThis is the core advantage: Julia is a high-level language, and CUDA.jl (the Julia library that interfaces with CUDA) allows us access to several conveniences, such as\n\nNot having to work with direct pointers to GPU memory. Instead, we work with the CuArray type, which behaves mostly the same as the regular Array type on the CPU\nIn C++, we’d have to allocate GPU memory and then copy the data from the CPU to the GPU ourselves. In Julia, we use the cu function on a CPU Array, and this function handles the entire transfer process for us, directly returning us a CuArray .\nWe can write generic kernels. In C++, we have to specify the type of GPU inputs (floats, in the case above). For instance, if we wanted an addition kernel that’d work with integers, it’d have to be written separately. In Julia, the kernel is compiled on-the-fly by default when new input data types are passed in, which means the same code above would work both on floats and ints.\n\n\n\nThe two language problem\nWriting our kernels in Julia also allows one other key advantage: avoiding the two language problem. Today, most high-performance computing libraries are written in two languages: the compute-intensive sections are written in C++ and are then accessed by the end-user with “glue” code written in a higher-level language, like Python. This is the pattern used in major libraries like TensorFlow or PyTorch.\nThe problem with this is double-fold: Firstly, your library becomes harder to maintain (since low-level languages are inherently more mentally taxing). Secondly, your end-users (who might only know Python) are unable to contribute much to improving the performance of your code (since that would be in C++).\nBy having all your code in one language, not only does it become easier for you to reason through the code, but you’re far more likely to catch bugs in an open-source setting, as your end users can understand and contribute to the code as well."
  },
  {
    "objectID": "blog/cudajulia/index.html#the-cuda-programming-model",
    "href": "blog/cudajulia/index.html#the-cuda-programming-model",
    "title": "CUDA programming with Julia",
    "section": "The CUDA Programming Model",
    "text": "The CUDA Programming Model\nParallel programming requires one big, obvious paradigm shift from the standard programming models you are used to: your code will be run on thousands of cores at the same time. To make the most of this new paradigm, there are two concepts we need to understand: kernels and the thread hierarchy.\n\nThreads\n\n\n\n\n\nIt is helpful to think of each thread as a worker that is assigned a small sub-task of the overall task. Note that the number of threads is not the number of cores on a GPU; the thread is a logical unit, whereas the core is a physical unit. For large problems, you can request threads far in excess of the number of cores; the GPU will begin running as many of those threads as it physically can and run more threads as the already running threads are completed. Logically, from the programmer’s point of view, all the threads you request are run at the same time; CUDA takes care of the actual scheduling and ordering of the threads for you.\nCUDA then allows you, the programmer, to not worry about the details of the parallelization. All you need to do is break the task down so that each thread has a clearly defined sub-task (e.g. when adding two 1D vectors \\(a\\) and \\(b\\) the ith thread performs \\(a_i + b_i\\)), and then request as many threads as needed to solve the entire problem.\n\n\nKernels\nWith ordinary functions, when it is called once, it is also executed once. When a function is called once, through CUDA, the function is executed \\(N\\) times by each of the \\(N\\) threads on the GPU. These functions are called kernels, and each of the three code snippets we’ve seen so far is a kernel.\nA fragment of code very common when writing kernels is for a thread to compute what numbered thread it is globally. Here, this happens on line 5:\nconst n = 2^20 # 1048576\nconst THREADS_PER_BLOCK = 256\n\nfunction add!(c, a, b)\n    x = (blockIdx().x - 1) * blockDim().x + threadIdx().x \n\n    @inbounds c[x] = a[x] + b[x]\n    return\nend\n\n@cuda threads=THREADS_PER_BLOCK blocks=n÷THREADS_PER_BLOCK add!(C, A, B)\nThe computed value of x changes on the thread executing it (thread 1 will obtain x = 1, whereas thread 1252 will obtain x = 1252). Then, each thread subsequently accesses and sums together different parts of the input on line 7. This means every single thread carries out the same instruction, just on different parts of the data, leading to the acronym SIMT: Single Instruction, Multiple Threads. This approach mean we only have to do two things to achieve parallelism:\n\nWrite one set of instructions (i.e. one kernel function) for all the threads to carry out (i.e. execute)\nEnsure each thread accesses the correct part of the input for its sub-task.\n\n\n\nThread Hierarchy\n\nBlocks\n\n\n\n\n\nFor tasks more interesting than just basic addition, threads may need to work with each other. Hence, with CUDA, we don’t simply carry out “launch 1,048,576 threads”. Rather, threads are grouped into blocks, and we ask CUDA to “launch 4,096 blocks, with 256 threads each”.\nWhen run, threads in the same block have access to a shared memory that they can use to transfer information between each other. This memory resides directly on the GPU chip, so accessing it has very low latency. A common strategy is to break a large problem (e.g. multiplying a \\(512 \\times 512\\) matrix) into many smaller problems (e.g. multiply many \\(8 \\times 8\\) matrices), and then for a block of threads to load into shared memory only the part of the original problem it then needs to work on together.\nImportantly, different blocks run independently of each other and must be able to run in any order. While this limits communication between blocks, this constraint produces two advantages:\n\nIt allows the GPU to run blocks in parallel without causing any side effects since each block is programmed to run independently.\nIt allows the programmer to focus on implementing code only across the threads of one block correctly (instead of every single thread on the GPU) because it can then be automatically parallelized across the entire problem.\n\nIn order to have access to shared memory, threads in the same block are run on the same physical SM, that is, a streaming multiprocessor (a group of 64 CUDA cores). This results in a practical constraint: a block cannot have more than 1024 threads, and CUDA enforces this.\n\n\nGrid\n\nThe grid refers to all the blocks scheduled to run for a problem. Like threads and blocks, it is a logical (and not a physical) construct. Logically, from the programmer’s point of view, all blocks on the grid execute in parallel; physically, the GPU will run as many blocks as it can on all available SMs in parallel and start running new blocks as the existing ones are completed."
  },
  {
    "objectID": "blog/cudajulia/index.html#writing-a-kernel",
    "href": "blog/cudajulia/index.html#writing-a-kernel",
    "title": "CUDA programming with Julia",
    "section": "Writing a kernel",
    "text": "Writing a kernel\n\nThe 1D add kernel\nWe now return to a more fleshed-out implementation of our code snippet from earlier. The code below does three things:\n\ncreates three 1D arrays (i.e. vectors) of length 1,048,576 and sends them to the GPU\ndefines an addition kernel\nlaunches the kernel with the correct number of blocks and threads to perform the actual calculation\n\nFor simplicity, we set the length of the vector to be \\(2^{20}=1048576\\), a number divisible by the number of threads we assign per block (256).\nusing CUDA \n\nconst n = 2^20 # 1048576, number of elements in 1D arrays\nconst THREADS_PER_BLOCK = 256\n\n# create a vector [0.0, 0.0, 0.0...], and send to GPU\nC = zeros(Float32, n) |&gt; cu \n\n# create two vectors, fill them with 1s and 2s, and send to GPU\nA = fill(1.0f0, n) |&gt; cu \nB = fill(2.0f0, n) |&gt; cu\n\nfunction add!(c, a, b)\n    # compute the thread_id\n    x = (blockIdx().x - 1) * blockDim().x + threadIdx().x \n\n    # i'th thread should get the i'th elements of a and b \n    # sum them, and then store them in i'th element of c\n    @inbounds c[x] = a[x] + b[x]\n    return\nend\n\nthreads = THREADS_PER_BLOCK # 256\nblocks = n ÷ threads # 4096\n\n# launch the kernel with 4096 blocks, of 256 threads each\n@cuda threads=threads blocks=blocks add!(C, A, B)\n\n\nSending data to the GPU\nWe create a vector C, to hold our output values, as well as vectors A and B that are filled with 1s and 2s, respectively. We then use the cu function described earlier3 which takes in an Array on the CPU, and sends it to the GPU, returning a CuArray .\n# create a vector [0.0, 0.0, 0.0...], and send to GPU\nC = zeros(Float32, n) |&gt; cu \n\n# create two vectors, fill them with 1s and 2s, and send to GPU\nA = fill(1.0f0, n) |&gt; cu \nB = fill(2.0f0, n) |&gt; cu\nOne thing of note is that all three vectors have 32-bit (“single-precision”) floats4 as opposed to 64-bit (“double-precision”) values. GPUs perform better when working with single-precision data, and unless you have good reason to be very numerically precise (e.g. simulating a chaotic system), single-precision data should more than suffice.\n\n\nZooming in\nLet’s zoom in on the kernel itself, free of its boilerplate:\nfunction add!(c, a, b)\n    # compute the thread_id\n    x = (blockIdx().x - 1) * blockDim().x + threadIdx().x \n\n    # i'th thread should get the i'th elements of a and b \n    # sum them, and then store them in i'th element of c\n    @inbounds c[x] = a[x] + b[x]\n    return\nend\nThere are four things of note going on here:\n\nInside a kernel, a thread has access to special functions. Here, we use three:\n\nblockIdx() : returns what block number the thread is part of\nblockDim() : returns the number of threads in that block\nthreadIdx() : returns which thread in the block the current thread is\n\n\nFor instance, when the 3rd thread in the 3rd block calls these functions, it will be evaluated in the way shown below5, resulting in a global index of 515.\n\n\nOnce we have the thread’s global index, we index out the relevant values from the a and b vectors, add them and then store them into the corresponding location in c on line 7.\nJulia typically performs bounds checking to make sure we’re not using an index value outside the boundaries of an array. While useful with typical CPU code, this comes with a performance hit. To make the most of the GPU’s capabilities, it is recommended to use the @inbounds macro as we do on line 7, which tells Julia to disable bounds checking for that line.\nOne important aspect of kernels is that they cannot return values directly. All changes they make have to be in the form of changes to their inputs6, which is why we use vector C to store all the output values. On line 8, we only have the keyword return, which instructs the function to have no return value.\n\n\n\nLaunching the kernel\nOnce we have our data and our kernel, we need to actually launch our kernel, which we can do using the @cuda macro:\nthreads = THREADS_PER_BLOCK # 256\nblocks = n ÷ threads # 4096\n\n# launch the kernel with 4096 blocks, of 256 threads each\n@cuda threads=threads blocks=blocks add!(C, A, B)\nHere, we choose to use 256 threads per block. Since each thread is processing one element, and we have 1,048,576 elements total, we will need 1,048,576 threads. We calculate the number of blocks (of 256 threads) we’ll need to have that many threads, and it comes out to 4,096 blocks.\nWe then launch the kernel using the @cuda macro, passing in the number of threads and blocks we’d like to use. This modifies the array C in-place, replacing its 0-initialized values with the ones from our computation. Hence, we get C = [3.0, 3.0,…, 3.0] .\n\n\nQuick check\nTake a minute, and try to figure out what would happen to the vector C if we initialized C to a vector of 0’s, and ran the following: @cuda threads=1 blocks=1 add!(C, A, B) i.e. if we launched the kernel with only one block and one thread in that block.\n\n\\(\\cdot \\cdot \\cdot\\)\n\nIn this case, there’s one thread overall. That thread computes its index to be 1 and updates the first element of C with A[1]+B[1]. Since there are no more threads, all other elements of C remain untouched and have their initial value of 0. Then, C = [3.0, 0.0,…, 0.0]."
  },
  {
    "objectID": "blog/cudajulia/index.html#finishing-the-kernel",
    "href": "blog/cudajulia/index.html#finishing-the-kernel",
    "title": "CUDA programming with Julia",
    "section": "Finishing the kernel",
    "text": "Finishing the kernel\nNow that we understand how to write kernels, let’s improve the addition kernel into a function an end-user can use.\n\nHandling all input sizes\nThe current implementation of the kernel can only process vectors whose number of elements is a multiple of the number of threads per block. To lift this constraint, we make the two following changes:\n\nPreviously, we performed direct integer division to get the number of blocks (blocks = n ÷ threads). This is only sensible when the length of the vector is an integer multiple of the number of threads. Instead, now we perform regular division and then round up:\n\nthreads = THREADS_PER_BLOCK # 256\n\n# calculate number of blocks needed to get total threads\n\n# equal to number of elements, then round *up* to nearest integer\n# this previously was: blocks = n ÷ threads\nblocks = ceil(Int64, length(C)/threads) \n\n# launch the kernel with specified number of blocks, of 256 threads each\n@cuda threads=threads blocks=blocks add!(C, A, B)\n\nThat is, we launch the minimum number of blocks such that the number of total threads is equal to or more than the number of elements in our vector.\n\n\nNow, we might have more threads than entries in the vector. For instance, if the vector has 1,000,000 elements, 3907 blocks will be launched, with 1,000,192 threads total. When thread 1,000,101 will try to obtain the 1,000,101st element of C it’ll face an indexing error. To avoid this, we update our kernel as such:\n\nfunction add!(c, a, b)\n    # compute the thread_id\n    x = (blockIdx().x - 1) * blockDim().x + threadIdx().x \n\n    # only perform action if thread id is not greater than the number of elements\n    if x &lt;= size(c)[1]\n        # i'th thread should get the i'th elements of a and b \n        # sum them, and then store them in i'th element of c\n        @inbounds c[x] = a[x] + b[x]\n    end\n    return\nend\n\nFor threads with an index greater than the number of elements, the if condition is never met, and they simply go to the next line (the return on line 11), finish running and are marked by the GPU as completed.\n\n\n\nPreparing for the end-user\nTo run our custom kernel on the GPU we need to create an appropriately sized output vector, as well as launch the kernel with the correct number of blocks and threads. This is fine for prototype code, but if we want this to be used by others (who presumably only care about adding two vectors), we should simplify our code by writing a “companion function” for the kernel, that launches the kernel for the user:\nfunction add(a::CuVector, b::CuVector)\n    # check a and b have the same length\n​    if length(a) != length(b)\n​        throw(DimensionMismatch(\"A and B must have same number of elements; A has $(length(a)) elements, B has $(length(b)) elements\"))\n​    end\n\n    # number of elements for vector c\n    n = length(a)\n    \n    # get the types of elements of vector a and b and\n    # use that to compute the element type for vector c\n    typea = eltype(a)\n    typeb = eltype(b)\n    typec = promote_type(type_a, type_b)\n    \n    # create a vector of zeros (directly on the GPU)\n    # of length n and type typec \n    c = CUDA.zeros(typec, n)\n    \n    # compute the number of threads and blocks\n    threads = THREADS_PER_BLOCK # 256\n    blocks = ceil(Int64, n/threads) \n    \n    # launch the kernel\n    @cuda threads=threads blocks=blocks add!(c, a, b)\n    \n    return c\nend\nSuch functions will usually do three things: First, check any necessary conditions are met. Second, create new arrays to hold the output. Finally, launch the kernel.\n\n\nChecking conditions\nHere, we check whether both inputs are indeed 1-D arrays on the GPU.\nfunction add(a::CuVector, b::CuVector)\n    # check a and b have the same length\n    if length(a) != length(b)\n        throw(DimensionMismatch(\"A and B must have same number of elements; A has $(length(a)) elements, B has $(length(b)) elements\"))\n    end\nThis is done by adding7 ::CuVector8 to the function argument. We then also check that a and b have the same number of elements, throwing a DimensionMismatch error otherwise.\n\n\nCreate output arrays\nIn this case, we need to create an output array c to hold the results of our computation. The length of the vector should be n, but what should be the type of its elements? For instance, if we’re adding two vectors of Float32 values, then c should be Float32, but what if a is CuVector{Float32}9 while b is CuVector{Int32} ?\nOur goal is to write a generic function, that is, it should work for all possible combinations of the element types of a and b. CUDA.jl will automatically compile new versions of the kernel, but we also need to make sure that the output vector c is of the correct type.\ntypea = eltype(a)\ntypeb = eltype(b)\ntypec = promote_type(type_a, type_b)\n\nc = CUDA.zeros(typec, n)\nWe do this by using the standard Julia functions for this purpose: use eltype to get the element types of a and b, then use promote_type to get the correct type needed for c . For instance:\n\nif a and b are of types CuVector{Float32} and CuVector{Int32} respectively\nthen, typea and typeb are Float32 and Int32\nthen, typec is computed to be Float32 under the rules of type promotion in Julia\n\nWe can then create a vector of zeros of typec elements and of length n directly on the GPU using CUDA.zeros(typec, n) .\n\n\nLaunching the kernel, v.2\nThis proceeds as previously: we calculate the number of threads and blocks as previously and launch the kernel. The only difference is that this time, the kernel launch happens inside a function that the end-user does not have to interact with. We then return the vector holding the outputs, c, as the output of the companion function.\nWith our companion function written, all our end-user has to do now to add two CuVectors A and B is call C = add(A, B) . This will run the necessary checks, allocate an output array and launch the required kernel. The user then gets returned a new vector C with the results of the addition.\n\n\nPerformance\nI ran a quick, informal benchmark of the kernel on the task of summing two vectors with 10 million elements each, against the default way to do this on a CPU and GPU (which is to use A .+ B , as previously mentioned). I obtained the following10:\n\n\n\nFunction Call\nTime (ms)\n\n\n\n\nA .+ B (CPU, Xeon on GCP)\n17.11\n\n\nadd(A, B) (GPU, Tesla T4)\n0.651\n\n\nA .+ B (GPU, Tesla T4)\n0.476\n\n\n\nOur barebones implementation on the GPU is already over 26x faster than the CPU implementation (it still lags behind Julia’s optimized kernels, which are nearly 36x faster). This ties into the message at the start of this blog post: for the right problems, GPUs are orders of magnitude faster than CPUs. At the same time, to make the most of this advantage, focus your efforts on writing kernels for problems where optimized ones don’t already exist.\n(We’ll begin exploring concepts related to optimizing GPU code, such as shared memory, warps, and thread divergence over the next few blog posts)"
  },
  {
    "objectID": "blog/cudajulia/index.html#conclusion",
    "href": "blog/cudajulia/index.html#conclusion",
    "title": "CUDA programming with Julia",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough there aren’t as many resources for CUDA programming in Julia as compared to for C++, skimming through them can still be very beneficial to learn tips and tricks that can be translated into Julia code. I would recommend the CUDA Programming Guide for more details on how to program Nvidia GPUs and Mark Harris’s blog posts (one of his posts, An Even Easier Introduction to CUDA was the source of inspiration for this blog post).\nParallel programming is a powerful skill to have in an increasingly data-heavy world, and Julia’s support for it is poised to make it more accessible than ever before. I do hope that, having read this blog post, the snippet of code at the start feels more familiar now. We’ve just scratched the surface, and I intend to write more about CUDA programming in Julia over the coming weeks.\nUpdate, Oct 2022: A lot has happened since I originally wrote this! Even though I still find Julia pretty cool, these days I find myself spending most of my time in Python. I might write a follow-up at some point, especially as the infrastructure surrounding Julia continues to mature."
  },
  {
    "objectID": "blog/cudajulia/index.html#footnotes",
    "href": "blog/cudajulia/index.html#footnotes",
    "title": "CUDA programming with Julia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use dot syntax here, which automatically converts the + operator to a vectorized version that is applied over the entire array↩︎\nIn this case specifically, using the dot syntax results in Julia fusing the operation and creating a custom kernel under the hood.↩︎\nYou might notice instead of directly calling cu on an input X (i.e. cu(X)), we use this syntax: |&gt; cu. |&gt; is the pipe operator, and x |&gt; f is equivalent to f(x), that is, the input is “piped” into the function. While subjective, I would consider this an idiomatic way to use cu, as the input is “piped” to the GPU.↩︎\nThe fill(1.0f0, n) syntax means “create a vector of length n, filled with 1.0f0”. The 1.0 is the value, and the f0 at the end tells Julia this is a Float32.↩︎\nNote that the indices for both the threads and blocks start at 1. This is to remain consistent with Julia’s 1-indexing.\n(For comparison, in C++ the functions return indices starting at 0. You can see this behavior in the blog post that originally inspired this diagram)↩︎\nThis is also why kernels have their function names ending with !. The Julia convention for functions that modify one or more of their arguments is to append a ! to the name.↩︎\nIn Julia, x::type is a type annotation, that “x is of type type”.↩︎\nCuVector is a type alias for CuArray, defined by CuVector{T} = CuArray{T,1}. That is, a CuVector is a CuArray where the elements are of any type, but the CuArray is only one dimensional.↩︎\nthat is, a CuVector whose elements are Float32 values↩︎\nA and B are randomly initialized Float32 arrays. For the CPU version, A .+ B is called directly, while for the GPU version, A and B are moved to the GPU first before the addition.\nThe benchmarks (done using BenchmarkTools.jl’s @benchmark macro) cover the time needed to allocate the output array and perform the computation. (They do not cover the time needed to move the arrays to the GPU)↩︎"
  },
  {
    "objectID": "blog/spherical-harmonics/index.html",
    "href": "blog/spherical-harmonics/index.html",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "",
    "text": "Spherical harmonics are powerful mathematical tools, allowing us to represent any function on a sphere as the sum of simpler basis functions (much like a Fourier series!). This post aims to explain spherical harmonics through the lens of Fourier series, by “lifting” them from a circle to a sphere, extensively relying on visual explanations.\nCode\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport numpy as np"
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#functions-on-circles",
    "href": "blog/spherical-harmonics/index.html#functions-on-circles",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Functions on Circles",
    "text": "Functions on Circles\nSuppose we have a periodic function \\(f(\\theta)\\), with a period \\(2\\pi\\). As an example, let \\(f(\\theta) = \\sin(\\theta) - 0.5\\cos(2\\theta) + 0.25\\sin(3\\theta)\\). This is the standard way to view the function: angle \\(θ\\) on the x-axis, \\(f(θ)\\) on the y-axis.\n\n\n\nThe red box denotes one full cycle, which we can see is from 0 to 2π\n\n\nSince the function is periodic, there is an equivalent way to view this: as a function defined on a circle with radius 1 (and in turn, a circumference of 2π). The period here is represented counter-clockwise (as is convention in trigonometry). Just as in the previous plot, we can see that the function first rises to a large “hill”, before descending into two small “valleys”, returning to the starting point.\n\n\n\nColor denotes the value of the function at \\(x = cos(θ), y = sin(θ)\\)\n\n\n\nFourier Series\nOne of the most widely used discoveries in mathematics is that any real-valued, periodic function1 can be represented as the weighted sum of sines and cosines2 of varying frequencies, called a Fourier Series. Specifically, for a function with a period of \\(2\\pi\\), there exists weights \\(a_n\\) (weights for the cosines) and \\(b_n\\) (weights for the sines) such that:\n\\[f(\\theta) = \\frac{a_0}{2} + \\sum_{n=1}^\\infty a_n \\cos(n\\theta) + b_n \\sin(n\\theta)\\]\nThe process of finding the exact weights belong to the study of Fourier Analysis. In our case, since the function we’ve chosen is already neatly written as a sum of sines and cosines, \\(f(\\theta) = \\sin(\\theta) - 0.5\\cos(2\\theta) + 0.25\\sin(3\\theta)\\), the weights can be read off directly: \\(b_1 = 1\\), \\(a_1 = -0.5\\) and \\(b_3 = 0.25\\); all the other weights are zero3. We can also visualize \\(f(θ)\\) with the relevant sine and cosine functions (a.k.a the “basis functions”)4.\n\n\n\nThe blue function is obtained as a weighted sum of the red functions\n\n\nNote that for a finite number of terms, this is exact only when the function can be expressed in terms of sines and cosines; otherwise (with function such as say, a square wave) this only exact when the sum uses an infinite number of terms.\n\n\n\nThe approximation by trigonometric functions becomes increasingly more accurate as more terms are added. (Figure by Thenub314, original here)\n\n\nLooping back5 to earlier, these basis functions can also be represented on a circle:\n\n\n\nAs would be expected, \\(sin(θ)\\) only finishes one “cycle” on the circle, whereas \\(sin(3θ)\\) finishes 3.\n\n\nThe takeaway here is that even though the circle plots look very different from the standard plot (with \\(θ\\) on the x-axis), they’re simply two different ways of depicting the exact same function on a circle."
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#spherical-harmonics",
    "href": "blog/spherical-harmonics/index.html#spherical-harmonics",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Spherical Harmonics",
    "text": "Spherical Harmonics\nIf you’ve followed the visuals so far, you already know what a spherical harmonic is: instead of basis functions defined on a circle, they’re basis functions defined on a sphere 6. Just like how \\(\\sin(\\theta)\\) forms a function on a circle, the spherical harmonics are functions on a sphere. Here’s one (it’s interactive!):\n\n\nCode\nfig = make_subplots(rows=1, cols=1, specs=[[{'is_3d': True}]])\n\ns = np.linspace(0, 2 * np.pi, 100)\nt = np.linspace(0, np.pi, 100)\ntGrid, sGrid = np.meshgrid(s, t)\n\nr = 1\nx = r * np.cos(sGrid) * np.sin(tGrid)\ny = r * np.sin(sGrid) * np.sin(tGrid)\nz = r * np.cos(tGrid)\n\n\nfig.add_trace(\n  go.Surface(x=x, y=y, z=z, \n    surfacecolor=np.sqrt(3/4*np.pi)*y, \n    colorscale='RdBu', \n    colorbar=dict(thickness=10)\n  )\n)\n\nfig.update_layout(\n    font_family=\"JuliaMono\",\n    showlegend=False,\n    margin=go.layout.Margin(l=0, r=0, b=0, t=0),\n    paper_bgcolor='rgba(0,0,0,0)',\n)\n\nfig.update_traces(showscale=False)\nfig.show()\n\n\n\n\n                                                \nInteractive 1: Here, red is lower, blue is higher. This is the harmonic Y₁,₋₁\n\n\n\nOn the circle, Fourier Analysis allows us to convert any function defined on a circle (i.e. a periodic function) into the weighted sum of sines and cosines of varying frequencies. Now, on the sphere, any function on a sphere can be decomposed into a weighted sum of the spherical harmonic functions \\(Y_{lm}\\). That is, there exist \\(a_{lm}\\) such that:\n\\[\n\\begin{align*}\nf(x, y, z) &= \\sum_{l=0}^\\infty \\sum_{m=-l}^l a_{lm} Y_{lm}(x,y,z) \\\\\n&\\text{where } x^2+y^2+z^2 = 1\n\\end{align*}\n\\]\nThere’s a few things to note here:\n\nDegree l: Instead of one sum as in a Fourier series, there’s two here. The index of the outer sum, \\(l\\) can be thought of as frequency. Just like how \\(sin(θ)\\) and \\(sin(2θ)\\) refer to one and two cycles on the circle, a harmonic with \\(l=1\\) has one cycle, whereas a harmonic with \\(l=2\\) has two, as we can see. \\(l\\) is the “degree” of the function.\n\n\n\n\nOne loop around the circle on the x-y plane would result in one “cycle” on left (degree 1), and two on the right (degree 2)\n\n\n\nOrder m: The inner sum corresponds to the fact there are \\((2l+1)\\) functions of degree l. (there are 3 functions of degree 1, 5 of degree 2, and so on). This diverges from the Fourier series, where each “frequency” \\(n\\) had 2 functions. To distinguish them from each other, each function also has an “order” \\(m\\). Here’s all three functions of degree 1:\n\n\n\n\nThe three degree-1 harmonics change across the y, z and x axis respectively. Higher-degree harmonics take on more complex forms.\n\n\n\nConstant term: Just as the Fourier series has the \\(\\frac{a_0}{2}\\) term (which can be seen as \\(\\frac{a_0}{2} \\cos(0\\theta)\\), as \\(\\cos(0\\theta) = 1\\)), the spherical harmonics have a constant term in \\(a_{00}Y_{00}\\), as the harmonic \\(Y_{00}\\) is a function that has the same value everywhere on the circle.\n\n\n\n\nLeft, we have the “0th” frequency term in the Fourier series. Right, we have the 0th degree spherical harmonic. Both have constant value over their domains.\n\n\n\nReal vs Complex: Both indices are subscripts on \\(Y\\), i.e. of the form \\(Y_{lm}\\). This convention reflects that we’re looking at the real spherical harmonics here (complex ones have indices of the form \\(Y_l^m\\))\nCoordinates: Note that we’re using cartesian coordinates (x, y, z) as inputs to the spherical harmonics, not angles. We could equivalently use harmonics of the form \\(Y_{lm}(\\theta, \\phi)\\) to stay consistent with the Fourier series, but they’re simpler in cartesian form (and as we’ll see in a bit, equivalent)."
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#harmonics-visualization",
    "href": "blog/spherical-harmonics/index.html#harmonics-visualization",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Harmonics: Visualization",
    "text": "Harmonics: Visualization\nOften, spherical harmonics are not depicted on the sphere directly, but in a different form. For instance, in the alternate depiction \\(Y_{1, -1}\\) looks as such:\n\n\nCode\nfig = make_subplots(rows=1, cols=1,\n                    specs=[[{'is_3d': True}]])\n\ntheta = np.linspace(0, np.pi, 100)\nphi = np.linspace(0, 2*np.pi, 100)\ntheta, phi = np.meshgrid(theta, phi)\n\nx = np.sin(theta) * np.cos(phi)\ny = np.sin(theta) * np.sin(phi)\nz = np.cos(theta)\n\nr0a = np.sqrt(3/4*np.pi)*y\nr0 = np.abs(r0a)\nfig.add_trace(\n  go.Surface(\n    x=r0*x, y=r0*y, z=r0*z, \n    surfacecolor=r0a, \n    colorscale='RdBu'\n  )\n)\n\n\nfig.update_layout(\n    font_family=\"JuliaMono\",\n    showlegend=False,\n    margin=go.layout.Margin(l=0, r=0, b=0, t=0),\n    paper_bgcolor='rgba(0,0,0,0)',\n)\n\nfig.update_layout(\n  scene = dict(\n    xaxis = dict(nticks=4, range=[-1.8,1.8],),\n    yaxis = dict(nticks=4, range=[-1.8,1.8],),\n    zaxis = dict(nticks=4, range=[-1.8,1.8],)\n  ),\n  scene1_aspectratio=dict(x=1, y=1, z=1)\n)\n\nfig.update_traces(showscale=False)\nfig.show()\n\n\n\n\n                                                \nInteractive 2: The value of the function is dual-coded in plots like these: both in distance from origin, and the color. Note this is the same harmonic as in the previous interactive diagram, Y₁,₋₁\n\n\n\nNote that while it appears that \\(Y_{1, -1}\\) is defined over two mini-spheres, this is incorrect! The harmonic is still defined over the sphere; this plot simply represents it in a different way. Previously, we plotted a surface through all points on the sphere, that is:\n\\[\n\\{\\hspace{0.2cm} \\mathbf{x} \\hspace{0.2cm} \\bigl\\vert \\hspace{0.2cm}  \\| \\mathbf{x} \\|_2 = 1\\}\n\\]\nNow, we’re plotting a surface through all points:\n\\[\n\\{\\hspace{0.2cm} \\mathbf{x} \\lvert f(\\mathbf{x}) \\rvert \\hspace{0.2cm}  \\bigl\\vert \\hspace{0.2cm}  \\| \\mathbf{x} \\|_2 = 1\\} \\\\\n\\]\nSpecifically, we allow the magnitude of the function \\(\\lvert f(x,y,z) \\rvert\\) to modulate how far the point on the surface in that direction is from the origin. When the magnitude is small, the surface is “shrunk” closer to the origin; where it’s large, it is “expanded” away from the origin, such that the distance to the origin is the magnitude of the function in that direction. The surface is still colored by value; here, blue still represents positive values, red represents negative values.\nAs we can see, on the plane \\(y=0\\) (where \\(Y_{1, -1} = 0\\)), the points have indeed been shrunk directly to the origin. Note the underlying function is the exact same; only thing changed is how we visualize it.\n\nDropping down to 2D\nTo really visually grasp this, it’s helpful to drop back down to the 2D case. Let’s do the shrink and expand procedure we just described to \\(\\sin(\\theta)\\). We end up with the following plot. Remember, this is only a visual aid, \\(\\sin(\\theta)\\) is still only defined on the circle and the left is the “true” plot.\n\n\n\nThe plot on the right looks similar to the 3D case; it should, as both \\(sin(θ)\\) and Y₁,₋₁ represent “one full cycle” components in their respective series.\n\n\nCompleting the visual analogy, here’s the original and alternate version diagrams for \\(Y_{1, -1}\\) side-by-side:\n\n\n\nSo, why this?\nThe alternate visual depiction of the harmonics may seem needlessly complicated at first. But they serve an important purpose: as the harmonics become more complex as the degree goes up and we begin linearly combining them together, the alternate form allows a clearer depiction of how different values of the function on the sphere compare to each other. For example, here’s \\(Y_{4, 0} + 2Y_{2, -1}\\):\n\n\nCode\nfig = make_subplots(rows=1, cols=1,\n                    specs=[[{'is_3d': True}]])\n\ntheta = np.linspace(0, np.pi, 100)\nphi = np.linspace(0, 2*np.pi, 100)\ntheta, phi = np.meshgrid(theta, phi)\n\nx = np.sin(theta) * np.cos(phi)\ny = np.sin(theta) * np.sin(phi)\nz = np.cos(theta)\n\nr0a = 3/16*np.sqrt(1/np.pi)*(35*z**4-30*z**2+3) + 2*(1/2*np.sqrt(15/np.pi)*y*z)\nr0 = np.abs(r0a)\n\nfig.add_trace(\n  go.Surface(\n    x=r0*x, y=r0*y, z=r0*z, \n    surfacecolor=r0a, \n    colorscale='RdBu'\n  )\n)\n\n\nfig.update_layout(\n    font_family=\"JuliaMono\",\n    showlegend=False,\n    margin=go.layout.Margin(l=0, r=0, b=0, t=0),\n    paper_bgcolor='rgba(0,0,0,0)',\n)\n\nfig.update_layout(\n  scene = dict(\n    xaxis = dict(nticks=4, range=[-1.8,1.8],),\n    yaxis = dict(nticks=4, range=[-1.8,1.8],),\n    zaxis = dict(nticks=4, range=[-1.8,1.8],)\n  ),\n  scene1_aspectratio=dict(x=1, y=1, z=1)\n)\n\ncamera = dict(eye=dict(x=1.5, y=1.5, z=1.5))\nfig.layout.scene1.camera = camera\n\nfig.update_traces(showscale=False)\nfig.show()\n\n\n\n\n                                                \nInteractive 3: We can immediately tell this function has two large negative regions on the sphere, and two smaller positive ones, as well as which way they’re oriented."
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#harmonics-algebra",
    "href": "blog/spherical-harmonics/index.html#harmonics-algebra",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Harmonics: Algebra",
    "text": "Harmonics: Algebra\nYou may have noticed I’ve been referring to the harmonics by their degree and order (such as \\(Y_{1,1}\\)), but not their exact algebraic forms. Unlike the Fourier series (where each of the \\(n^{th}\\) sine and cosine terms are simply \\(\\sin(n\\theta)\\) and \\(\\cos(n\\theta)\\)), the spherical harmonics are a more complex set of functions7; here’s some of them8:\n\\[\n\\begin{align*}\nY_{00}(x, y, z) &= \\frac{1}{2} \\sqrt{\\frac{1}{\\pi}} \\\\\nY_{1, -1}(x, y, z) &= \\sqrt{\\frac{3}{4\\pi}} y \\\\\nY_{3, -2}(x, y, z) &= \\frac{1}{2}\\sqrt{\\frac{105}{\\pi}} xyz \\\\\n\\end{align*}\n\\]\nThese are the Cartesian versions of the spherical harmonics, but it is important to know that conventions vary significantly across fields (for instance, the quantum mechanics community implictly places a factor of \\((-1)^m\\) in every harmonic, whereas the magnetics community does not). It is well advised to check which version of the harmonics are being used in a specific application.\nTo get a version of the harmonics in spherical coordinates (to get something akin to terms in the Fourier series), all one has to do is apply the standard change of basis formula9:\n\\[\n\\begin{align*}\nx &= \\sin(\\theta) \\cos(\\phi) \\\\\ny &= \\sin(\\theta) \\sin(\\phi) \\\\\nz &= \\cos(\\theta)\n\\end{align*}\n\\]\nAs an example, \\(Y_{1, -1}\\) now becomes \\(Y_{1, -1}(\\theta, \\phi) = \\sqrt{\\frac{3}{4\\pi}} \\sin(\\theta) \\sin(\\phi)\\). We could plot this function on the 2D plane10, just as we plotted \\(\\sin(\\theta)\\) on the 1D line at the start of this article:\n\n\nCode\nfig = make_subplots(rows=1, cols=1,\n                    specs=[[{'is_3d': True}]])\n\ntheta = np.linspace(0, 2*np.pi, 100)\nphi = np.linspace(0, 2*np.pi, 100)\ntheta, phi = np.meshgrid(theta, phi)\n\nx = np.sin(theta) * np.cos(phi)\ny = np.sin(theta) * np.sin(phi)\nz = np.cos(theta)\n\nr0a = np.sqrt(3/4*np.pi) * np.sin(theta) * np.sin(phi)\n# r0 = np.abs(r0a)\nfig.add_trace(\n  go.Surface(x=theta, y=phi, z=r0a, \n    surfacecolor=r0a, \n    colorscale='RdBu'\n  )\n)\n\nfig.update_layout(\n    font_family=\"JuliaMono\",\n    showlegend=False,\n    margin=go.layout.Margin(l=0, r=0, b=0, t=0),\n    paper_bgcolor='rgba(0,0,0,0)',\n)\n\nfig.update_layout(\n  scene = dict(\n    xaxis = dict(\n      tickvals=[0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi],\n      ticktext=['0', 'pi/4', 'pi/2', '3pi/4', 'pi'], \n      range=[0,np.pi], \n      title_text=\"θ\"),\n    yaxis = dict(\n      tickvals=[0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi, 5*np.pi/4, 6*np.pi/4, 7*np.pi/4, 8*np.pi/4],\n      ticktext=['0', 'pi/4', 'pi/2', '3pi/4', 'pi', '5pi/4', '3/2pi', '7pi/4', '2pi'], \n      nticks=4, \n      range=[0,2*np.pi], \n      title_text=\"ϕ\"),\n    zaxis = dict(\n      title_text=\"Y(θ, ϕ)\",\n      nticks=3, \n      tickvals=[-1.5, 0, 1.5])),\n  scene1_aspectratio=dict(x=1, y=2, z=1),\n  margin=dict(r=20, l=10, b=10, t=10)\n)\n\ncamera = dict(eye=dict(x=2, y=2, z=2))\nfig.layout.scene1.camera = camera\n\nfig.update_traces(showscale=False)\nfig.show()\n\n\n\n\n                                                \nInteractive 4: Y₁,₋₁ is a harmonic of degree and as we see, it completes one full cycle over the domain\n\n\n\nJust as how we were able to interpret a periodic function on the 1D line as being on a circle, we can now see certain periodic functions on the 2D plane as existing on a sphere11. This now also means we have three different ways of looking at periodic functions on circles and spheres!\nFirstly, in the 1D case, we can represent \\(\\sin(\\theta)\\) as:\n\n\n\nFirst there’s the standard angle on x-axis, value on y-axis plot, and the other two are the cartesian plots discussed.\n\n\nSimilarly, in the 2D case, we can represent \\(Y_{1,-1}\\) as:\n\n\n\nAgain, all of them are different visual representations of the same thing: a function on a sphere."
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#harmonics-real-world",
    "href": "blog/spherical-harmonics/index.html#harmonics-real-world",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Harmonics: Real world",
    "text": "Harmonics: Real world\nNow that we’ve finished making the connection between the Fourier series and Spherical harmonics, one might ask: what are they actually used for? There’s plenty (they pop up rather frequently when functions on a sphere are involved), here are some:\n\nSolving PDEs: Recall that the motivating reason behind the discovery of the Fourier series was to solve the heat equation, which is a partial differential equation. Likewise, we can now decompose any (well-behaved) function on a sphere into a sum of spherical harmonics as basis functions, and solve the PDE using these basis functions (instead of the more complicated original)\nPhysics: One particularly important PDE is Schrödinger’s equation. Focusing on hydrogen-like atoms, the spherical harmonics can be use to describe the angular component of the solutions, which can then be converted into the probability that the electron will be found in a specific direction. Note that the angular component cannot tell us the probability how far from the origin the electron will be found (that’d be the radial component), only the probability in a given direction.\nMachine Learning: One problem with ordinary neural networks is that they do not respect input geometry. For example, if your input is a molecule (with atom coordinates) and your output is toxicity, if you rotate the input, the hidden layer outputs will change in an ill-defined way and return a different prediction, even though your input is the same molecule! Spherical harmonics are used to build rotationally equivariant neural networks, such that the change for a rotation properly is defined, and then ultimately produce an invariant prediction: predicting the same toxicity level for a molecule, regardless of input orientation."
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#conclusion",
    "href": "blog/spherical-harmonics/index.html#conclusion",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Conclusion",
    "text": "Conclusion\nSpherical harmonics were one of those things that felt intimidating to me at first, because of the dense math involved. But ultimately, they’re basically a sibling of the Fourier series, the only difference being that they’re for functions defined on a sphere instead of a circle. The math, once visualized, is beautiful, and I hope this has been a helpful read!\n\nFurther Resources:\nAppendix B, Spherical Harmonics: This chapter from Wojciech Jarosz’s Ph.D. dissertation is a great resource on spherical harmonics, particularly the real-valued ones (it’s written from a computer graphics standpoint; most physics-oriented ones would focus on the complex-valued ones).\ne3nn library: This PyTorch library is geared toward equivariant neural networks broadly, but also contains optimized implementations to compute spherical harmonics.\nSEGNNs paper: This is a recent paper on constructing equivariant neural networks; the appendix has a great section on how spherical harmonics are used to build them."
  },
  {
    "objectID": "blog/spherical-harmonics/index.html#footnotes",
    "href": "blog/spherical-harmonics/index.html#footnotes",
    "title": "Visual Notes on Spherical Harmonics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr more broadly, even nonperiodic and/or complex-valued functions. We’re not taking a look at them here, but for a deeper dive there’s this great 3b1b video.↩︎\nThere are many different conventions on how to represent fourier series (such as the amplitude-phase form, the exponential form, etc.), which you can find here. We’re using the sine-cosine form for this article.↩︎\nThe curious reader could verify this directly! Each coefficient is defined as the integral over the period (here, \\(0\\) to \\(2π\\)) of the product of the function \\(f(θ)\\) and the trigonometric function. For example, from \\(0\\) to \\(2π\\), \\(\\int f(\\theta)sin(3\\theta)d\\theta = 0.25\\)↩︎\nThey’re called basis functions, as a linear combination of these functions can produce any periodic function; akin to how a linear combination of a basis in \\(\\mathbb{R}^3\\) (e.g. \\([1,0,0]\\), \\([0,1,0]\\) and \\([0,0,1]\\)) can produce any vector in R³.\nMore formally, these infinitely many sine and cosine functions together form a basis for “square-integrable” functions defined on the circle, that is, for \\(L_2({S^1})\\).↩︎\nTerrible pun absolutely intended.↩︎\nMore precisely, they form a basis for \\(L_2(S^2)\\), the space of square integrable functions defined on the sphere.↩︎\nThis also means in practice, efficient calculation of the values are non-trivial, since there are so many different functions to account for. The functions themselves are not arbitrary; they’re made of the derivatives of Legendre polynomials of varying degrees.↩︎\nA more exhaustive table of them can be found on Wikipedia here. Note that we’re assuming r=1 throughout this article.↩︎\nFor a recap of Spherical coordinates, this may be helpful. Note that there’s multiple conventions! We’re using the physics convention here (in the math one, \\(ϕ\\) and \\(θ\\) are flipped).↩︎\nNote that we plot \\(ϕ\\) from \\(0\\) to \\(2π\\), but \\(θ\\) only from \\(0\\) to \\(π\\); this covers the whole sphere. This is because \\(θ\\) can be seen as selecting the “height” on the sphere, and then we traverse a full cycle (with \\(ϕ\\)) on the circle at that height. \\(θ\\) being from \\(0\\) to \\(π\\) is enough, otherwise we’d use each height twice.↩︎\nNote for this to hold, the values on the entire boundary must be the same (not just the endpoint as in the 1D case), as they get mapped to one of the two poles on the sphere; here all points on the boundary have the value 0.\nFor a more general periodic function (where the entire boundary doesn’t have the same value), where the period endpoints of any 1D slice (parallel to the x or y axes) of the 2D function are the same, it is possible to see that function existing on a torus, that is, \\(L_2(S^1 \\times S^1)\\) instead of \\(L_2(S^2)\\).↩︎"
  }
]